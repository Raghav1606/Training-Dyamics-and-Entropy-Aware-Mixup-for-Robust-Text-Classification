{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce386863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaPreTrainedModel,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaPooler,\n",
    "    RobertaClassificationHead\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    SequenceClassifierOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3989a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG = True\n",
    "LAMBDA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd248152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaMixerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb4590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaMixerModel(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n",
    "    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
    "    Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = RobertaMixerEncoder(config)\n",
    "\n",
    "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "\n",
    "            \n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba067bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaMixerForSequenceClassification(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaMixerModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        if FLAG:\n",
    "            self.mixup_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "            self.mixup_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "            self.mixup_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_1: Optional[torch.LongTensor] = None,\n",
    "        attention_mask_1: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids_1: Optional[torch.LongTensor] = None,\n",
    "        input_ids_2: Optional[torch.LongTensor] = None,\n",
    "        attention_mask_2: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids_2: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels_1: Optional[torch.LongTensor] = None,\n",
    "        labels_2: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs_1 = self.roberta(\n",
    "            input_ids_1,\n",
    "            attention_mask=attention_mask_1,\n",
    "            token_type_ids=token_type_ids_1,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output_1 = outputs_1[0]\n",
    "        \n",
    "        # Mixup train\n",
    "        if (input_ids_2 is not None) and (attention_mask_2 is not None) and (labels_2 is not None):\n",
    "            \n",
    "            outputs_2 = self.roberta(\n",
    "                input_ids_2,\n",
    "                attention_mask=attention_mask_2,\n",
    "                token_type_ids=token_type_ids_2,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            sequence_output_2 = outputs_2[0]\n",
    "\n",
    "            sequence_output = (LAMBDA * sequence_output_1) + ((1 - LAMBDA) * sequence_output_2)\n",
    "\n",
    "            if FLAG:\n",
    "                sequence_output = self.mixup_dense(sequence_output)\n",
    "                sequence_output = self.mixup_layernorm(sequence_output)\n",
    "                sequence_output = self.mixup_dropout(sequence_output)\n",
    "\n",
    "            logits = self.classifier(sequence_output)\n",
    "\n",
    "            loss = None\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = (LAMBDA * loss_fct(logits.view(-1, self.num_labels), labels_1.view(-1))) + ((1 - LAMBDA) * loss_fct(logits.view(-1, self.num_labels), labels_2.view(-1)))\n",
    "\n",
    "        # Mixup eval\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output_1)\n",
    "            loss = None\n",
    "            if labels_1 is not None:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels_1.view(-1))\n",
    "         \n",
    "        # Return logits, loss, and hidden states\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83065817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaMixerForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaMixerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaMixerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaMixerForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'mixup_layernorm.weight', 'mixup_layernorm.bias', 'mixup_dense.bias', 'classifier.dense.weight', 'mixup_dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL = RobertaMixerForSequenceClassification.from_pretrained('roberta-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61da58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    ConcatDataset,\n",
    "    DataLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994b0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbda2db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67344</th>\n",
       "      <td>67344</td>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67345</th>\n",
       "      <td>67345</td>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67346</th>\n",
       "      <td>67346</td>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67347</th>\n",
       "      <td>67347</td>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>67348</td>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0          0       hide new secretions from the parental units       0   \n",
       "1          1               contains no wit , only labored gags       0   \n",
       "2          2  that loves its characters and communicates som...      1   \n",
       "3          3  remains utterly satisfied to remain the same t...      0   \n",
       "4          4  on the worst revenge-of-the-nerds clichés the ...      0   \n",
       "...      ...                                                ...    ...   \n",
       "67344  67344                               a delightful comedy       1   \n",
       "67345  67345                   anguish , anger and frustration       0   \n",
       "67346  67346  at achieving the modest , crowd-pleasing goals...      1   \n",
       "67347  67347                                  a patient viewer       1   \n",
       "67348  67348  this new jangle of noise , mayhem and stupidit...      0   \n",
       "\n",
       "      data_type  \n",
       "0          easy  \n",
       "1          hard  \n",
       "2          easy  \n",
       "3          easy  \n",
       "4          hard  \n",
       "...         ...  \n",
       "67344      hard  \n",
       "67345      easy  \n",
       "67346      easy  \n",
       "67347      hard  \n",
       "67348      easy  \n",
       "\n",
       "[67349 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/imdb/with_idx/test/test.csv')\n",
    "df['data_type'] = random.choices(['easy', 'hard', 'ambiguous'], weights=(50, 35, 15), k=len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9585c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "easy         33547\n",
       "hard         23794\n",
       "ambiguous    10008\n",
       "Name: data_type, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa7a1574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57336</th>\n",
       "      <td>67344</td>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57337</th>\n",
       "      <td>67345</td>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57338</th>\n",
       "      <td>67346</td>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57339</th>\n",
       "      <td>67347</td>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57340</th>\n",
       "      <td>67348</td>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57341 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0          0       hide new secretions from the parental units       0   \n",
       "1          1               contains no wit , only labored gags       0   \n",
       "2          2  that loves its characters and communicates som...      1   \n",
       "3          3  remains utterly satisfied to remain the same t...      0   \n",
       "4          4  on the worst revenge-of-the-nerds clichés the ...      0   \n",
       "...      ...                                                ...    ...   \n",
       "57336  67344                               a delightful comedy       1   \n",
       "57337  67345                   anguish , anger and frustration       0   \n",
       "57338  67346  at achieving the modest , crowd-pleasing goals...      1   \n",
       "57339  67347                                  a patient viewer       1   \n",
       "57340  67348  this new jangle of noise , mayhem and stupidit...      0   \n",
       "\n",
       "      data_type  \n",
       "0          easy  \n",
       "1          hard  \n",
       "2          easy  \n",
       "3          easy  \n",
       "4          hard  \n",
       "...         ...  \n",
       "57336      hard  \n",
       "57337      easy  \n",
       "57338      easy  \n",
       "57339      hard  \n",
       "57340      easy  \n",
       "\n",
       "[57341 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['data_type'] != 'ambiguous'].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0bb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mixup_data(data, mixup_type='random_same'):\n",
    "    if mixup_type == 'random_same':\n",
    "        data_easy = data[data['data_type'] == 'easy'].reset_index(drop=True)\n",
    "        temp_easy = data_easy[['idx', 'label', 'data_type']].copy().rename(columns={\"idx\": \"idx_2\", \"label\": \"label_2\", \"data_type\": \"data_type_2\"}).sample(frac=1).reset_index(drop=True)\n",
    "        data_easy = pd.concat([data_easy, temp_easy], axis=1)\n",
    "        \n",
    "        data_hard = data[data['data_type'] == 'hard'].reset_index(drop=True)\n",
    "        temp_hard = data_hard[['idx', 'label', 'data_type']].copy().rename(columns={\"idx\": \"idx_2\", \"label\": \"label_2\", \"data_type\": \"data_type_2\"}).sample(frac=1).reset_index(drop=True)\n",
    "        data_hard = pd.concat([data_hard, temp_hard], axis=1)\n",
    "        \n",
    "        return pd.concat([data_easy, data_hard]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    elif mixup_type == 'random_opposite':\n",
    "        data_easy = data[data['data_type'] == 'easy'].reset_index(drop=True)\n",
    "        data_hard = data[data['data_type'] == 'hard'].reset_index(drop=True)\n",
    "        \n",
    "        easy_tuple = list(zip(data_easy['idx'].tolist(), data_easy['label'].tolist(), data_easy['data_type'].tolist()))\n",
    "        hard_tuple = list(zip(data_hard['idx'].tolist(), data_hard['label'].tolist(), data_hard['data_type'].tolist()))\n",
    "        \n",
    "        hard4easy = random.choices(hard_tuple, weights=np.ones(len(hard_tuple)), k=len(data_easy))\n",
    "        easy4hard = random.choices(easy_tuple, weights=np.ones(len(easy_tuple)), k=len(data_hard))\n",
    "\n",
    "        hard4easy = pd.DataFrame(hard4easy, columns=['idx_2', 'label_2', 'data_type_2'])\n",
    "        data_easy = pd.concat([data_easy, hard4easy], axis=1)\n",
    "        \n",
    "        easy4hard = pd.DataFrame(easy4hard, columns=['idx_2', 'label_2', 'data_type_2'])\n",
    "        data_hard = pd.concat([data_hard, easy4hard], axis=1)\n",
    "        \n",
    "        return pd.concat([data_easy, data_hard]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b829f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>data_type_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61850</td>\n",
       "      <td>imagine anyone managing to steal a movie not o...</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "      <td>48438</td>\n",
       "      <td>0</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40875</td>\n",
       "      <td>too simplistic to maintain interest</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>39288</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47460</td>\n",
       "      <td>most delightful moments</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>6053</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14570</td>\n",
       "      <td>is so clumsily sentimental and ineptly directe...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>43743</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44630</td>\n",
       "      <td>too big</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>31129</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57336</th>\n",
       "      <td>8268</td>\n",
       "      <td>pointless meditation on losers in a gone-to-se...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>15609</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57337</th>\n",
       "      <td>5844</td>\n",
       "      <td>amusing and unsettling</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "      <td>1890</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57338</th>\n",
       "      <td>57187</td>\n",
       "      <td>, like kubrick before him , may not touch the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>56026</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57339</th>\n",
       "      <td>60336</td>\n",
       "      <td>hampered by its predictable plot and paper-thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>48655</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57340</th>\n",
       "      <td>18553</td>\n",
       "      <td>is certainly amusing</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>40955</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57341 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0      61850  imagine anyone managing to steal a movie not o...      1   \n",
       "1      40875               too simplistic to maintain interest       0   \n",
       "2      47460                           most delightful moments       1   \n",
       "3      14570  is so clumsily sentimental and ineptly directe...      0   \n",
       "4      44630                                           too big       0   \n",
       "...      ...                                                ...    ...   \n",
       "57336   8268  pointless meditation on losers in a gone-to-se...      0   \n",
       "57337   5844                            amusing and unsettling       1   \n",
       "57338  57187  , like kubrick before him , may not touch the ...      1   \n",
       "57339  60336  hampered by its predictable plot and paper-thi...      0   \n",
       "57340  18553                              is certainly amusing       1   \n",
       "\n",
       "      data_type  idx_2  label_2 data_type_2  \n",
       "0          hard  48438        0        hard  \n",
       "1          easy  39288        0        easy  \n",
       "2          easy   6053        1        easy  \n",
       "3          easy  43743        0        easy  \n",
       "4          easy  31129        1        easy  \n",
       "...         ...    ...      ...         ...  \n",
       "57336      easy  15609        0        easy  \n",
       "57337      hard   1890        1        hard  \n",
       "57338      easy  56026        0        easy  \n",
       "57339      easy  48655        1        easy  \n",
       "57340      easy  40955        1        easy  \n",
       "\n",
       "[57341 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_same_df = get_mixup_data(df, mixup_type='random_same')\n",
    "random_same_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "259b6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_opposite_df = get_mixup_data(df, mixup_type='random_opposite')\n",
    "# random_opposite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b22dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMN = 'text'\n",
    "DATA_COLUMN = 'data_type'\n",
    "OUTPUT_COLUMN = 'label'\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a316bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseDataLoader(Dataset):\n",
    "    \n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         tokenizer,\n",
    "#         data: pd.DataFrame,\n",
    "#         data_type: str\n",
    "#     ):\n",
    "#         self.data = data\n",
    "#         self.data_type = data_type\n",
    "#         self.data = self.data[self.data[DATA_COLUMN] == self.data_type].reset_index(drop=True)\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.tokenized_data = tokenizer.batch_encode_plus(\n",
    "#             self.data[INPUT_COLUMN].tolist(),\n",
    "#             max_length=MAX_LEN,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_attention_mask=True,\n",
    "#             add_special_tokens=True,\n",
    "#             return_token_type_ids=True,            \n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "        \n",
    "        \n",
    "        \n",
    "#     def __len__(\n",
    "#         self\n",
    "#     ):\n",
    "#         return len(self.data)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def __getitem__(\n",
    "#         self,\n",
    "#         index: int\n",
    "#     ):\n",
    "#         data = {\n",
    "#             'data_type': self.data_type,\n",
    "#             'input_ids_1': self.tokenized_data['input_ids'][index].flatten(),\n",
    "#             'attention_mask_1': self.tokenized_data['attention_mask'][index].flatten(),\n",
    "#             'token_type_ids_1': self.tokenized_data['token_type_ids'][index].flatten(),\n",
    "#             'labels_1': torch.tensor(self.data.iloc[index][OUTPUT_COLUMN], dtype=torch.long),\n",
    "#         }\n",
    "        \n",
    "#         print(self.data)\n",
    "#         idx2 = self.data.iloc[index]['idx_2']\n",
    "#         index2 = self.data[self.data['idx'] == idx2].index[0]        \n",
    "#         print(self.data.iloc[index]['idx'], index, self.data.iloc[index]['idx_2'], index2, \"\\n\\n\")\n",
    "\n",
    "#         data['input_ids_2'] = self.tokenized_data['input_ids'][index2].flatten()\n",
    "#         data['attention_mask_2'] = self.tokenized_data['attention_mask'][index2].flatten()\n",
    "#         data['token_type_ids_2'] = self.tokenized_data['token_type_ids'][index2].flatten()\n",
    "#         data['labels_2'] = torch.tensor(self.data.iloc[index2][OUTPUT_COLUMN], dtype=torch.long)\n",
    "#         return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a516daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataLoader(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer,\n",
    "        data: pd.DataFrame,\n",
    "        data_type: str\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.data_type = data_type\n",
    "        self.data = self.data[self.data[DATA_COLUMN] == self.data_type].reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenizer.batch_encode_plus(\n",
    "            self.data[INPUT_COLUMN].tolist(),\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,            \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(\n",
    "        self\n",
    "    ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        data = {\n",
    "            'data_type': self.data_type,\n",
    "            'input_ids_1': self.tokenized_data['input_ids'][index].flatten(),\n",
    "            'attention_mask_1': self.tokenized_data['attention_mask'][index].flatten(),\n",
    "            'token_type_ids_1': self.tokenized_data['token_type_ids'][index].flatten(),\n",
    "            'labels_1': torch.tensor(self.data.iloc[index][OUTPUT_COLUMN], dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        idx2 = self.data.iloc[index]['idx_2']\n",
    "        index2 = self.data[self.data['idx'] == idx2].index[0]\n",
    "\n",
    "        text = self.data.iloc[index]['text']\n",
    "        text2 = self.data[self.data['idx'] == idx2]['text'].tolist()[0]\n",
    "\n",
    "        print(self.data.iloc[index]['idx'], text, idx2, \"text: \", text2, \"\\n\\n\")\n",
    "\n",
    "        data['input_ids_2'] = self.tokenized_data['input_ids'][index2].flatten()\n",
    "        data['attention_mask_2'] = self.tokenized_data['attention_mask'][index2].flatten()\n",
    "        data['token_type_ids_2'] = self.tokenized_data['token_type_ids'][index2].flatten()\n",
    "        data['labels_2'] = torch.tensor(self.data.iloc[index2][OUTPUT_COLUMN], dtype=torch.long)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6d1cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_data = BaseDataLoader(\n",
    "    tokenizer=TOKENIZER,\n",
    "    data=random_same_df,\n",
    "    data_type='easy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8317c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_data = BaseDataLoader(\n",
    "    tokenizer=TOKENIZER,\n",
    "    data=random_same_df,\n",
    "    data_type='hard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d406b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = ConcatDataset([easy_data, hard_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38cf2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    final_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c888ee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61850 0 48438 785\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "idx2 = random_same_df[random_same_df['idx'] == random_same_df.iloc[index]['idx_2']].index[0]        \n",
    "print(random_same_df.iloc[index]['idx'], index, random_same_df.iloc[index]['idx_2'], idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d6a14f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data Iteration:   0%|                                                                                       | 0/1792 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40875 too simplistic to maintain interest  39288 text:  an exceptionally dreary and overwrought bit  \n",
      "\n",
      "\n",
      "47460 most delightful moments  6053 text:  proves to be sensational  \n",
      "\n",
      "\n",
      "14570 is so clumsily sentimental and ineptly directed it may leave you speaking in tongues .  43743 text:  there is nothing redeeming about this movie .  \n",
      "\n",
      "\n",
      "44630 too big  31129 text:  leaves shockwaves  \n",
      "\n",
      "\n",
      "25494 scotland looks wonderful , the fans are often funny fanatics  26611 text:  lawrence unleashes his trademark misogyny -- er , comedy -- like a human volcano or an overflowing septic tank  \n",
      "\n",
      "\n",
      "43726 the really bad blair witch project  34663 text:  looked like crap ,  \n",
      "\n",
      "\n",
      "25461 with such clarity  54683 text:  the low-key direction is pleasingly emphatic in this properly intense , claustrophobic tale of obsessive love .  \n",
      "\n",
      "\n",
      "40424 its exquisite acting , inventive screenplay ,  5984 text:  a lot smarter than your average bond .  \n",
      "\n",
      "\n",
      "57872 we never truly come to care about the main characters and whether or not they 'll wind up together ,  29570 text:  genuine and singular artist  \n",
      "\n",
      "\n",
      "44901 the excellent performances  46175 text:  constantly pulling the rug from underneath us , seeing things from new sides , plunging deeper , getting more intense  \n",
      "\n",
      "\n",
      "20196 interested  12597 text:  drowns in sap .  \n",
      "\n",
      "\n",
      "20958 chai 's structure and pacing are disconcertingly slack .  44499 text:  excellent performances from jacqueline bisset and martha plimpton  \n",
      "\n",
      "\n",
      "26551 a rip-off twice removed , modeled after ( seagal 's ) earlier copycat under siege , sometimes referred to as die hard on a boat .  13616 text:  chan has done in the united states  \n",
      "\n",
      "\n",
      "14225 've a taste for the quirky  34029 text:  can depress you about life itself  \n",
      "\n",
      "\n",
      "56442 chelsea hotel  21189 text:  aggrieved father cliché  \n",
      "\n",
      "\n",
      "60024 a more graceful way  45014 text:  eventually works its way up to merely bad rather than painfully awful  \n",
      "\n",
      "\n",
      "42988 to farts , urine , feces , semen , or any of the other foul substances  54240 text:  it won -- and wins still  \n",
      "\n",
      "\n",
      "18182 , the country bears ... should keep parents amused with its low groan-to-guffaw ratio .  65758 text:  , starving and untalented  \n",
      "\n",
      "\n",
      "19788 what `` empire '' lacks in depth it makes up for with its heart .  63311 text:  director jay russell stomps in hobnail boots over natalie babbitt 's gentle , endearing 1975 children 's novel .  \n",
      "\n",
      "\n",
      "21002 coppola has made a film of intoxicating atmosphere and little else .  17194 text:  a severe case of hollywood-itis  \n",
      "\n",
      "\n",
      "47256 interesting than any of the character dramas , which never reach satisfying conclusions  30718 text:  cold , pretentious , thoroughly dislikable study in sociopathy .  \n",
      "\n",
      "\n",
      "36462 leaves a lot to be desired .  19123 text:  a thriller with an edge --  \n",
      "\n",
      "\n",
      "63671 master of disguise runs for only 71 minutes and feels like three hours .  17774 text:  on the amateurish  \n",
      "\n",
      "\n",
      "14290 its apparent glee  14941 text:  be wholesome and subversive at the same time  \n",
      "\n",
      "\n",
      "207 a résumé loaded with credits like `` girl in bar # 3  40548 text:  a mechanical action-comedy whose seeming purpose is to market the charismatic jackie chan to even younger audiences .  \n",
      "\n",
      "\n",
      "49015 in its place a sweetness , clarity and emotional openness that recalls the classics of early italian neorealism  1199 text:  of sleaziness  \n",
      "\n",
      "\n",
      "11860 the master of disguise represents adam sandler 's latest attempt to dumb down the universe .  33809 text:  shows why , after only three films , director/co-writer jacques audiard , though little known in this country , belongs in the very top rank of french filmmakers  \n",
      "\n",
      "\n",
      "1296 spies  34297 text:  , the movie possesses its own languorous charm .  \n",
      "\n",
      "\n",
      "43612 does a good job of establishing a time and place , and of telling a fascinating character 's story  16998 text:  one of those movies that catches you up in something bigger than yourself  \n",
      "\n",
      "\n",
      "55893 the always hilarious meara and levy  66405 text:  bring on the battle bots , please  \n",
      "\n",
      "\n",
      "15453 a moving tale of love and destruction in unexpected places  42339 text:  a suffocating rape-payback horror show that hinges on the subgenre 's most enabling victim ... and an ebullient affection for industrial-model meat freezers .  \n",
      "\n",
      "\n",
      "7260 would have been better than the fiction it has concocted  11074 text:  works its magic with such exuberance and passion that the film 's length becomes a part of its fun .  \n",
      "\n",
      "\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32])\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(data_loader, desc=\"data Iteration\")):\n",
    "    data_type = batch['data_type']\n",
    "    input_ids_1, attention_mask_1, token_type_ids_1, labels_1 = batch['input_ids_1'], batch['attention_mask_1'], batch['token_type_ids_1'], batch['labels_1']\n",
    "    input_ids_2, attention_mask_2, token_type_ids_2, labels_2 = batch['input_ids_2'], batch['attention_mask_2'], batch['token_type_ids_2'], batch['labels_2']\n",
    "\n",
    "    print(input_ids_1.shape)\n",
    "    print(attention_mask_1.shape)\n",
    "    print(token_type_ids_1.shape)\n",
    "    print(labels_1.shape)\n",
    "    \n",
    "    print(input_ids_2.shape)\n",
    "    print(attention_mask_2.shape)\n",
    "    print(token_type_ids_2.shape)\n",
    "    print(labels_2.shape)\n",
    "    \n",
    "    print(len(data_type))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a61c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61850 imagine anyone managing to steal a movie not only from charismatic rising star jake gyllenhaal but also from accomplished oscar winners susan sarandon , dustin hoffman and holly hunter , yet newcomer ellen pompeo pulls off the feat with aplomb  48438 text:  deep vein  \n",
      "\n",
      "\n",
      "40875 too simplistic to maintain interest  39288 text:  an exceptionally dreary and overwrought bit  \n",
      "\n",
      "\n",
      "47460 most delightful moments  6053 text:  proves to be sensational  \n",
      "\n",
      "\n",
      "14570 is so clumsily sentimental and ineptly directed it may leave you speaking in tongues .  43743 text:  there is nothing redeeming about this movie .  \n",
      "\n",
      "\n",
      "44630 too big  31129 text:  leaves shockwaves  \n",
      "\n",
      "\n",
      "58251 the film is also imbued with strong themes of familial ties and spirituality that are powerful and moving without stooping to base melodrama  63601 text:  that clever angle  \n",
      "\n",
      "\n",
      "25494 scotland looks wonderful , the fans are often funny fanatics  26611 text:  lawrence unleashes his trademark misogyny -- er , comedy -- like a human volcano or an overflowing septic tank  \n",
      "\n",
      "\n",
      "43726 the really bad blair witch project  34663 text:  looked like crap ,  \n",
      "\n",
      "\n",
      "25461 with such clarity  54683 text:  the low-key direction is pleasingly emphatic in this properly intense , claustrophobic tale of obsessive love .  \n",
      "\n",
      "\n",
      "40424 its exquisite acting , inventive screenplay ,  5984 text:  a lot smarter than your average bond .  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    idx2 = random_same_df.iloc[i]['idx_2']\n",
    "    index2 = random_same_df[random_same_df['idx'] == idx2].index[0]\n",
    "    \n",
    "    text = random_same_df.iloc[i]['text']\n",
    "    text2 = random_same_df[random_same_df['idx'] == idx2]['text'].tolist()[0]\n",
    "    \n",
    "    print(random_same_df.iloc[i]['idx'], text, idx2, \"text: \", text2, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "664b5ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  3463, 32326,  ...,     1,     1,     1],\n",
       "        [    0,   627,   822,  ...,     1,     1,     1],\n",
       "        [    0,  3245,   295,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  1322,  1690,  ...,     1,     1,     1],\n",
       "        [    0, 17330,     8,  ...,     1,     1,     1],\n",
       "        [    0,   102, 17846,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08327df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   405,  1072,  ...,     1,     1,     1],\n",
       "        [    0, 37945,  1951,  ...,     1,     1,     1],\n",
       "        [    0,  7424,    45,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  9056, 11926,  ...,     1,     1,     1],\n",
       "        [    0, 33844, 13510,  ...,     1,     1,     1],\n",
       "        [    0,   354,   818,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8586a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idx                                 12062\n",
       "text           exceedingly memorable one \n",
       "label                                   1\n",
       "data_type                            easy\n",
       "idx_2                               48976\n",
       "label_2                                 0\n",
       "data_type_2                          easy\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44805ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
