Loading dataset
Loading RoBERTa tokenizer and model
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing dataset
(1808, 2)
(872, 2)
Test Data Loaded
Fine-tuning model
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Iteration:   0%|          | 0/5 [00:00<?, ?it/s][A
Iteration:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:02,  1.95it/s][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:01,  2.60it/s][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.89it/s][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  3.05it/s][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.40it/s]
train loss: 0.6897626872326656
Evaluating model

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:  15%|â–ˆâ–Œ        | 2/13 [00:00<00:01, 10.66it/s][A
Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 10.61it/s][A
Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 10.59it/s][A
Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 10.56it/s][A
Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 10.50it/s][A
Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 10.48it/s][AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 10.93it/s]
eval loss: 0.6943029467275627
eval accuracy: 0.42
Epoch:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.66s/it]
Iteration:   0%|          | 0/5 [00:00<?, ?it/s][A
Iteration:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  3.34it/s][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00,  3.31it/s][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:00<00:00,  3.32it/s][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  3.31it/s][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.93it/s]
train loss: 0.669772993945539
Evaluating model

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:  15%|â–ˆâ–Œ        | 2/13 [00:00<00:01, 10.62it/s][A
Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 10.54it/s][A
Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 10.50it/s][A
Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 10.48it/s][A
Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 10.43it/s][A
Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 10.44it/s][AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 10.28it/s]
eval loss: 0.7082057302908142
eval accuracy: 0.42
Epoch:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.59s/it]
Iteration:   0%|          | 0/5 [00:00<?, ?it/s][A
Iteration:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  3.29it/s][A
Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00,  3.32it/s][A
Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:00<00:00,  3.32it/s][A
Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  3.33it/s][AIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.94it/s]
train loss: 0.5220387818004216
Evaluating model

Evaluating:   0%|          | 0/13 [00:00<?, ?it/s][A
Evaluating:  15%|â–ˆâ–Œ        | 2/13 [00:00<00:01, 10.58it/s][A
Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 10.51it/s][A
Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 10.48it/s][A
Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 10.47it/s][A
Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 10.42it/s][A
Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 10.42it/s][AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 10.85it/s]
eval loss: 0.7174323242881494
eval accuracy: 0.42
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.53s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.56s/it]
Saving model and tokenizer
Evaluating model
Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]Evaluating:  15%|â–ˆâ–Œ        | 2/13 [00:00<00:01, 10.65it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 10.57it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 10.53it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 10.51it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 10.46it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 10.46it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 10.89it/s]
eval loss: 0.7254769912024204
eval accuracy: 0.42


--------DONE--------
