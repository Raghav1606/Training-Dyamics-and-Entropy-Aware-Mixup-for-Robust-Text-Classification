{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7016ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fire\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    set_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9defa",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f39739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_COLUMN:  text\n",
      "DATA_COLUMN:  category\n",
      "OUTPUT_COLUMN:  label\n",
      "NUM_EPOCHS:  2\n",
      "MAX_LEN:  256\n",
      "BATCH_SIZE:  32\n",
      "LAMBDA:  0.5\n",
      "FLAG:  False\n",
      "MIXUP_START:  10\n"
     ]
    }
   ],
   "source": [
    "FLAG = False\n",
    "MIXUP_START = 10\n",
    "LAMBDA = 0.5\n",
    "\n",
    "INPUT_COLUMN = 'text'\n",
    "DATA_COLUMN = 'category'\n",
    "OUTPUT_COLUMN = 'label'\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"INPUT_COLUMN: \", INPUT_COLUMN)\n",
    "print(\"DATA_COLUMN: \", DATA_COLUMN)\n",
    "print(\"OUTPUT_COLUMN: \", OUTPUT_COLUMN)\n",
    "print(\"NUM_EPOCHS: \", NUM_EPOCHS)\n",
    "print(\"MAX_LEN: \", MAX_LEN)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"LAMBDA: \", LAMBDA)\n",
    "print(\"FLAG: \", FLAG)\n",
    "print(\"MIXUP_START: \", MIXUP_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9b61a",
   "metadata": {},
   "source": [
    "# Mixup Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83ce9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaPreTrainedModel,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaPooler,\n",
    "    RobertaClassificationHead\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    SequenceClassifierOutput\n",
    ")\n",
    "\n",
    "# from config import *\n",
    "\n",
    "\n",
    "class RobertaMixerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class RobertaMixerModel(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n",
    "    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
    "    Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = RobertaMixerEncoder(config)\n",
    "\n",
    "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "\n",
    "            \n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class RobertaMixerForSequenceClassification(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaMixerModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        if FLAG:\n",
    "            self.mixup_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "            self.mixup_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "            self.mixup_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_1: Optional[torch.LongTensor] = None,\n",
    "        attention_mask_1: Optional[torch.FloatTensor] = None,\n",
    "        input_ids_2: Optional[torch.LongTensor] = None,\n",
    "        attention_mask_2: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels_1: Optional[torch.LongTensor] = None,\n",
    "        labels_2: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs_1 = self.roberta(\n",
    "            input_ids_1,\n",
    "            attention_mask=attention_mask_1,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output_1 = outputs_1[0]\n",
    "        \n",
    "        # Mixup train\n",
    "        if (input_ids_2 is not None) and (attention_mask_2 is not None) and (labels_2 is not None):\n",
    "            \n",
    "            outputs_2 = self.roberta(\n",
    "                input_ids_2,\n",
    "                attention_mask=attention_mask_2,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            sequence_output_2 = outputs_2[0]\n",
    "\n",
    "            sequence_output = (LAMBDA * sequence_output_1) + ((1 - LAMBDA) * sequence_output_2)\n",
    "\n",
    "            if FLAG:\n",
    "                sequence_output = self.mixup_dense(sequence_output)\n",
    "                sequence_output = self.mixup_layernorm(sequence_output)\n",
    "                sequence_output = self.mixup_dropout(sequence_output)\n",
    "\n",
    "            logits = self.classifier(sequence_output)\n",
    "\n",
    "            loss = None\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = (LAMBDA * loss_fct(logits.view(-1, self.num_labels), labels_1.view(-1))) + ((1 - LAMBDA) * loss_fct(logits.view(-1, self.num_labels), labels_2.view(-1)))\n",
    "\n",
    "        # Mixup eval\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output_1)\n",
    "            loss = None\n",
    "            if labels_1 is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels_1.view(-1))\n",
    "         \n",
    "        # Return logits, loss, and hidden states\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs_1[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs_1.hidden_states,\n",
    "            attentions=outputs_1.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1db9e",
   "metadata": {},
   "source": [
    "## MixupDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875aa2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a829e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(data, info_data=None, mixup=False):\n",
    "    \n",
    "#     # Remove none and hard examples\n",
    "#     data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "\n",
    "#     # Add softmax and entropy info\n",
    "#     if (info_data is not None) and mixup:\n",
    "#         data = pd.merge(data, info_data, on='idx')[['idx', 'text', 'label', 'category', 'softmax', 'entropy']]\n",
    "#         mixup_size = len(info_data) - len(data)\n",
    "\n",
    "#         # --------------------------------------- Same class mixup ---------------------------------------  \n",
    "\n",
    "#         # Easy-Easy Mixup\n",
    "#         easy_data = data[data['category'] == 'easy']\n",
    "#         easy_low_ent_idx = easy_data.sort_values('entropy', ascending=True).head(mixup_size//3)['idx'].tolist()\n",
    "#         easy_high_ent_idx = easy_data.sort_values('entropy', ascending=False).head(mixup_size//3)['idx'].tolist()\n",
    "        \n",
    "#         easy_mixup_data = easy_data[easy_data['idx'].isin(easy_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(easy_high_ent_idx)\n",
    "#         easy_mixup_data['idx_2'] = easy_high_ent_idx\n",
    "#         easy_mixup_data['text_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['text'].values[0])\n",
    "#         easy_mixup_data['label_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['label'].values[0])\n",
    "#         easy_mixup_data['category_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['category'].values[0])\n",
    "#         easy_mixup_data['mixup_type'] = 'same_easy'\n",
    "        \n",
    "#         # Ambi-Ambi Mixup\n",
    "#         ambiguous_data = data[data['category'] == 'ambiguous']\n",
    "#         ambiguous_low_ent_idx = ambiguous_data.sort_values('entropy', ascending=True).head(mixup_size//3)['idx'].tolist()\n",
    "#         ambiguous_high_ent_idx = ambiguous_data.sort_values('entropy', ascending=False).head(mixup_size//3)['idx'].tolist()\n",
    "        \n",
    "#         ambiguous_mixup_data = ambiguous_data[ambiguous_data['idx'].isin(ambiguous_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(ambiguous_high_ent_idx)\n",
    "#         ambiguous_mixup_data['idx_2'] = ambiguous_high_ent_idx\n",
    "#         ambiguous_mixup_data['text_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['text'].values[0])\n",
    "#         ambiguous_mixup_data['label_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['label'].values[0])\n",
    "#         ambiguous_mixup_data['category_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['category'].values[0])\n",
    "#         ambiguous_mixup_data['mixup_type'] = 'same_ambiguous'\n",
    "        \n",
    "#         same_mixup_data = pd.concat([easy_mixup_data, ambiguous_mixup_data])\n",
    "        \n",
    "#         # --------------------------------------- Different class mixup ---------------------------------------  \n",
    "        \n",
    "#         # Random easy-ambi mixup\n",
    "#         different_samples = mixup_size - len(same_mixup_data)\n",
    "#         easy_tuple = list(zip(easy_data['idx'].tolist(), easy_data['text'].tolist(), easy_data['label'].tolist(), easy_data['category'].tolist()))\n",
    "#         ambiguous_tuple = list(zip(ambiguous_data['idx'].tolist(), ambiguous_data['text'].tolist(), ambiguous_data['label'].tolist(), ambiguous_data['category'].tolist()))\n",
    "        \n",
    "#         easy_data = easy_data.sample(n=different_samples//2).reset_index(drop=True)\n",
    "#         ambiguous4easy = random.choices(ambiguous_tuple, weights=np.ones(len(ambiguous_tuple)), k=different_samples//2)\n",
    "#         ambiguous4easy = pd.DataFrame(ambiguous4easy, columns=['idx_2', 'text_2', 'label_2', 'category_2'])\n",
    "#         ambiguous4easy = pd.concat([easy_data, ambiguous4easy], axis=1).reset_index(drop=True)\n",
    "#         ambiguous4easy['mixup_type'] = 'ambiguous_easy'\n",
    "        \n",
    "#         ambiguous_data = ambiguous_data.sample(n=different_samples//2).reset_index(drop=True)\n",
    "#         easy4ambiguous = random.choices(easy_tuple, weights=np.ones(len(easy_tuple)), k=different_samples//2)\n",
    "#         easy4ambiguous = pd.DataFrame(easy4ambiguous, columns=['idx_2', 'text_2', 'label_2', 'category_2'])\n",
    "#         easy4ambiguous = pd.concat([ambiguous_data, easy4ambiguous], axis=1).reset_index(drop=True)\n",
    "#         easy4ambiguous['mixup_type'] = 'easy_ambiguous'\n",
    "        \n",
    "#         return pd.concat([same_mixup_data, easy4ambiguous, ambiguous4easy]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aafd0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_data(df, label=0):\n",
    "    df_label = df[df['label'] == label].reset_index(drop=True)\n",
    "    temp_label = df_label.sample(frac=1).reset_index(drop=True)\n",
    "    temp_label = temp_label.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})   \n",
    "    return pd.concat([df_label, temp_label], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "892a511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_original(data):\n",
    "#     return data[(data['category'] != 'none') & (data['category'] != 'hard')].sample(frac=1).reset_index(drop=True)\n",
    "    return data[data['category'] != 'hard'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a458859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_random_mixup(data, info_data=None, use_label=False):\n",
    "    data_len = len(data)\n",
    "    data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "    \n",
    "    if use_label:\n",
    "        data_0 = get_label_data(data, label=0)\n",
    "        data_1 = get_label_data(data, label=1)\n",
    "        final_data = pd.concat([data_0, data_1]).reset_index(drop=True)\n",
    "        \n",
    "    else:\n",
    "        temp = data.copy()\n",
    "        temp = temp.sample(frac=1).reset_index(drop=True)\n",
    "        temp = temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "        final_data = pd.concat([data, temp], axis=1)\n",
    "        \n",
    "    random_subset_1 = data.sample(n=data_len-len(final_data)).reset_index(drop=True)\n",
    "    random_subset_2 = data.sample(n=data_len-len(final_data)).reset_index(drop=True)\n",
    "    random_subset_2 = random_subset_2.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "    random_subset = pd.concat([random_subset_1, random_subset_2], axis=1).reset_index(drop=True)  \n",
    "\n",
    "    return pd.concat([final_data, random_subset]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "886f894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_category_mixup(data, info_data=None, use_label=False, use_entropy=False):\n",
    "    data_len = len(data)\n",
    "    data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "#     data = data[data['category'] != 'hard'].reset_index(drop=True)\n",
    "    \n",
    "    if use_entropy:\n",
    "        # Easy-Easy Mixup\n",
    "        easy_data = data[data['category'] == 'easy']\n",
    "        easy_low_ent_idx = easy_data.sort_values('entropy', ascending=True).head(mixup_size//2)['idx'].tolist()\n",
    "        easy_high_ent_idx = easy_data.sort_values('entropy', ascending=False).head(mixup_size//2)['idx'].tolist()\n",
    "        \n",
    "        easy_mixup_data = easy_data[easy_data['idx'].isin(easy_low_ent_idx)].reset_index(drop=True)\n",
    "        random.shuffle(easy_high_ent_idx)\n",
    "        easy_mixup_data['idx_2'] = easy_high_ent_idx\n",
    "        easy_mixup_data['text_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['text'].values[0])\n",
    "        easy_mixup_data['label_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['label'].values[0])\n",
    "        easy_mixup_data['category_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['category'].values[0])\n",
    "        easy_mixup_data['mixup_type'] = 'same_easy'\n",
    "        \n",
    "        # Ambi-Ambi Mixup\n",
    "        ambiguous_data = data[data['category'] == 'ambiguous']\n",
    "        ambiguous_low_ent_idx = ambiguous_data.sort_values('entropy', ascending=True).head(mixup_size//2)['idx'].tolist()\n",
    "        ambiguous_high_ent_idx = ambiguous_data.sort_values('entropy', ascending=False).head(mixup_size//2)['idx'].tolist()\n",
    "        \n",
    "        ambiguous_mixup_data = ambiguous_data[ambiguous_data['idx'].isin(ambiguous_low_ent_idx)].reset_index(drop=True)\n",
    "        random.shuffle(ambiguous_high_ent_idx)\n",
    "        ambiguous_mixup_data['idx_2'] = ambiguous_high_ent_idx\n",
    "        ambiguous_mixup_data['text_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['text'].values[0])\n",
    "        ambiguous_mixup_data['label_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['label'].values[0])\n",
    "        ambiguous_mixup_data['category_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['category'].values[0])\n",
    "        ambiguous_mixup_data['mixup_type'] = 'same_ambiguous'\n",
    "        \n",
    "        final_data = pd.concat([easy_mixup_data, ambiguous_mixup_data]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    else:\n",
    "        if use_label:\n",
    "            # Easy-Easy mixup\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_data_0 = get_label_data(easy_data, label=0)\n",
    "            easy_data_1 = get_label_data(easy_data, label=1)\n",
    "            easy_data  = pd.concat([easy_data_0, easy_data_1]).reset_index(drop=True)\n",
    "            \n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_data_0 = get_label_data(ambiguous_data, label=0)\n",
    "            ambiguous_data_1 = get_label_data(ambiguous_data, label=1)\n",
    "            ambiguous_data  = pd.concat([ambiguous_data_0, ambiguous_data_1]).reset_index(drop=True)\n",
    "            \n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            # Easy-Ambi Mixup\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            # Easy-Easy mixup\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_temp = easy_data.copy()\n",
    "            easy_temp = easy_temp.sample(frac=1).reset_index(drop=True)\n",
    "            easy_temp = easy_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "            easy_data = pd.concat([easy_data, easy_temp], axis=1).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "\n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_data.copy()\n",
    "            ambiguous_temp = ambiguous_temp.sample(frac=1).reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "            ambiguous_data = pd.concat([ambiguous_data, ambiguous_temp], axis=1).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "            \n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            # Easy-Ambi Mixup\n",
    "            easy_ambiguous_len = data_len - len(final_data)\n",
    "            easy_ambiguous_data = easy_data.sample(n=easy_ambiguous_len)[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "            easy_ambiguous_temp = ambiguous_data.sample(n=easy_ambiguous_len)[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "            easy_ambiguous_temp = easy_ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "            easy_ambiguous_data = pd.concat([easy_ambiguous_data, easy_ambiguous_temp], axis=1)\n",
    "            \n",
    "            return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "897f36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(data, info_data=None, mixup=False):\n",
    "    \n",
    "#     # Remove none and hard examples\n",
    "#     data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "\n",
    "#     # Add softmax and entropy info\n",
    "#     if (info_data is not None) and mixup:\n",
    "#         data = pd.merge(data, info_data, on='idx')[['idx', 'text', 'label', 'category', 'softmax', 'entropy']]\n",
    "#         mixup_size = len(info_data) - len(data)\n",
    "\n",
    "#         # --------------------------------------- Same class mixup ---------------------------------------  \n",
    "\n",
    "#         # Easy-Easy Mixup\n",
    "#         easy_data = data[data['category'] == 'easy']\n",
    "#         easy_low_ent_idx = easy_data.sort_values('entropy', ascending=True).head(mixup_size//2)['idx'].tolist()\n",
    "#         easy_high_ent_idx = easy_data.sort_values('entropy', ascending=False).head(mixup_size//2)['idx'].tolist()\n",
    "        \n",
    "#         easy_mixup_data = easy_data[easy_data['idx'].isin(easy_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(easy_high_ent_idx)\n",
    "#         easy_mixup_data['idx_2'] = easy_high_ent_idx\n",
    "#         easy_mixup_data['text_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['text'].values[0])\n",
    "#         easy_mixup_data['label_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['label'].values[0])\n",
    "#         easy_mixup_data['category_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['category'].values[0])\n",
    "#         easy_mixup_data['mixup_type'] = 'same_easy'\n",
    "        \n",
    "#         # Ambi-Ambi Mixup\n",
    "#         ambiguous_data = data[data['category'] == 'ambiguous']\n",
    "#         ambiguous_low_ent_idx = ambiguous_data.sort_values('entropy', ascending=True).head(mixup_size//2)['idx'].tolist()\n",
    "#         ambiguous_high_ent_idx = ambiguous_data.sort_values('entropy', ascending=False).head(mixup_size//2)['idx'].tolist()\n",
    "        \n",
    "#         ambiguous_mixup_data = ambiguous_data[ambiguous_data['idx'].isin(ambiguous_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(ambiguous_high_ent_idx)\n",
    "#         ambiguous_mixup_data['idx_2'] = ambiguous_high_ent_idx\n",
    "#         ambiguous_mixup_data['text_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['text'].values[0])\n",
    "#         ambiguous_mixup_data['label_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['label'].values[0])\n",
    "#         ambiguous_mixup_data['category_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['category'].values[0])\n",
    "#         ambiguous_mixup_data['mixup_type'] = 'same_ambiguous'\n",
    "        \n",
    "#         return pd.concat([easy_mixup_data, ambiguous_mixup_data]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b3a9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(df, label1, label2):\n",
    "    cnt_label = 0\n",
    "    cnt_category = 0\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, label1] == df.at[i, f\"{label1}_2\"]:\n",
    "            cnt_label += 1\n",
    "        if df.at[i, label2] == df.at[i, f\"{label2}_2\"]:\n",
    "            cnt_category += 1\n",
    "\n",
    "    print(\"same label: \", cnt_label/len(df))\n",
    "    print(\"same category: \", cnt_category/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcdde589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>24995</td>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>24996</td>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>24997</td>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>24998</td>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>24999</td>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0          0  I rented I AM CURIOUS-YELLOW from my video sto...      0   \n",
       "1          1  \"I Am Curious: Yellow\" is a risible and preten...      0   \n",
       "2          2  If only to avoid making this type of film in t...      0   \n",
       "3          3  This film was probably inspired by Godard's Ma...      0   \n",
       "4          4  Oh, brother...after hearing about this ridicul...      0   \n",
       "...      ...                                                ...    ...   \n",
       "24995  24995  A hit at the time but now better categorised a...      1   \n",
       "24996  24996  I love this movie like no other. Another time ...      1   \n",
       "24997  24997  This film and it's sequel Barry Mckenzie holds...      1   \n",
       "24998  24998  'The Adventures Of Barry McKenzie' started lif...      1   \n",
       "24999  24999  The story centers around Barry McKenzie who mu...      1   \n",
       "\n",
       "        category  \n",
       "0      ambiguous  \n",
       "1           easy  \n",
       "2           easy  \n",
       "3           easy  \n",
       "4           none  \n",
       "...          ...  \n",
       "24995  ambiguous  \n",
       "24996       none  \n",
       "24997       none  \n",
       "24998  ambiguous  \n",
       "24999       none  \n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/projects/metis2/atharvak/Data_Cartography/datasets/imdb/imdb_categorized.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21a6ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idx</th>\n",
       "      <th>gold</th>\n",
       "      <th>softmax</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>23493</td>\n",
       "      <td>1</td>\n",
       "      <td>[8.37615955e-04 9.99162384e-01]</td>\n",
       "      <td>0.006772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24825</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0032071 0.9967929]</td>\n",
       "      <td>0.021618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20509</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.00222997 0.99777003]</td>\n",
       "      <td>0.015843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10239</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.99423855e-01 5.76144926e-04]</td>\n",
       "      <td>0.004874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10998</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.99719573e-01 2.80426921e-04]</td>\n",
       "      <td>0.002574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>24995</td>\n",
       "      <td>4317</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99804368 0.00195632]</td>\n",
       "      <td>0.014155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>24996</td>\n",
       "      <td>23824</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.00474844 0.99525156]</td>\n",
       "      <td>0.030141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>24997</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.99438949e-01 5.61051043e-04]</td>\n",
       "      <td>0.004761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>24998</td>\n",
       "      <td>11183</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.96347178 0.03652822]</td>\n",
       "      <td>0.156749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>24999</td>\n",
       "      <td>24926</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.00347956 0.99652044]</td>\n",
       "      <td>0.023171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    idx  gold                          softmax   entropy\n",
       "0               0  23493     1  [8.37615955e-04 9.99162384e-01]  0.006772\n",
       "1               1  24825     1            [0.0032071 0.9967929]  0.021618\n",
       "2               2  20509     1          [0.00222997 0.99777003]  0.015843\n",
       "3               3  10239     0  [9.99423855e-01 5.76144926e-04]  0.004874\n",
       "4               4  10998     0  [9.99719573e-01 2.80426921e-04]  0.002574\n",
       "...           ...    ...   ...                              ...       ...\n",
       "24995       24995   4317     0          [0.99804368 0.00195632]  0.014155\n",
       "24996       24996  23824     1          [0.00474844 0.99525156]  0.030141\n",
       "24997       24997    332     0  [9.99438949e-01 5.61051043e-04]  0.004761\n",
       "24998       24998  11183     0          [0.96347178 0.03652822]  0.156749\n",
       "24999       24999  24926     1          [0.00347956 0.99652044]  0.023171\n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df = pd.read_csv('/projects/metis2/atharvak/Data_Cartography/dy_log/imdb/roberta-base/training_dynamics/final_4.csv')\n",
    "info_df = info_df.rename(columns={'guid': 'idx', 'sm': 'softmax', 'en': 'entropy'})\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4212da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24537</th>\n",
       "      <td>24995</td>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24538</th>\n",
       "      <td>24996</td>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24539</th>\n",
       "      <td>24997</td>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24540</th>\n",
       "      <td>24998</td>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24541</th>\n",
       "      <td>24999</td>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24542 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0          0  I rented I AM CURIOUS-YELLOW from my video sto...      0   \n",
       "1          1  \"I Am Curious: Yellow\" is a risible and preten...      0   \n",
       "2          2  If only to avoid making this type of film in t...      0   \n",
       "3          3  This film was probably inspired by Godard's Ma...      0   \n",
       "4          4  Oh, brother...after hearing about this ridicul...      0   \n",
       "...      ...                                                ...    ...   \n",
       "24537  24995  A hit at the time but now better categorised a...      1   \n",
       "24538  24996  I love this movie like no other. Another time ...      1   \n",
       "24539  24997  This film and it's sequel Barry Mckenzie holds...      1   \n",
       "24540  24998  'The Adventures Of Barry McKenzie' started lif...      1   \n",
       "24541  24999  The story centers around Barry McKenzie who mu...      1   \n",
       "\n",
       "        category  \n",
       "0      ambiguous  \n",
       "1           easy  \n",
       "2           easy  \n",
       "3           easy  \n",
       "4           none  \n",
       "...          ...  \n",
       "24537  ambiguous  \n",
       "24538       none  \n",
       "24539       none  \n",
       "24540  ambiguous  \n",
       "24541       none  \n",
       "\n",
       "[24542 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_dataset = prepare_dataset_original(df)\n",
    "normal_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2708d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.50736\n",
      "same category:  0.49936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2102</td>\n",
       "      <td>Quite frankly it seemed like seven hours of bo...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>28</td>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>600</td>\n",
       "      <td>I've been going through the AFI's list of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>13182</td>\n",
       "      <td>Fascist principal Miss Togar(Mary Woronov, who...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7403</td>\n",
       "      <td>When I saw the previews for this movie, I didn...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>24303</td>\n",
       "      <td>Ya know when one looks at this Brian DePalma f...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18873</td>\n",
       "      <td>Great fun. I went with 8 friends to a sneak pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>8080</td>\n",
       "      <td>The mood of this movie is pretty good and it c...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22667</td>\n",
       "      <td>A strong woman oriented subject after long, di...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>16542</td>\n",
       "      <td>A complex story laid on the background of part...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>3287</td>\n",
       "      <td>This is the first Guinea Pig film from Japan a...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>12808</td>\n",
       "      <td>This movie of 370 minutes was aired by the Ita...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>4254</td>\n",
       "      <td>This was the WORST Christmas movie I ever saw....</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>11730</td>\n",
       "      <td>A fun concept, but poorly executed. Except for...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>3674</td>\n",
       "      <td>I hated this show when I was a kid. That was b...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>18066</td>\n",
       "      <td>Cheech &amp; Chong's Next Movie (1980) was the sec...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>4887</td>\n",
       "      <td>I was expecting the movie based on Grendel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>18086</td>\n",
       "      <td>The silent film the Pride of the Clan starring...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>21161</td>\n",
       "      <td>Pop quiz: you're a part of the modern armed fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>5220</td>\n",
       "      <td>'Blue Desert' may have had the potential to be...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0       2102  Quite frankly it seemed like seven hours of bo...      0   \n",
       "1        600  I've been going through the AFI's list of the ...      0   \n",
       "2       7403  When I saw the previews for this movie, I didn...      0   \n",
       "3      18873  Great fun. I went with 8 friends to a sneak pr...      1   \n",
       "4      22667  A strong woman oriented subject after long, di...      1   \n",
       "...      ...                                                ...    ...   \n",
       "24995   3287  This is the first Guinea Pig film from Japan a...      0   \n",
       "24996   4254  This was the WORST Christmas movie I ever saw....      0   \n",
       "24997   3674  I hated this show when I was a kid. That was b...      0   \n",
       "24998   4887  I was expecting the movie based on Grendel, th...      0   \n",
       "24999  21161  Pop quiz: you're a part of the modern armed fo...      1   \n",
       "\n",
       "        category  idx_2                                             text_2  \\\n",
       "0           easy     28  Some films that you pick up for a pound turn o...   \n",
       "1           easy  13182  Fascist principal Miss Togar(Mary Woronov, who...   \n",
       "2           easy  24303  Ya know when one looks at this Brian DePalma f...   \n",
       "3      ambiguous   8080  The mood of this movie is pretty good and it c...   \n",
       "4      ambiguous  16542  A complex story laid on the background of part...   \n",
       "...          ...    ...                                                ...   \n",
       "24995       easy  12808  This movie of 370 minutes was aired by the Ita...   \n",
       "24996       easy  11730  A fun concept, but poorly executed. Except for...   \n",
       "24997  ambiguous  18066  Cheech & Chong's Next Movie (1980) was the sec...   \n",
       "24998       easy  18086  The silent film the Pride of the Clan starring...   \n",
       "24999  ambiguous   5220  'Blue Desert' may have had the potential to be...   \n",
       "\n",
       "       label_2 category_2  \n",
       "0            0       easy  \n",
       "1            1  ambiguous  \n",
       "2            1  ambiguous  \n",
       "3            0  ambiguous  \n",
       "4            1  ambiguous  \n",
       "...        ...        ...  \n",
       "24995        1  ambiguous  \n",
       "24996        0       easy  \n",
       "24997        1  ambiguous  \n",
       "24998        1  ambiguous  \n",
       "24999        0  ambiguous  \n",
       "\n",
       "[25000 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_mixup_dataset = prepare_dataset_random_mixup(df, info_data=info_df, use_label=False)\n",
    "get_count(random_mixup_dataset, 'label', 'category')\n",
    "random_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ecacf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.82956\n",
      "same category:  0.53616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7731</td>\n",
       "      <td>I remember watching ATTACK when it first came ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>5961</td>\n",
       "      <td>If this is supposed to be the black experience...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17744</td>\n",
       "      <td>An unmarried, twenty-something hick (played by...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>21644</td>\n",
       "      <td>Playing a character from a literary classic ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13968</td>\n",
       "      <td>He's stocky, sweaty, slightly cross-eyed and r...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>15257</td>\n",
       "      <td>Imagine turning out the lights in your remote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21265</td>\n",
       "      <td>I meant that in a GOOD way, believe me. True t...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>15926</td>\n",
       "      <td>While Rome goes mad celebrating Hitler's visit...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15037</td>\n",
       "      <td>Cinematography--Compared to 'The Wrestler,' a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>5021</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;Whether any indictment was intende...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>23329</td>\n",
       "      <td>I'm on the opposite end of the previous commen...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>19209</td>\n",
       "      <td>I used to write comments at IMDb, but I don't ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>345</td>\n",
       "      <td>SHALLOW GRAVE begins with either a tribute or ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>7298</td>\n",
       "      <td>Watching beautiful women sneaking around, play...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>12900</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;There is STAR TREK canon -- lots o...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>23936</td>\n",
       "      <td>Zero day has a purpose and this is not simply ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>5134</td>\n",
       "      <td>Pretty awful but watchable and entertaining. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>4324</td>\n",
       "      <td>1 out of 10.&lt;br /&gt;&lt;br /&gt;This is the kind of mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>2313</td>\n",
       "      <td>This movie does not really promote kids to be ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>5060</td>\n",
       "      <td>I'm both amused and disgusted by the people wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0       7731  I remember watching ATTACK when it first came ...      0   \n",
       "1      17744  An unmarried, twenty-something hick (played by...      1   \n",
       "2      13968  He's stocky, sweaty, slightly cross-eyed and r...      1   \n",
       "3      21265  I meant that in a GOOD way, believe me. True t...      1   \n",
       "4      15037  Cinematography--Compared to 'The Wrestler,' a ...      1   \n",
       "...      ...                                                ...    ...   \n",
       "24995  23329  I'm on the opposite end of the previous commen...      1   \n",
       "24996    345  SHALLOW GRAVE begins with either a tribute or ...      0   \n",
       "24997  12900  <br /><br />There is STAR TREK canon -- lots o...      1   \n",
       "24998   5134  Pretty awful but watchable and entertaining. I...      0   \n",
       "24999   2313  This movie does not really promote kids to be ...      0   \n",
       "\n",
       "        category  idx_2                                             text_2  \\\n",
       "0           easy   5961  If this is supposed to be the black experience...   \n",
       "1      ambiguous  21644  Playing a character from a literary classic ca...   \n",
       "2      ambiguous  15257  Imagine turning out the lights in your remote ...   \n",
       "3      ambiguous  15926  While Rome goes mad celebrating Hitler's visit...   \n",
       "4      ambiguous   5021  <br /><br />Whether any indictment was intende...   \n",
       "...          ...    ...                                                ...   \n",
       "24995  ambiguous  19209  I used to write comments at IMDb, but I don't ...   \n",
       "24996  ambiguous   7298  Watching beautiful women sneaking around, play...   \n",
       "24997  ambiguous  23936  Zero day has a purpose and this is not simply ...   \n",
       "24998  ambiguous   4324  1 out of 10.<br /><br />This is the kind of mo...   \n",
       "24999       easy   5060  I'm both amused and disgusted by the people wh...   \n",
       "\n",
       "       label_2 category_2  \n",
       "0            0  ambiguous  \n",
       "1            1  ambiguous  \n",
       "2            1  ambiguous  \n",
       "3            1  ambiguous  \n",
       "4            0  ambiguous  \n",
       "...        ...        ...  \n",
       "24995        1  ambiguous  \n",
       "24996        0  ambiguous  \n",
       "24997        1  ambiguous  \n",
       "24998        0       easy  \n",
       "24999        0  ambiguous  \n",
       "\n",
       "[25000 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_mixup_dataset = prepare_dataset_random_mixup(df, info_data=info_df, use_label=True)\n",
    "get_count(random_mixup_dataset, 'label', 'category')\n",
    "random_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dca96ef5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m category_mixup_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset_category_mixup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_entropy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m get_count(category_mixup_dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m category_mixup_dataset\n",
      "Cell \u001b[0;32mIn[19], line 74\u001b[0m, in \u001b[0;36mprepare_dataset_category_mixup\u001b[0;34m(data, info_data, use_label, use_entropy)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Easy-Ambi Mixup\u001b[39;00m\n\u001b[1;32m     73\u001b[0m easy_ambiguous_len \u001b[38;5;241m=\u001b[39m data_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(final_data)\n\u001b[0;32m---> 74\u001b[0m easy_ambiguous_data \u001b[38;5;241m=\u001b[39m \u001b[43measy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43measy_ambiguous_len\u001b[49m\u001b[43m)\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m easy_ambiguous_temp \u001b[38;5;241m=\u001b[39m ambiguous_data\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39measy_ambiguous_len)[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     76\u001b[0m easy_ambiguous_temp \u001b[38;5;241m=\u001b[39m easy_ambiguous_temp\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_2\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m/projects/metis2/atharvak/miniconda3/envs/11711_env/lib/python3.10/site-packages/pandas/core/generic.py:5773\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5771\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 5773\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5774\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   5776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m/projects/metis2/atharvak/miniconda3/envs/11711_env/lib/python3.10/site-packages/pandas/core/sample.py:150\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    151\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    152\u001b[0m )\n",
      "File \u001b[0;32mmtrand.pyx:965\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, use_label=False, use_entropy=False)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a0c04df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.82976\n",
      "same category:  0.83052\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9662</td>\n",
       "      <td>Interesting? Hardly. The 'scientific evidence'...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>5958</td>\n",
       "      <td>A black guy fights ..... and supposedly wins ....</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7507</td>\n",
       "      <td>Probably the only thing that got the movie up ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>5482</td>\n",
       "      <td>I think this is one of the weakest of the Kenn...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16012</td>\n",
       "      <td>Scott Henderson (Alan Curtis) meets a mystery ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>7759</td>\n",
       "      <td>....this mini does not get better with age. I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18021</td>\n",
       "      <td>It took a long time until I could find the tit...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>15581</td>\n",
       "      <td>This is your standard musical comedy from the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8846</td>\n",
       "      <td>I must admit, there are few books with corresp...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>10887</td>\n",
       "      <td>Words cannot describe how utterly abysmal this...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>8012</td>\n",
       "      <td>I'm sure this was one of those \"WOAH!\" attract...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>20496</td>\n",
       "      <td>This movie is great entertainment to watch wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>16559</td>\n",
       "      <td>'The Mother' is that extraordinary piece of fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>23995</td>\n",
       "      <td>An excellent family movie... gives a lot to th...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>14413</td>\n",
       "      <td>Saw this Saturday night at the Provincetown Fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>19821</td>\n",
       "      <td>Xizao is a rare little movie. It is simple and...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>19143</td>\n",
       "      <td>To be honest, I had to go see this movie backw...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>10197</td>\n",
       "      <td>how can this movie have a 5.5 this movie was a...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>22440</td>\n",
       "      <td>I have seen Slaughter High several times over ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>19180</td>\n",
       "      <td>WHERE THE SIDEWALK ENDS deserves to be a bette...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0       9662  Interesting? Hardly. The 'scientific evidence'...      0   \n",
       "1       7507  Probably the only thing that got the movie up ...      0   \n",
       "2      16012  Scott Henderson (Alan Curtis) meets a mystery ...      1   \n",
       "3      18021  It took a long time until I could find the tit...      1   \n",
       "4       8846  I must admit, there are few books with corresp...      0   \n",
       "...      ...                                                ...    ...   \n",
       "24995   8012  I'm sure this was one of those \"WOAH!\" attract...      0   \n",
       "24996  16559  'The Mother' is that extraordinary piece of fi...      1   \n",
       "24997  14413  Saw this Saturday night at the Provincetown Fi...      1   \n",
       "24998  19143  To be honest, I had to go see this movie backw...      1   \n",
       "24999  22440  I have seen Slaughter High several times over ...      1   \n",
       "\n",
       "        category  idx_2                                             text_2  \\\n",
       "0           easy   5958  A black guy fights ..... and supposedly wins ....   \n",
       "1      ambiguous   5482  I think this is one of the weakest of the Kenn...   \n",
       "2      ambiguous   7759  ....this mini does not get better with age. I ...   \n",
       "3      ambiguous  15581  This is your standard musical comedy from the ...   \n",
       "4           easy  10887  Words cannot describe how utterly abysmal this...   \n",
       "...          ...    ...                                                ...   \n",
       "24995       easy  20496  This movie is great entertainment to watch wit...   \n",
       "24996       easy  23995  An excellent family movie... gives a lot to th...   \n",
       "24997       easy  19821  Xizao is a rare little movie. It is simple and...   \n",
       "24998  ambiguous  10197  how can this movie have a 5.5 this movie was a...   \n",
       "24999  ambiguous  19180  WHERE THE SIDEWALK ENDS deserves to be a bette...   \n",
       "\n",
       "       label_2 category_2  \n",
       "0            0       easy  \n",
       "1            0  ambiguous  \n",
       "2            0       easy  \n",
       "3            1  ambiguous  \n",
       "4            0       easy  \n",
       "...        ...        ...  \n",
       "24995        1       easy  \n",
       "24996        1       easy  \n",
       "24997        1       easy  \n",
       "24998        0       easy  \n",
       "24999        1  ambiguous  \n",
       "\n",
       "[25000 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, use_label=True, use_entropy=False)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f960f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "same_easy         2833\n",
       "same_ambiguous    2833\n",
       "easy_ambiguous    1417\n",
       "ambiguous_easy    1417\n",
       "Name: mixup_type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixup_dataset['mixup_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "715a2cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>sm</th>\n",
       "      <th>en</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>mixup_type</th>\n",
       "      <th>unique_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17484</td>\n",
       "      <td>It takes a Serbian or at least a Balkan famili...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[9.55682524e-04 9.99044317e-01]</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>16986</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same</td>\n",
       "      <td>16986_17484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7166</td>\n",
       "      <td>What else can you say about this movie,except ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[9.99527225e-01 4.72774999e-04]</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>2838</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>different</td>\n",
       "      <td>2838_7166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16389</td>\n",
       "      <td>Passionate, dramatic, riveting as Flamenco its...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.00268855 0.99731145]</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>1789</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same</td>\n",
       "      <td>1789_16389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1891</td>\n",
       "      <td>Such is the dilemma(above) that Debbie must fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.76936242 0.23063758]</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>1381</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same</td>\n",
       "      <td>1381_1891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18374</td>\n",
       "      <td>Since I am so interested in lake monsters i re...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.0023722 0.9976278]</td>\n",
       "      <td>0.016707</td>\n",
       "      <td>10354</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>different</td>\n",
       "      <td>10354_18374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32995</th>\n",
       "      <td>20219</td>\n",
       "      <td>This gloriously turgid melodrama represents Do...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[7.48858525e-04 9.99251141e-01]</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>2190</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same</td>\n",
       "      <td>2190_20219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32996</th>\n",
       "      <td>8508</td>\n",
       "      <td>I was so disappointed by this show. After hear...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[9.99726178e-01 2.73821708e-04]</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>6912</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same</td>\n",
       "      <td>6912_8508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32997</th>\n",
       "      <td>133</td>\n",
       "      <td>Having seen three other versions of the same f...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[9.99084106e-01 9.15893645e-04]</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>9262</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>different</td>\n",
       "      <td>133_9262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32998</th>\n",
       "      <td>21351</td>\n",
       "      <td>The autobiography on which this movie is based...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.00133933 0.99866067]</td>\n",
       "      <td>0.010199</td>\n",
       "      <td>8020</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same</td>\n",
       "      <td>8020_21351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32999</th>\n",
       "      <td>7008</td>\n",
       "      <td>This movie is deeply idiotic. A man wants reve...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[9.99664432e-01 3.35567602e-04]</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>4473</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same</td>\n",
       "      <td>4473_7008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0      17484  It takes a Serbian or at least a Balkan famili...      1   \n",
       "1       7166  What else can you say about this movie,except ...      0   \n",
       "2      16389  Passionate, dramatic, riveting as Flamenco its...      1   \n",
       "3       1891  Such is the dilemma(above) that Debbie must fa...      0   \n",
       "4      18374  Since I am so interested in lake monsters i re...      1   \n",
       "...      ...                                                ...    ...   \n",
       "32995  20219  This gloriously turgid melodrama represents Do...      1   \n",
       "32996   8508  I was so disappointed by this show. After hear...      0   \n",
       "32997    133  Having seen three other versions of the same f...      0   \n",
       "32998  21351  The autobiography on which this movie is based...      1   \n",
       "32999   7008  This movie is deeply idiotic. A man wants reve...      0   \n",
       "\n",
       "        category                               sm        en  idx_2  label_2  \\\n",
       "0      ambiguous  [9.55682524e-04 9.99044317e-01]  0.007600  16986        1   \n",
       "1           easy  [9.99527225e-01 4.72774999e-04]  0.004093   2838        0   \n",
       "2           easy          [0.00268855 0.99731145]  0.018598   1789        0   \n",
       "3      ambiguous          [0.76936242 0.23063758]  0.540046   1381        0   \n",
       "4      ambiguous            [0.0023722 0.9976278]  0.016707  10354        0   \n",
       "...          ...                              ...       ...    ...      ...   \n",
       "32995  ambiguous  [7.48858525e-04 9.99251141e-01]  0.006138   2190        0   \n",
       "32996       easy  [9.99726178e-01 2.73821708e-04]  0.002520   6912        0   \n",
       "32997  ambiguous  [9.99084106e-01 9.15893645e-04]  0.007323   9262        0   \n",
       "32998       easy          [0.00133933 0.99866067]  0.010199   8020        0   \n",
       "32999       easy  [9.99664432e-01 3.35567602e-04]  0.003020   4473        0   \n",
       "\n",
       "      category_2 mixup_type  unique_pair  \n",
       "0      ambiguous       same  16986_17484  \n",
       "1      ambiguous  different    2838_7166  \n",
       "2           easy       same   1789_16389  \n",
       "3      ambiguous       same    1381_1891  \n",
       "4           easy  different  10354_18374  \n",
       "...          ...        ...          ...  \n",
       "32995  ambiguous       same   2190_20219  \n",
       "32996       easy       same    6912_8508  \n",
       "32997       easy  different     133_9262  \n",
       "32998       easy       same   8020_21351  \n",
       "32999       easy       same    4473_7008  \n",
       "\n",
       "[33000 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixup_dataset['unique_pair'] = mixup_dataset.apply(lambda x: \"_\".join(str(index) for index in sorted([x.idx, x.idx_2])), axis=1)\n",
    "mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852c117d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>mixup_type</th>\n",
       "      <th>unique_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24929</td>\n",
       "      <td>Well, this film is a difficult one really. To ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>10558</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same</td>\n",
       "      <td>10558_24929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17556</td>\n",
       "      <td>I didn't expect much when I first saw the DVD ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>1515</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>different</td>\n",
       "      <td>1515_17556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8880</td>\n",
       "      <td>This is one of those horror flicks where twent...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>8804</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>different</td>\n",
       "      <td>8804_8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>513</td>\n",
       "      <td>The Good Earth is perhaps the most boring film...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>21561</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>different</td>\n",
       "      <td>513_21561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22906</td>\n",
       "      <td>Forget the campy 'religious' movies that have ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>5824</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>different</td>\n",
       "      <td>5824_22906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32995</th>\n",
       "      <td>2378</td>\n",
       "      <td>Let's hope this is the final nightmare. This i...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>12304</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>different</td>\n",
       "      <td>2378_12304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32996</th>\n",
       "      <td>22330</td>\n",
       "      <td>...in our household. Like everyone else who ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>10697</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>different</td>\n",
       "      <td>10697_22330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32997</th>\n",
       "      <td>15119</td>\n",
       "      <td>I think Lion King 1 1/2 is one of the best seq...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>5661</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same</td>\n",
       "      <td>5661_15119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32998</th>\n",
       "      <td>24347</td>\n",
       "      <td>Time for a rant, eh: I thought Spirit was a gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>11618</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>different</td>\n",
       "      <td>11618_24347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32999</th>\n",
       "      <td>14858</td>\n",
       "      <td>I have waited a long time for someone to film ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easy</td>\n",
       "      <td>15617</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>different</td>\n",
       "      <td>14858_15617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32998 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  label  \\\n",
       "0      24929  Well, this film is a difficult one really. To ...      1   \n",
       "1      17556  I didn't expect much when I first saw the DVD ...      1   \n",
       "2       8880  This is one of those horror flicks where twent...      0   \n",
       "3        513  The Good Earth is perhaps the most boring film...      0   \n",
       "4      22906  Forget the campy 'religious' movies that have ...      1   \n",
       "...      ...                                                ...    ...   \n",
       "32995   2378  Let's hope this is the final nightmare. This i...      0   \n",
       "32996  22330  ...in our household. Like everyone else who ha...      1   \n",
       "32997  15119  I think Lion King 1 1/2 is one of the best seq...      1   \n",
       "32998  24347  Time for a rant, eh: I thought Spirit was a gr...      1   \n",
       "32999  14858  I have waited a long time for someone to film ...      1   \n",
       "\n",
       "        category  idx_2  label_2 category_2 mixup_type  unique_pair  \n",
       "0      ambiguous  10558        0  ambiguous       same  10558_24929  \n",
       "1           easy   1515        0  ambiguous  different   1515_17556  \n",
       "2      ambiguous   8804        0       easy  different    8804_8880  \n",
       "3           easy  21561        1  ambiguous  different    513_21561  \n",
       "4      ambiguous   5824        0       easy  different   5824_22906  \n",
       "...          ...    ...      ...        ...        ...          ...  \n",
       "32995       easy  12304        0  ambiguous  different   2378_12304  \n",
       "32996  ambiguous  10697        0       easy  different  10697_22330  \n",
       "32997       easy   5661        0       easy       same   5661_15119  \n",
       "32998  ambiguous  11618        0       easy  different  11618_24347  \n",
       "32999       easy  15617        1  ambiguous  different  14858_15617  \n",
       "\n",
       "[32998 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixup_dataset.drop_duplicates(subset=[\"unique_pair\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6fef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixupDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        data: pd.DataFrame,\n",
    "        sampling_type: str\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.sampling_type = sampling_type\n",
    "        \n",
    "        if self.sampling_type == 'sequential':\n",
    "            sorting_dict = {\n",
    "                'non_mixup_easy': 0,\n",
    "                'non_mixup_ambiguous': 1,\n",
    "                'same_easy': 2,\n",
    "                'different_easy': 3,\n",
    "                'same_ambiguous': 4,\n",
    "                'different_ambiguous': 5\n",
    "            }\n",
    "            self.data['data_type'] = self.data['mixup_type'] + '_' + self.data['category']\n",
    "            self.data = self.data.iloc[self.data.data_type.map(sorting_dict).argsort()].reset_index(drop=True)\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenizer.batch_encode_plus(\n",
    "            self.data[INPUT_COLUMN].tolist(),\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,            \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "\n",
    "                \n",
    "    def __len__(\n",
    "        self\n",
    "    ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        data = {\n",
    "            'input_ids_1': self.tokenized_data['input_ids'][index].flatten(),\n",
    "            'attention_mask_1': self.tokenized_data['attention_mask'][index].flatten(),\n",
    "            'labels_1': torch.tensor(self.data.iloc[index][OUTPUT_COLUMN], dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        idx2 = self.data.iloc[index]['idx_2']\n",
    "\n",
    "        index2 = self.data[self.data['idx'] == idx2].index[0]\n",
    "        data['input_ids_2'] = self.tokenized_data['input_ids'][index2].flatten()\n",
    "        data['attention_mask_2'] = self.tokenized_data['attention_mask'][index2].flatten()\n",
    "        data['labels_2'] = torch.tensor(self.data.iloc[index2][OUTPUT_COLUMN], dtype=torch.long)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f241da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MixupDataset(tokenizer=tokenizer, data=dataset, sampling_type='random')\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6631d2",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e8145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, optimizer, device, train_loader, num_epochs, output_dir):\n",
    "    losses = []\n",
    "    train_iterator = trange(int(num_epochs), desc='Epoch')\n",
    "    for _ in train_iterator:\n",
    "        tr_loss = 0\n",
    "        step = None\n",
    "        epoch_iterator = tqdm(train_loader, desc='Training')\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "\n",
    "            inputs = {}\n",
    "            for k, v in batch.items():\n",
    "                if isinstance(v, list):\n",
    "                    inputs[k] = None\n",
    "                else:\n",
    "                    inputs[k] = v.to(device)\n",
    "\n",
    "            labels = inputs['labels_1']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            out = outputs['logits'].double().to(device)\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            tr_loss += loss.item()\n",
    "        losses.append(tr_loss/(step+1))\n",
    "        print('train loss: {}'.format(tr_loss/(step+1)))\n",
    "\n",
    "    # save model and tokenizer\n",
    "    print('Saving model and tokenizer')\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69183448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, eval_loader, device, with_labels=True):\n",
    "    probs = None\n",
    "    gold_labels = None\n",
    "\n",
    "    eval_loss = 0\n",
    "    step = None\n",
    "    eval_iterator = tqdm(eval_loader, desc='Evaluating')\n",
    "    for step, batch in enumerate(eval_iterator):\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            inputs = {k:v.to(device) for k, v in batch.items()}\n",
    "            labels = inputs['labels_1']\n",
    "            del inputs['labels_1']\n",
    "            del inputs['labels_2']\n",
    " \n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            out = outputs['logits'].double().to(device)\n",
    "            out = F.softmax(out, dim=1)\n",
    "\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            if probs is None:\n",
    "                probs = out.detach().cpu().numpy()\n",
    "                if with_labels:\n",
    "                    gold_labels = labels.detach().cpu().numpy()\n",
    "            else:\n",
    "                probs = np.append(probs, out.detach().cpu().numpy(), axis=0)\n",
    "                if with_labels:\n",
    "                    gold_labels = np.append(gold_labels, labels.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            if with_labels:\n",
    "                eval_loss += loss.item()\n",
    "    \n",
    "    if with_labels:\n",
    "        eval_loss /= (step+1)\n",
    "        print('eval loss: {}'.format(eval_loss))\n",
    "\n",
    "        # compute accuracy\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        accuracy = np.sum(preds == gold_labels)/len(preds)\n",
    "        print('eval accuracy: {}'.format(accuracy))\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b1df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a6f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 8250, 8250, 8250, 0.33)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/imdb/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_imdb = json.load(f)\n",
    "f.close()\n",
    "print(data_imdb.keys())\n",
    "\n",
    "df_imdb = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/imdb/train.csv')\n",
    "len(df_imdb), len(data_imdb['easy']), len(data_imdb['ambiguous']), len(data_imdb['hard']), len(data_imdb['easy'])/len(df_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01fad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " set(),\n",
       " {0,\n",
       "  13,\n",
       "  14,\n",
       "  20,\n",
       "  23,\n",
       "  32,\n",
       "  39,\n",
       "  48,\n",
       "  50,\n",
       "  52,\n",
       "  53,\n",
       "  55,\n",
       "  61,\n",
       "  65,\n",
       "  69,\n",
       "  70,\n",
       "  74,\n",
       "  77,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  84,\n",
       "  87,\n",
       "  89,\n",
       "  90,\n",
       "  92,\n",
       "  97,\n",
       "  98,\n",
       "  105,\n",
       "  108,\n",
       "  109,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  130,\n",
       "  132,\n",
       "  133,\n",
       "  136,\n",
       "  138,\n",
       "  142,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  163,\n",
       "  167,\n",
       "  168,\n",
       "  171,\n",
       "  173,\n",
       "  179,\n",
       "  180,\n",
       "  188,\n",
       "  190,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  224,\n",
       "  228,\n",
       "  233,\n",
       "  236,\n",
       "  245,\n",
       "  246,\n",
       "  252,\n",
       "  255,\n",
       "  257,\n",
       "  260,\n",
       "  263,\n",
       "  264,\n",
       "  267,\n",
       "  271,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  285,\n",
       "  289,\n",
       "  292,\n",
       "  299,\n",
       "  301,\n",
       "  304,\n",
       "  306,\n",
       "  313,\n",
       "  318,\n",
       "  336,\n",
       "  339,\n",
       "  340,\n",
       "  342,\n",
       "  345,\n",
       "  346,\n",
       "  348,\n",
       "  354,\n",
       "  362,\n",
       "  366,\n",
       "  367,\n",
       "  371,\n",
       "  373,\n",
       "  375,\n",
       "  377,\n",
       "  378,\n",
       "  384,\n",
       "  387,\n",
       "  394,\n",
       "  397,\n",
       "  403,\n",
       "  405,\n",
       "  406,\n",
       "  408,\n",
       "  410,\n",
       "  413,\n",
       "  418,\n",
       "  419,\n",
       "  421,\n",
       "  427,\n",
       "  430,\n",
       "  440,\n",
       "  441,\n",
       "  446,\n",
       "  455,\n",
       "  457,\n",
       "  459,\n",
       "  461,\n",
       "  464,\n",
       "  469,\n",
       "  471,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  481,\n",
       "  483,\n",
       "  488,\n",
       "  492,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  501,\n",
       "  504,\n",
       "  509,\n",
       "  511,\n",
       "  515,\n",
       "  516,\n",
       "  518,\n",
       "  522,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  532,\n",
       "  540,\n",
       "  543,\n",
       "  544,\n",
       "  552,\n",
       "  553,\n",
       "  564,\n",
       "  567,\n",
       "  569,\n",
       "  587,\n",
       "  591,\n",
       "  593,\n",
       "  603,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  614,\n",
       "  615,\n",
       "  618,\n",
       "  623,\n",
       "  624,\n",
       "  628,\n",
       "  634,\n",
       "  642,\n",
       "  650,\n",
       "  653,\n",
       "  655,\n",
       "  671,\n",
       "  672,\n",
       "  685,\n",
       "  687,\n",
       "  688,\n",
       "  695,\n",
       "  699,\n",
       "  701,\n",
       "  704,\n",
       "  706,\n",
       "  707,\n",
       "  710,\n",
       "  712,\n",
       "  721,\n",
       "  724,\n",
       "  726,\n",
       "  727,\n",
       "  737,\n",
       "  739,\n",
       "  742,\n",
       "  744,\n",
       "  750,\n",
       "  751,\n",
       "  753,\n",
       "  758,\n",
       "  766,\n",
       "  768,\n",
       "  772,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  781,\n",
       "  783,\n",
       "  784,\n",
       "  788,\n",
       "  790,\n",
       "  791,\n",
       "  793,\n",
       "  794,\n",
       "  802,\n",
       "  815,\n",
       "  816,\n",
       "  825,\n",
       "  827,\n",
       "  828,\n",
       "  832,\n",
       "  835,\n",
       "  838,\n",
       "  845,\n",
       "  848,\n",
       "  853,\n",
       "  858,\n",
       "  860,\n",
       "  863,\n",
       "  865,\n",
       "  868,\n",
       "  871,\n",
       "  872,\n",
       "  876,\n",
       "  877,\n",
       "  879,\n",
       "  885,\n",
       "  886,\n",
       "  888,\n",
       "  891,\n",
       "  893,\n",
       "  895,\n",
       "  896,\n",
       "  899,\n",
       "  905,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  918,\n",
       "  922,\n",
       "  924,\n",
       "  926,\n",
       "  930,\n",
       "  932,\n",
       "  939,\n",
       "  954,\n",
       "  964,\n",
       "  965,\n",
       "  972,\n",
       "  974,\n",
       "  976,\n",
       "  990,\n",
       "  993,\n",
       "  994,\n",
       "  1001,\n",
       "  1009,\n",
       "  1015,\n",
       "  1022,\n",
       "  1026,\n",
       "  1029,\n",
       "  1031,\n",
       "  1034,\n",
       "  1035,\n",
       "  1038,\n",
       "  1040,\n",
       "  1050,\n",
       "  1073,\n",
       "  1075,\n",
       "  1079,\n",
       "  1081,\n",
       "  1082,\n",
       "  1092,\n",
       "  1097,\n",
       "  1106,\n",
       "  1107,\n",
       "  1112,\n",
       "  1114,\n",
       "  1117,\n",
       "  1119,\n",
       "  1121,\n",
       "  1123,\n",
       "  1124,\n",
       "  1125,\n",
       "  1127,\n",
       "  1131,\n",
       "  1142,\n",
       "  1145,\n",
       "  1146,\n",
       "  1152,\n",
       "  1154,\n",
       "  1162,\n",
       "  1166,\n",
       "  1168,\n",
       "  1170,\n",
       "  1175,\n",
       "  1177,\n",
       "  1181,\n",
       "  1187,\n",
       "  1197,\n",
       "  1200,\n",
       "  1204,\n",
       "  1210,\n",
       "  1212,\n",
       "  1214,\n",
       "  1221,\n",
       "  1224,\n",
       "  1225,\n",
       "  1227,\n",
       "  1233,\n",
       "  1242,\n",
       "  1244,\n",
       "  1246,\n",
       "  1251,\n",
       "  1256,\n",
       "  1261,\n",
       "  1263,\n",
       "  1267,\n",
       "  1270,\n",
       "  1275,\n",
       "  1277,\n",
       "  1279,\n",
       "  1281,\n",
       "  1282,\n",
       "  1283,\n",
       "  1285,\n",
       "  1289,\n",
       "  1293,\n",
       "  1296,\n",
       "  1297,\n",
       "  1306,\n",
       "  1308,\n",
       "  1310,\n",
       "  1318,\n",
       "  1321,\n",
       "  1331,\n",
       "  1336,\n",
       "  1340,\n",
       "  1341,\n",
       "  1345,\n",
       "  1347,\n",
       "  1348,\n",
       "  1349,\n",
       "  1351,\n",
       "  1353,\n",
       "  1362,\n",
       "  1367,\n",
       "  1370,\n",
       "  1371,\n",
       "  1372,\n",
       "  1375,\n",
       "  1376,\n",
       "  1377,\n",
       "  1381,\n",
       "  1386,\n",
       "  1390,\n",
       "  1391,\n",
       "  1400,\n",
       "  1401,\n",
       "  1419,\n",
       "  1421,\n",
       "  1422,\n",
       "  1434,\n",
       "  1457,\n",
       "  1466,\n",
       "  1467,\n",
       "  1469,\n",
       "  1477,\n",
       "  1478,\n",
       "  1485,\n",
       "  1493,\n",
       "  1496,\n",
       "  1498,\n",
       "  1499,\n",
       "  1508,\n",
       "  1510,\n",
       "  1511,\n",
       "  1515,\n",
       "  1518,\n",
       "  1523,\n",
       "  1525,\n",
       "  1526,\n",
       "  1530,\n",
       "  1533,\n",
       "  1534,\n",
       "  1539,\n",
       "  1550,\n",
       "  1565,\n",
       "  1568,\n",
       "  1570,\n",
       "  1578,\n",
       "  1582,\n",
       "  1583,\n",
       "  1584,\n",
       "  1585,\n",
       "  1586,\n",
       "  1590,\n",
       "  1594,\n",
       "  1599,\n",
       "  1603,\n",
       "  1606,\n",
       "  1612,\n",
       "  1613,\n",
       "  1614,\n",
       "  1616,\n",
       "  1622,\n",
       "  1643,\n",
       "  1646,\n",
       "  1648,\n",
       "  1651,\n",
       "  1654,\n",
       "  1655,\n",
       "  1661,\n",
       "  1664,\n",
       "  1666,\n",
       "  1668,\n",
       "  1673,\n",
       "  1674,\n",
       "  1675,\n",
       "  1680,\n",
       "  1681,\n",
       "  1686,\n",
       "  1691,\n",
       "  1701,\n",
       "  1705,\n",
       "  1711,\n",
       "  1716,\n",
       "  1722,\n",
       "  1723,\n",
       "  1724,\n",
       "  1729,\n",
       "  1737,\n",
       "  1740,\n",
       "  1755,\n",
       "  1757,\n",
       "  1760,\n",
       "  1773,\n",
       "  1776,\n",
       "  1779,\n",
       "  1782,\n",
       "  1783,\n",
       "  1784,\n",
       "  1788,\n",
       "  1790,\n",
       "  1791,\n",
       "  1792,\n",
       "  1793,\n",
       "  1796,\n",
       "  1797,\n",
       "  1799,\n",
       "  1810,\n",
       "  1811,\n",
       "  1822,\n",
       "  1823,\n",
       "  1831,\n",
       "  1833,\n",
       "  1835,\n",
       "  1841,\n",
       "  1842,\n",
       "  1850,\n",
       "  1851,\n",
       "  1852,\n",
       "  1853,\n",
       "  1856,\n",
       "  1861,\n",
       "  1866,\n",
       "  1867,\n",
       "  1870,\n",
       "  1879,\n",
       "  1891,\n",
       "  1894,\n",
       "  1895,\n",
       "  1897,\n",
       "  1899,\n",
       "  1903,\n",
       "  1905,\n",
       "  1907,\n",
       "  1908,\n",
       "  1910,\n",
       "  1917,\n",
       "  1931,\n",
       "  1937,\n",
       "  1940,\n",
       "  1947,\n",
       "  1949,\n",
       "  1955,\n",
       "  1959,\n",
       "  1961,\n",
       "  1963,\n",
       "  1964,\n",
       "  1968,\n",
       "  1969,\n",
       "  1974,\n",
       "  1984,\n",
       "  1986,\n",
       "  1993,\n",
       "  1994,\n",
       "  1997,\n",
       "  1999,\n",
       "  2000,\n",
       "  2004,\n",
       "  2008,\n",
       "  2013,\n",
       "  2015,\n",
       "  2016,\n",
       "  2021,\n",
       "  2025,\n",
       "  2028,\n",
       "  2029,\n",
       "  2033,\n",
       "  2034,\n",
       "  2039,\n",
       "  2040,\n",
       "  2042,\n",
       "  2043,\n",
       "  2044,\n",
       "  2046,\n",
       "  2052,\n",
       "  2053,\n",
       "  2056,\n",
       "  2057,\n",
       "  2066,\n",
       "  2073,\n",
       "  2079,\n",
       "  2080,\n",
       "  2081,\n",
       "  2083,\n",
       "  2085,\n",
       "  2090,\n",
       "  2096,\n",
       "  2097,\n",
       "  2101,\n",
       "  2103,\n",
       "  2104,\n",
       "  2106,\n",
       "  2110,\n",
       "  2111,\n",
       "  2112,\n",
       "  2113,\n",
       "  2114,\n",
       "  2115,\n",
       "  2116,\n",
       "  2122,\n",
       "  2123,\n",
       "  2126,\n",
       "  2128,\n",
       "  2129,\n",
       "  2132,\n",
       "  2144,\n",
       "  2154,\n",
       "  2168,\n",
       "  2169,\n",
       "  2175,\n",
       "  2177,\n",
       "  2178,\n",
       "  2181,\n",
       "  2182,\n",
       "  2183,\n",
       "  2187,\n",
       "  2188,\n",
       "  2190,\n",
       "  2192,\n",
       "  2195,\n",
       "  2196,\n",
       "  2198,\n",
       "  2199,\n",
       "  2201,\n",
       "  2202,\n",
       "  2203,\n",
       "  2222,\n",
       "  2223,\n",
       "  2225,\n",
       "  2226,\n",
       "  2229,\n",
       "  2233,\n",
       "  2235,\n",
       "  2239,\n",
       "  2240,\n",
       "  2252,\n",
       "  2255,\n",
       "  2256,\n",
       "  2257,\n",
       "  2269,\n",
       "  2270,\n",
       "  2271,\n",
       "  2272,\n",
       "  2279,\n",
       "  2283,\n",
       "  2294,\n",
       "  2300,\n",
       "  2301,\n",
       "  2305,\n",
       "  2312,\n",
       "  2315,\n",
       "  2316,\n",
       "  2321,\n",
       "  2324,\n",
       "  2338,\n",
       "  2344,\n",
       "  2346,\n",
       "  2359,\n",
       "  2362,\n",
       "  2365,\n",
       "  2371,\n",
       "  2379,\n",
       "  2380,\n",
       "  2387,\n",
       "  2392,\n",
       "  2394,\n",
       "  2395,\n",
       "  2397,\n",
       "  2405,\n",
       "  2406,\n",
       "  2407,\n",
       "  2414,\n",
       "  2415,\n",
       "  2418,\n",
       "  2419,\n",
       "  2422,\n",
       "  2429,\n",
       "  2439,\n",
       "  2444,\n",
       "  2447,\n",
       "  2449,\n",
       "  2451,\n",
       "  2455,\n",
       "  2456,\n",
       "  2458,\n",
       "  2459,\n",
       "  2460,\n",
       "  2463,\n",
       "  2464,\n",
       "  2467,\n",
       "  2473,\n",
       "  2476,\n",
       "  2477,\n",
       "  2478,\n",
       "  2479,\n",
       "  2481,\n",
       "  2482,\n",
       "  2485,\n",
       "  2496,\n",
       "  2502,\n",
       "  2504,\n",
       "  2511,\n",
       "  2512,\n",
       "  2524,\n",
       "  2525,\n",
       "  2528,\n",
       "  2529,\n",
       "  2530,\n",
       "  2532,\n",
       "  2534,\n",
       "  2535,\n",
       "  2536,\n",
       "  2537,\n",
       "  2539,\n",
       "  2541,\n",
       "  2544,\n",
       "  2545,\n",
       "  2547,\n",
       "  2553,\n",
       "  2559,\n",
       "  2562,\n",
       "  2565,\n",
       "  2568,\n",
       "  2570,\n",
       "  2585,\n",
       "  2596,\n",
       "  2601,\n",
       "  2618,\n",
       "  2619,\n",
       "  2621,\n",
       "  2625,\n",
       "  2627,\n",
       "  2628,\n",
       "  2632,\n",
       "  2635,\n",
       "  2645,\n",
       "  2650,\n",
       "  2659,\n",
       "  2674,\n",
       "  2675,\n",
       "  2679,\n",
       "  2686,\n",
       "  2689,\n",
       "  2692,\n",
       "  2694,\n",
       "  2699,\n",
       "  2708,\n",
       "  2713,\n",
       "  2715,\n",
       "  2716,\n",
       "  2717,\n",
       "  2718,\n",
       "  2719,\n",
       "  2723,\n",
       "  2733,\n",
       "  2740,\n",
       "  2742,\n",
       "  2743,\n",
       "  2747,\n",
       "  2757,\n",
       "  2758,\n",
       "  2761,\n",
       "  2768,\n",
       "  2769,\n",
       "  2771,\n",
       "  2774,\n",
       "  2775,\n",
       "  2776,\n",
       "  2777,\n",
       "  2778,\n",
       "  2779,\n",
       "  2780,\n",
       "  2783,\n",
       "  2785,\n",
       "  2788,\n",
       "  2789,\n",
       "  2802,\n",
       "  2803,\n",
       "  2804,\n",
       "  2806,\n",
       "  2810,\n",
       "  2818,\n",
       "  2819,\n",
       "  2825,\n",
       "  2828,\n",
       "  2835,\n",
       "  2836,\n",
       "  2837,\n",
       "  2838,\n",
       "  2841,\n",
       "  2845,\n",
       "  2846,\n",
       "  2849,\n",
       "  2854,\n",
       "  2865,\n",
       "  2882,\n",
       "  2885,\n",
       "  2890,\n",
       "  2892,\n",
       "  2902,\n",
       "  2904,\n",
       "  2911,\n",
       "  2916,\n",
       "  2917,\n",
       "  2920,\n",
       "  2924,\n",
       "  2927,\n",
       "  2928,\n",
       "  2931,\n",
       "  2932,\n",
       "  2941,\n",
       "  2943,\n",
       "  2950,\n",
       "  2955,\n",
       "  2964,\n",
       "  2968,\n",
       "  2969,\n",
       "  2970,\n",
       "  2971,\n",
       "  2974,\n",
       "  2993,\n",
       "  3001,\n",
       "  3002,\n",
       "  3003,\n",
       "  3006,\n",
       "  3007,\n",
       "  3009,\n",
       "  3012,\n",
       "  3015,\n",
       "  3021,\n",
       "  3022,\n",
       "  3023,\n",
       "  3024,\n",
       "  3025,\n",
       "  3026,\n",
       "  3030,\n",
       "  3031,\n",
       "  3032,\n",
       "  3034,\n",
       "  3035,\n",
       "  3036,\n",
       "  3037,\n",
       "  3038,\n",
       "  3040,\n",
       "  3042,\n",
       "  3043,\n",
       "  3045,\n",
       "  3046,\n",
       "  3050,\n",
       "  3072,\n",
       "  3073,\n",
       "  3074,\n",
       "  3075,\n",
       "  3076,\n",
       "  3085,\n",
       "  3087,\n",
       "  3089,\n",
       "  3090,\n",
       "  3093,\n",
       "  3094,\n",
       "  3095,\n",
       "  3096,\n",
       "  3100,\n",
       "  3101,\n",
       "  3104,\n",
       "  3112,\n",
       "  3117,\n",
       "  3123,\n",
       "  3124,\n",
       "  3126,\n",
       "  3128,\n",
       "  3130,\n",
       "  3131,\n",
       "  3139,\n",
       "  3140,\n",
       "  3144,\n",
       "  3146,\n",
       "  3147,\n",
       "  3149,\n",
       "  3151,\n",
       "  3152,\n",
       "  3155,\n",
       "  3161,\n",
       "  3162,\n",
       "  3168,\n",
       "  3177,\n",
       "  3183,\n",
       "  3192,\n",
       "  3199,\n",
       "  3201,\n",
       "  3203,\n",
       "  3206,\n",
       "  3209,\n",
       "  3215,\n",
       "  3221,\n",
       "  3223,\n",
       "  3227,\n",
       "  3238,\n",
       "  3247,\n",
       "  3249,\n",
       "  3251,\n",
       "  3252,\n",
       "  3258,\n",
       "  3259,\n",
       "  3261,\n",
       "  3263,\n",
       "  3264,\n",
       "  3265,\n",
       "  3266,\n",
       "  3272,\n",
       "  3273,\n",
       "  3278,\n",
       "  3289,\n",
       "  3296,\n",
       "  3300,\n",
       "  3303,\n",
       "  3306,\n",
       "  3308,\n",
       "  3310,\n",
       "  3311,\n",
       "  3313,\n",
       "  3321,\n",
       "  3322,\n",
       "  3323,\n",
       "  3329,\n",
       "  3332,\n",
       "  3333,\n",
       "  3338,\n",
       "  3344,\n",
       "  3345,\n",
       "  3351,\n",
       "  3354,\n",
       "  3355,\n",
       "  3357,\n",
       "  3360,\n",
       "  3362,\n",
       "  3363,\n",
       "  3368,\n",
       "  3370,\n",
       "  3375,\n",
       "  3382,\n",
       "  3385,\n",
       "  3391,\n",
       "  3394,\n",
       "  3396,\n",
       "  3400,\n",
       "  3403,\n",
       "  3405,\n",
       "  3407,\n",
       "  3408,\n",
       "  3416,\n",
       "  3419,\n",
       "  3422,\n",
       "  3423,\n",
       "  3426,\n",
       "  3427,\n",
       "  3434,\n",
       "  3436,\n",
       "  3437,\n",
       "  3438,\n",
       "  3439,\n",
       "  3440,\n",
       "  3441,\n",
       "  3445,\n",
       "  3449,\n",
       "  3453,\n",
       "  3457,\n",
       "  3459,\n",
       "  3460,\n",
       "  3466,\n",
       "  3472,\n",
       "  3473,\n",
       "  3477,\n",
       "  3480,\n",
       "  3484,\n",
       "  3486,\n",
       "  3489,\n",
       "  3491,\n",
       "  3493,\n",
       "  3496,\n",
       "  3497,\n",
       "  3498,\n",
       "  3499,\n",
       "  3506,\n",
       "  3510,\n",
       "  3523,\n",
       "  3524,\n",
       "  3525,\n",
       "  3531,\n",
       "  3537,\n",
       "  3551,\n",
       "  3555,\n",
       "  3558,\n",
       "  3561,\n",
       "  3563,\n",
       "  3570,\n",
       "  3572,\n",
       "  3574,\n",
       "  3582,\n",
       "  3583,\n",
       "  3587,\n",
       "  3596,\n",
       "  3597,\n",
       "  3599,\n",
       "  3603,\n",
       "  3606,\n",
       "  3607,\n",
       "  3609,\n",
       "  3610,\n",
       "  3611,\n",
       "  3615,\n",
       "  3616,\n",
       "  3617,\n",
       "  3619,\n",
       "  3625,\n",
       "  3631,\n",
       "  3632,\n",
       "  3633,\n",
       "  3635,\n",
       "  3639,\n",
       "  3640,\n",
       "  3641,\n",
       "  3643,\n",
       "  3644,\n",
       "  3646,\n",
       "  3648,\n",
       "  3654,\n",
       "  3657,\n",
       "  3658,\n",
       "  3660,\n",
       "  3666,\n",
       "  3670,\n",
       "  3671,\n",
       "  3672,\n",
       "  3674,\n",
       "  3682,\n",
       "  3684,\n",
       "  3694,\n",
       "  3700,\n",
       "  3704,\n",
       "  3708,\n",
       "  3709,\n",
       "  3711,\n",
       "  3714,\n",
       "  3722,\n",
       "  3726,\n",
       "  3727,\n",
       "  3728,\n",
       "  3729,\n",
       "  3732,\n",
       "  ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_imdb['easy']).intersection(set(data_imdb['ambiguous'])), set(data_imdb['easy']).intersection(set(data_imdb['hard'])), set(data_imdb['hard']).intersection(set(data_imdb['ambiguous']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d082d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67349, 22225, 22225, 22225, 0.3299974758348305)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/sst2/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_sst2 = json.load(f)\n",
    "f.close()\n",
    "print(data_sst2.keys())\n",
    "df_sst2 = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/sst2/train.csv')\n",
    "len(df_sst2), len(data_sst2['easy']), len(data_sst2['ambiguous']), len(data_sst2['hard']), len(data_sst2['easy'])/len(df_sst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23f6304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(560000, 184800, 184800, 184800, 0.33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/yelp_polarity/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_yelp_polarity = json.load(f)\n",
    "f.close()\n",
    "print(data_yelp_polarity.keys())\n",
    "df_yelp_polarity = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/yelp_polarity/train.csv')\n",
    "len(df_yelp_polarity), len(data_yelp_polarity['easy']), len(data_yelp_polarity['ambiguous']), len(data_yelp_polarity['hard']), len(data_yelp_polarity['easy'])/len(df_yelp_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acbb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
