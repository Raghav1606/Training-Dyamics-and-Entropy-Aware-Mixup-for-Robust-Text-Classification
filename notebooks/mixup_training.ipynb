{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7016ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fire\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    set_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9defa",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f39739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_COLUMN:  text\n",
      "DATA_COLUMN:  category\n",
      "OUTPUT_COLUMN:  label\n",
      "NUM_EPOCHS:  2\n",
      "MAX_LEN:  256\n",
      "BATCH_SIZE:  32\n",
      "LAMBDA:  0.5\n",
      "FLAG:  False\n",
      "MIXUP_START:  10\n"
     ]
    }
   ],
   "source": [
    "FLAG = False\n",
    "MIXUP_START = 10\n",
    "LAMBDA = 0.5\n",
    "\n",
    "INPUT_COLUMN = 'text'\n",
    "DATA_COLUMN = 'category'\n",
    "OUTPUT_COLUMN = 'label'\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"INPUT_COLUMN: \", INPUT_COLUMN)\n",
    "print(\"DATA_COLUMN: \", DATA_COLUMN)\n",
    "print(\"OUTPUT_COLUMN: \", OUTPUT_COLUMN)\n",
    "print(\"NUM_EPOCHS: \", NUM_EPOCHS)\n",
    "print(\"MAX_LEN: \", MAX_LEN)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"LAMBDA: \", LAMBDA)\n",
    "print(\"FLAG: \", FLAG)\n",
    "print(\"MIXUP_START: \", MIXUP_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9b61a",
   "metadata": {},
   "source": [
    "# Mixup Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83ce9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaPreTrainedModel,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaPooler,\n",
    "    RobertaClassificationHead\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    SequenceClassifierOutput\n",
    ")\n",
    "\n",
    "# from config import *\n",
    "\n",
    "\n",
    "class RobertaMixerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class RobertaMixerModel(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n",
    "    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
    "    Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = RobertaMixerEncoder(config)\n",
    "\n",
    "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "\n",
    "            \n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class RobertaMixerForSequenceClassification(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaMixerModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        if FLAG:\n",
    "            self.mixup_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "            self.mixup_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "            self.mixup_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_1: Optional[torch.LongTensor] = None,\n",
    "        attention_mask_1: Optional[torch.FloatTensor] = None,\n",
    "        input_ids_2: Optional[torch.LongTensor] = None,\n",
    "        attention_mask_2: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels_1: Optional[torch.LongTensor] = None,\n",
    "        labels_2: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs_1 = self.roberta(\n",
    "            input_ids_1,\n",
    "            attention_mask=attention_mask_1,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output_1 = outputs_1[0]\n",
    "        \n",
    "        # Mixup train\n",
    "        if (input_ids_2 is not None) and (attention_mask_2 is not None) and (labels_2 is not None):\n",
    "            \n",
    "            outputs_2 = self.roberta(\n",
    "                input_ids_2,\n",
    "                attention_mask=attention_mask_2,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            sequence_output_2 = outputs_2[0]\n",
    "\n",
    "            sequence_output = (LAMBDA * sequence_output_1) + ((1 - LAMBDA) * sequence_output_2)\n",
    "\n",
    "            if FLAG:\n",
    "                sequence_output = self.mixup_dense(sequence_output)\n",
    "                sequence_output = self.mixup_layernorm(sequence_output)\n",
    "                sequence_output = self.mixup_dropout(sequence_output)\n",
    "\n",
    "            logits = self.classifier(sequence_output)\n",
    "\n",
    "            loss = None\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = (LAMBDA * loss_fct(logits.view(-1, self.num_labels), labels_1.view(-1))) + ((1 - LAMBDA) * loss_fct(logits.view(-1, self.num_labels), labels_2.view(-1)))\n",
    "\n",
    "        # Mixup eval\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output_1)\n",
    "            loss = None\n",
    "            if labels_1 is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels_1.view(-1))\n",
    "         \n",
    "        # Return logits, loss, and hidden states\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs_1[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs_1.hidden_states,\n",
    "            attentions=outputs_1.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1db9e",
   "metadata": {},
   "source": [
    "## MixupDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875aa2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a829e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(data, info_data=None, mixup=False):\n",
    "    \n",
    "#     # Remove none and hard examples\n",
    "#     data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "\n",
    "#     # Add softmax and entropy info\n",
    "#     if (info_data is not None) and mixup:\n",
    "#         data = pd.merge(data, info_data, on='idx')[['idx', 'text', 'label', 'category', 'softmax', 'entropy']]\n",
    "#         mixup_size = len(info_data) - len(data)\n",
    "\n",
    "#         # --------------------------------------- Same class mixup ---------------------------------------  \n",
    "\n",
    "#         # Easy-Easy Mixup\n",
    "#         easy_data = data[data['category'] == 'easy']\n",
    "#         easy_low_ent_idx = easy_data.sort_values('entropy', ascending=True).head(mixup_size//3)['idx'].tolist()\n",
    "#         easy_high_ent_idx = easy_data.sort_values('entropy', ascending=False).head(mixup_size//3)['idx'].tolist()\n",
    "        \n",
    "#         easy_mixup_data = easy_data[easy_data['idx'].isin(easy_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(easy_high_ent_idx)\n",
    "#         easy_mixup_data['idx_2'] = easy_high_ent_idx\n",
    "#         easy_mixup_data['text_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['text'].values[0])\n",
    "#         easy_mixup_data['label_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['label'].values[0])\n",
    "#         easy_mixup_data['category_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['category'].values[0])\n",
    "#         easy_mixup_data['mixup_type'] = 'same_easy'\n",
    "        \n",
    "#         # Ambi-Ambi Mixup\n",
    "#         ambiguous_data = data[data['category'] == 'ambiguous']\n",
    "#         ambiguous_low_ent_idx = ambiguous_data.sort_values('entropy', ascending=True).head(mixup_size//3)['idx'].tolist()\n",
    "#         ambiguous_high_ent_idx = ambiguous_data.sort_values('entropy', ascending=False).head(mixup_size//3)['idx'].tolist()\n",
    "        \n",
    "#         ambiguous_mixup_data = ambiguous_data[ambiguous_data['idx'].isin(ambiguous_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(ambiguous_high_ent_idx)\n",
    "#         ambiguous_mixup_data['idx_2'] = ambiguous_high_ent_idx\n",
    "#         ambiguous_mixup_data['text_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['text'].values[0])\n",
    "#         ambiguous_mixup_data['label_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['label'].values[0])\n",
    "#         ambiguous_mixup_data['category_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['category'].values[0])\n",
    "#         ambiguous_mixup_data['mixup_type'] = 'same_ambiguous'\n",
    "        \n",
    "#         same_mixup_data = pd.concat([easy_mixup_data, ambiguous_mixup_data])\n",
    "        \n",
    "#         # --------------------------------------- Different class mixup ---------------------------------------  \n",
    "        \n",
    "#         # Random easy-ambi mixup\n",
    "#         different_samples = mixup_size - len(same_mixup_data)\n",
    "#         easy_tuple = list(zip(easy_data['idx'].tolist(), easy_data['text'].tolist(), easy_data['label'].tolist(), easy_data['category'].tolist()))\n",
    "#         ambiguous_tuple = list(zip(ambiguous_data['idx'].tolist(), ambiguous_data['text'].tolist(), ambiguous_data['label'].tolist(), ambiguous_data['category'].tolist()))\n",
    "        \n",
    "#         easy_data = easy_data.sample(n=different_samples//2).reset_index(drop=True)\n",
    "#         ambiguous4easy = random.choices(ambiguous_tuple, weights=np.ones(len(ambiguous_tuple)), k=different_samples//2)\n",
    "#         ambiguous4easy = pd.DataFrame(ambiguous4easy, columns=['idx_2', 'text_2', 'label_2', 'category_2'])\n",
    "#         ambiguous4easy = pd.concat([easy_data, ambiguous4easy], axis=1).reset_index(drop=True)\n",
    "#         ambiguous4easy['mixup_type'] = 'ambiguous_easy'\n",
    "        \n",
    "#         ambiguous_data = ambiguous_data.sample(n=different_samples//2).reset_index(drop=True)\n",
    "#         easy4ambiguous = random.choices(easy_tuple, weights=np.ones(len(easy_tuple)), k=different_samples//2)\n",
    "#         easy4ambiguous = pd.DataFrame(easy4ambiguous, columns=['idx_2', 'text_2', 'label_2', 'category_2'])\n",
    "#         easy4ambiguous = pd.concat([ambiguous_data, easy4ambiguous], axis=1).reset_index(drop=True)\n",
    "#         easy4ambiguous['mixup_type'] = 'easy_ambiguous'\n",
    "        \n",
    "#         return pd.concat([same_mixup_data, easy4ambiguous, ambiguous4easy]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892a511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_original(data, include_none=False):\n",
    "    if include_none:\n",
    "        return data[data['category'] != 'hard'].reset_index(drop=True)\n",
    "    else:\n",
    "        return data[(data['category'] != 'none') & (data['category'] != 'hard')].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafd0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_data(df, label=0, use_entropy=False):\n",
    "    df_label = df[df['label'] == label].reset_index(drop=True)\n",
    "    if use_entropy:\n",
    "        df_label = df_label.sort_values('entropy', ascending=True).reset_index(drop=True)\n",
    "        temp_label = df_label.sort_values('entropy', ascending=False).reset_index(drop=True)\n",
    "        temp_label = temp_label.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "        return pd.concat([df_label, temp_label], axis=1).reset_index(drop=True)\n",
    "    else:\n",
    "        temp_label = df_label.sample(frac=1).reset_index(drop=True)\n",
    "        temp_label = temp_label.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})   \n",
    "        return pd.concat([df_label, temp_label], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a458859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_random_mixup(data, info_data=None, include_none=False, use_label=False):\n",
    "    \n",
    "    if include_none:\n",
    "        data = data[data['category'] != 'hard'].reset_index(drop=True)\n",
    "    else:\n",
    "        data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "    \n",
    "    data_len = len(data)\n",
    "    \n",
    "    if use_label:\n",
    "        data_0 = get_label_data(data, label=0)\n",
    "        data_1 = get_label_data(data, label=1)\n",
    "        final_data = pd.concat([data_0, data_1]).reset_index(drop=True)\n",
    "        \n",
    "    else:\n",
    "        temp = data.copy()\n",
    "        temp = temp.sample(frac=1).reset_index(drop=True)\n",
    "        temp = temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "        final_data = pd.concat([data, temp], axis=1)\n",
    "        \n",
    "    random_subset_1 = data.sample(n=data_len-len(final_data)).reset_index(drop=True)\n",
    "    random_subset_2 = data.sample(n=data_len-len(final_data)).reset_index(drop=True)\n",
    "    random_subset_2 = random_subset_2.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "    random_subset = pd.concat([random_subset_1, random_subset_2], axis=1).reset_index(drop=True)  \n",
    "\n",
    "    return pd.concat([final_data, random_subset]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886f894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_category_mixup(data, info_data=None, include_none=False, use_label=False, use_entropy=False):\n",
    "    data_len = len(data)\n",
    "    \n",
    "    if include_none:\n",
    "        data = data[data['category'] != 'hard'].reset_index(drop=True)\n",
    "    else:\n",
    "        data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "    \n",
    "    if use_entropy: \n",
    "        data = pd.merge(data, info_data, on='idx')[['idx', 'text', 'label', 'category', 'softmax', 'entropy']]\n",
    "\n",
    "        if use_label:\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_data_0 = get_label_data(easy_data, label=0, use_entropy=True)\n",
    "            easy_data_1 = get_label_data(easy_data, label=1, use_entropy=True)\n",
    "            easy_data  = pd.concat([easy_data_0, easy_data_1]).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "\n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_data_0 = get_label_data(ambiguous_data, label=0, use_entropy=True)\n",
    "            ambiguous_data_1 = get_label_data(ambiguous_data, label=1, use_entropy=True)\n",
    "            ambiguous_data  = pd.concat([ambiguous_data_0, ambiguous_data_1]).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "\n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            if include_none:\n",
    "                # none-none Mixup\n",
    "                none_data = data[data['category'] == 'none'].reset_index(drop=True)\n",
    "                none_data_0 = get_label_data(none_data, label=0, use_entropy=True)\n",
    "                none_data_1 = get_label_data(none_data, label=1, use_entropy=True)\n",
    "                none_data  = pd.concat([none_data_0, none_data_1]).reset_index(drop=True)\n",
    "                none_data['mixup_type'] = 'same_none'\n",
    "                \n",
    "                final_data = pd.concat([final_data, none_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            return final_data\n",
    "\n",
    "\n",
    "#             # Easy-Ambi Mixup\n",
    "#             easy_ambiguous_len = data_len - len(final_data)\n",
    "\n",
    "#             easy_0 = easy_data_0.head(min(easy_ambiguous_len//2, len(easy_data_0), len(ambiguous_data_0)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_0 = ambiguous_data_0.tail(min(easy_ambiguous_len//2, len(easy_data_0), len(ambiguous_data_0)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_0 = ambiguous_0.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "#             easy_ambiguous_0 = pd.concat([easy_0, ambiguous_0], axis=1).reset_index(drop=True)\n",
    "\n",
    "#             easy_1 = easy_data_1.head(min(easy_ambiguous_len//2, len(easy_data_1), len(ambiguous_data_1)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_1 = ambiguous_data_1.tail(min(easy_ambiguous_len//2, len(easy_data_1), len(ambiguous_data_1)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_1 = ambiguous_1.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "\n",
    "#             easy_ambiguous_1 = pd.concat([easy_1, ambiguous_1], axis=1).reset_index(drop=True)\n",
    "#             easy_ambiguous_data = pd.concat([easy_ambiguous_0, easy_ambiguous_1]).reset_index(drop=True)\n",
    "#             easy_ambiguous_data['mixup_type'] = 'easy_ambiguous'\n",
    "\n",
    "#             return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "        else:\n",
    "            # Easy-Easy Mixup\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_data = easy_data.sort_values('entropy', ascending=True).reset_index(drop=True)\n",
    "            easy_temp = easy_data.sort_values('entropy', ascending=False).reset_index(drop=True)\n",
    "            easy_temp = easy_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "            easy_data = pd.concat([easy_data, easy_temp], axis=1).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "\n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_data = ambiguous_data.sort_values('entropy', ascending=True).reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_data.sort_values('entropy', ascending=False).reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "            ambiguous_data = pd.concat([ambiguous_data, ambiguous_temp], axis=1).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "\n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "            if include_none:\n",
    "                # none-none Mixup\n",
    "                none_data = data[data['category'] == 'none'].reset_index(drop=True)\n",
    "                none_data = none_data.sort_values('entropy', ascending=True).reset_index(drop=True)\n",
    "                none_temp = none_data.sort_values('entropy', ascending=False).reset_index(drop=True)\n",
    "                none_temp = none_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "                none_data = pd.concat([none_data, none_temp], axis=1).reset_index(drop=True)\n",
    "                none_data['mixup_type'] = 'same_none'\n",
    "                \n",
    "                final_data = pd.concat([final_data, none_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            return final_data\n",
    "\n",
    "#             # Easy-Ambi Mixup\n",
    "#             easy_ambiguous_len = data_len - len(final_data)\n",
    "#             easy_ambiguous_data = easy_data.head(min(easy_ambiguous_len, len(easy_data)))[['idx', 'text', 'label', 'category', 'softmax', 'entropy']].reset_index(drop=True)\n",
    "\n",
    "#             easy_ambiguous_temp = ambiguous_data.tail(min(easy_ambiguous_len, len(ambiguous_data)))[['idx', 'text', 'label', 'category', 'softmax', 'entropy']].reset_index(drop=True)\n",
    "#             easy_ambiguous_temp = easy_ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "\n",
    "#             easy_ambiguous_data = pd.concat([easy_ambiguous_data, easy_ambiguous_temp], axis=1)\n",
    "#             easy_ambiguous_data['mixup_type'] = 'easy_ambiguous'\n",
    "\n",
    "#             return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        if use_label:\n",
    "            # Easy-Easy mixup\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_data_0 = get_label_data(easy_data, label=0)\n",
    "            easy_data_1 = get_label_data(easy_data, label=1)\n",
    "            easy_data  = pd.concat([easy_data_0, easy_data_1]).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "            \n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_data_0 = get_label_data(ambiguous_data, label=0)\n",
    "            ambiguous_data_1 = get_label_data(ambiguous_data, label=1)\n",
    "            ambiguous_data  = pd.concat([ambiguous_data_0, ambiguous_data_1]).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "            \n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            if include_none:\n",
    "                # none-none mixup\n",
    "                none_data = data[data['category'] == 'none'].reset_index(drop=True)\n",
    "                none_data_0 = get_label_data(none_data, label=0)\n",
    "                none_data_1 = get_label_data(none_data, label=1)\n",
    "                none_data  = pd.concat([none_data_0, none_data_1]).reset_index(drop=True)\n",
    "                none_data['mixup_type'] = 'same_none'\n",
    "                \n",
    "                final_data = pd.concat([final_data, none_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            return final_data\n",
    "            \n",
    "#             # Easy-Ambi Mixup\n",
    "#             easy_ambiguous_len = data_len - len(final_data)\n",
    "            \n",
    "#             easy_0 = easy_data_0.sample(n=min(easy_ambiguous_len//2, len(easy_data_0), len(ambiguous_data_0)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_0 = ambiguous_data_0.sample(n=min(easy_ambiguous_len//2, len(easy_data_0), len(ambiguous_data_0)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_0 = ambiguous_0.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "#             easy_ambiguous_0 = pd.concat([easy_0, ambiguous_0], axis=1).reset_index(drop=True)\n",
    "\n",
    "#             easy_1 = easy_data_1.sample(n=min(easy_ambiguous_len//2, len(easy_data_1), len(ambiguous_data_1)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_1 = ambiguous_data_1.sample(n=min(easy_ambiguous_len//2, len(easy_data_1), len(ambiguous_data_1)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_1 = ambiguous_1.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "#             easy_ambiguous_1 = pd.concat([easy_1, ambiguous_1], axis=1).reset_index(drop=True)\n",
    "            \n",
    "#             easy_ambiguous_data = pd.concat([easy_ambiguous_0, easy_ambiguous_1]).reset_index(drop=True)\n",
    "#             easy_ambiguous_data['mixup_type'] = 'easy_ambiguous'\n",
    "#             return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        else:\n",
    "            # Easy-Easy mixup\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_temp = easy_data.copy()\n",
    "            easy_temp = easy_temp.sample(frac=1).reset_index(drop=True)\n",
    "            easy_temp = easy_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "            easy_data = pd.concat([easy_data, easy_temp], axis=1).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "\n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_data.copy()\n",
    "            ambiguous_temp = ambiguous_temp.sample(frac=1).reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "            ambiguous_data = pd.concat([ambiguous_data, ambiguous_temp], axis=1).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "            \n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            if include_none:\n",
    "                # None-None mixup\n",
    "                none_data = data[data['category'] == 'none'].reset_index(drop=True)\n",
    "                none_temp = none_data.copy()\n",
    "                none_temp = none_temp.sample(frac=1).reset_index(drop=True)\n",
    "                none_temp = none_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "                none_data = pd.concat([none_data, none_temp], axis=1).reset_index(drop=True)\n",
    "                none_data['mixup_type'] = 'same_none'\n",
    "\n",
    "                final_data = pd.concat([final_data, none_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            return final_data\n",
    "        \n",
    "#             # Easy-Ambi Mixup\n",
    "#             easy_ambiguous_len = data_len - len(final_data)\n",
    "#             easy_ambiguous_data = easy_data.sample(n=min(easy_ambiguous_len, len(easy_data)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             easy_ambiguous_temp = ambiguous_data.sample(n=min(easy_ambiguous_len, len(ambiguous_data)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             easy_ambiguous_temp = easy_ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "#             easy_ambiguous_data = pd.concat([easy_ambiguous_data, easy_ambiguous_temp], axis=1)\n",
    "#             easy_ambiguous_data['mixup_type'] = 'easy_ambiguous'\n",
    "            \n",
    "#             return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897f36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(data, info_data=None, mixup=False):\n",
    "    \n",
    "#     # Remove none and hard examples\n",
    "#     data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "\n",
    "#     # Add softmax and entropy info\n",
    "#     if (info_data is not None) and mixup:\n",
    "#         data = pd.merge(data, info_data, on='idx')[['idx', 'text', 'label', 'category', 'softmax', 'entropy']]\n",
    "#         mixup_size = len(info_data) - len(data)\n",
    "\n",
    "#         # --------------------------------------- Same class mixup ---------------------------------------  \n",
    "\n",
    "#         # Easy-Easy Mixup\n",
    "#         easy_data = data[data['category'] == 'easy']\n",
    "#         easy_low_ent_idx = easy_data.sort_values('entropy', ascending=True).head(mixup_size//2)['idx'].tolist()\n",
    "#         easy_high_ent_idx = easy_data.sort_values('entropy', ascending=False).head(mixup_size//2)['idx'].tolist()\n",
    "        \n",
    "#         easy_mixup_data = easy_data[easy_data['idx'].isin(easy_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(easy_high_ent_idx)\n",
    "#         easy_mixup_data['idx_2'] = easy_high_ent_idx\n",
    "#         easy_mixup_data['text_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['text'].values[0])\n",
    "#         easy_mixup_data['label_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['label'].values[0])\n",
    "#         easy_mixup_data['category_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['category'].values[0])\n",
    "#         easy_mixup_data['mixup_type'] = 'same_easy'\n",
    "        \n",
    "#         # Ambi-Ambi Mixup\n",
    "#         ambiguous_data = data[data['category'] == 'ambiguous']\n",
    "#         ambiguous_low_ent_idx = ambiguous_data.sort_values('entropy', ascending=True).head(mixup_size//2)['idx'].tolist()\n",
    "#         ambiguous_high_ent_idx = ambiguous_data.sort_values('entropy', ascending=False).head(mixup_size//2)['idx'].tolist()\n",
    "        \n",
    "#         ambiguous_mixup_data = ambiguous_data[ambiguous_data['idx'].isin(ambiguous_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(ambiguous_high_ent_idx)\n",
    "#         ambiguous_mixup_data['idx_2'] = ambiguous_high_ent_idx\n",
    "#         ambiguous_mixup_data['text_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['text'].values[0])\n",
    "#         ambiguous_mixup_data['label_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['label'].values[0])\n",
    "#         ambiguous_mixup_data['category_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['category'].values[0])\n",
    "#         ambiguous_mixup_data['mixup_type'] = 'same_ambiguous'\n",
    "        \n",
    "#         return pd.concat([easy_mixup_data, ambiguous_mixup_data]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3a9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(df, label1, label2):\n",
    "    cnt_label = 0\n",
    "    cnt_category = 0\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, label1] == df.at[i, f\"{label1}_2\"]:\n",
    "            cnt_label += 1\n",
    "        if df.at[i, label2] == df.at[i, f\"{label2}_2\"]:\n",
    "            cnt_category += 1\n",
    "\n",
    "    print(\"same label: \", cnt_label/len(df))\n",
    "    print(\"same category: \", cnt_category/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f91e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'sarcasm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcdde589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>3462</td>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>3463</td>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>3464</td>\n",
       "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>3465</td>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466</td>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0        0  The only thing I got from college is a caffein...      1   \n",
       "1        1  I love it when professors draw a big question ...      1   \n",
       "2        2  Remember the hundred emails from companies whe...      1   \n",
       "3        3  Today my pop-pop told me I was not “forced” to...      1   \n",
       "4        4  @VolphanCarol @littlewhitty @mysticalmanatee I...      1   \n",
       "...    ...                                                ...    ...   \n",
       "3462  3462  The population spike in Chicago in 9 months is...      0   \n",
       "3463  3463  You'd think in the second to last English clas...      0   \n",
       "3464  3464  I’m finally surfacing after a holiday to Scotl...      0   \n",
       "3465  3465  Couldn't be prouder today. Well done to every ...      0   \n",
       "3466  3466  Overheard as my 13 year old games with a frien...      0   \n",
       "\n",
       "       category  \n",
       "0          hard  \n",
       "1     ambiguous  \n",
       "2     ambiguous  \n",
       "3     ambiguous  \n",
       "4     ambiguous  \n",
       "...         ...  \n",
       "3462       none  \n",
       "3463       none  \n",
       "3464       easy  \n",
       "3465       easy  \n",
       "3466       none  \n",
       "\n",
       "[3467 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'/projects/metis2/atharvak/Data_Cartography/datasets/{data_name}/{data_name}_categorized.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0a6b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ambiguous    1144\n",
       "easy         1144\n",
       "none          978\n",
       "hard          201\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a6ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>gold</th>\n",
       "      <th>softmax</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3061</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.96356308 0.03643692]</td>\n",
       "      <td>0.156450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.22491207 0.77508793]</td>\n",
       "      <td>0.533055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99220995 0.00779005]</td>\n",
       "      <td>0.045580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.00963458 0.99036542]</td>\n",
       "      <td>0.054316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99644967 0.00355033]</td>\n",
       "      <td>0.023570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>2640</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.96515393 0.03484607]</td>\n",
       "      <td>0.151204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>1204</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99699631 0.00300369]</td>\n",
       "      <td>0.020444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99511012 0.00488988]</td>\n",
       "      <td>0.030895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>2704</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99134994 0.00865006]</td>\n",
       "      <td>0.049702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>1133</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.97961178 0.02038822]</td>\n",
       "      <td>0.099546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  gold                  softmax   entropy\n",
       "0     3061     0  [0.96356308 0.03643692]  0.156450\n",
       "1      326     1  [0.22491207 0.77508793]  0.533055\n",
       "2     2802     0  [0.99220995 0.00779005]  0.045580\n",
       "3      365     1  [0.00963458 0.99036542]  0.054316\n",
       "4     2770     0  [0.99644967 0.00355033]  0.023570\n",
       "...    ...   ...                      ...       ...\n",
       "3462  2640     0  [0.96515393 0.03484607]  0.151204\n",
       "3463  1204     0  [0.99699631 0.00300369]  0.020444\n",
       "3464  1370     0  [0.99511012 0.00488988]  0.030895\n",
       "3465  2704     0  [0.99134994 0.00865006]  0.049702\n",
       "3466  1133     0  [0.97961178 0.02038822]  0.099546\n",
       "\n",
       "[3467 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df = pd.read_csv(f'/projects/metis2/atharvak/Data_Cartography/dy_log/{data_name}/roberta-base/training_dynamics/final_4.csv')\n",
    "info_df = info_df.rename(columns={'guid': 'idx', 'sm': 'softmax', 'en': 'entropy'})\n",
    "info_df = info_df[['idx', 'gold', 'softmax', 'entropy']]\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4212da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@jimrossignol I choose to interpret it as \"XD\"...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>3462</td>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>3463</td>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>3464</td>\n",
       "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>3465</td>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>3466</td>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0        1  I love it when professors draw a big question ...      1   \n",
       "1        2  Remember the hundred emails from companies whe...      1   \n",
       "2        3  Today my pop-pop told me I was not “forced” to...      1   \n",
       "3        4  @VolphanCarol @littlewhitty @mysticalmanatee I...      1   \n",
       "4        5  @jimrossignol I choose to interpret it as \"XD\"...      1   \n",
       "...    ...                                                ...    ...   \n",
       "3261  3462  The population spike in Chicago in 9 months is...      0   \n",
       "3262  3463  You'd think in the second to last English clas...      0   \n",
       "3263  3464  I’m finally surfacing after a holiday to Scotl...      0   \n",
       "3264  3465  Couldn't be prouder today. Well done to every ...      0   \n",
       "3265  3466  Overheard as my 13 year old games with a frien...      0   \n",
       "\n",
       "       category  \n",
       "0     ambiguous  \n",
       "1     ambiguous  \n",
       "2     ambiguous  \n",
       "3     ambiguous  \n",
       "4     ambiguous  \n",
       "...         ...  \n",
       "3261       none  \n",
       "3262       none  \n",
       "3263       easy  \n",
       "3264       easy  \n",
       "3265       none  \n",
       "\n",
       "[3266 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_dataset = prepare_dataset_original(df, include_none=True)\n",
    "normal_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2708d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.6270667483159829\n",
      "same category:  0.3303735456215554\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514</td>\n",
       "      <td>i want :( a corn dog :(( so bad wtf :(((</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2377</td>\n",
       "      <td>i keep thinking about the time that @becelliso...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2108</td>\n",
       "      <td>Hindsight is a wonderful thing, but surely the...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>636</td>\n",
       "      <td>No longer a female as I refuse to wear heels e...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1617</td>\n",
       "      <td>Casually looking at the 06Z forecast it seems ...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2190</td>\n",
       "      <td>HAPPY TWENTIETH @ocodom24!Eat lots of Poptarts...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1329</td>\n",
       "      <td>@sonofsama1 @BoqorofCeel U got more than me🥺</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>413</td>\n",
       "      <td>why is the weather having a mid life crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3021</td>\n",
       "      <td>okay \"deja vu\" is doing something for me that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>3140</td>\n",
       "      <td>went to the bathroom at about 5am yesterday, m...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>1731</td>\n",
       "      <td>have to somehow become fluent in Spanish in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>1103</td>\n",
       "      <td>i just sewed through my finger and my brain is...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>2399</td>\n",
       "      <td>But by far, my favorite part was a very drunk ...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2125</td>\n",
       "      <td>I hired a housekeeper...I didn’t want to but I...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>2700</td>\n",
       "      <td>can u believe shaz hung up on me to see if her...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>2184</td>\n",
       "      <td>Nothing pisses me off more than people with do...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>1038</td>\n",
       "      <td>Just came online to remind y'all that Davido i...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>1524</td>\n",
       "      <td>The nice thing about being back in school is t...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>1494</td>\n",
       "      <td>because my neighbours are just so unbelievably...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2969</td>\n",
       "      <td>The Fourth of July is my favorite holiday for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     1514           i want :( a corn dog :(( so bad wtf :(((      0   \n",
       "1     2108  Hindsight is a wonderful thing, but surely the...      0   \n",
       "2     1617  Casually looking at the 06Z forecast it seems ...      0   \n",
       "3     1329       @sonofsama1 @BoqorofCeel U got more than me🥺      0   \n",
       "4     3021  okay \"deja vu\" is doing something for me that ...      0   \n",
       "...    ...                                                ...    ...   \n",
       "3261  1731  have to somehow become fluent in Spanish in th...      0   \n",
       "3262  2399  But by far, my favorite part was a very drunk ...      0   \n",
       "3263  2700  can u believe shaz hung up on me to see if her...      0   \n",
       "3264  1038  Just came online to remind y'all that Davido i...      0   \n",
       "3265  1494  because my neighbours are just so unbelievably...      0   \n",
       "\n",
       "       category  idx_2                                             text_2  \\\n",
       "0          none   2377  i keep thinking about the time that @becelliso...   \n",
       "1     ambiguous    636  No longer a female as I refuse to wear heels e...   \n",
       "2          none   2190  HAPPY TWENTIETH @ocodom24!Eat lots of Poptarts...   \n",
       "3     ambiguous    413        why is the weather having a mid life crisis   \n",
       "4          none   3140  went to the bathroom at about 5am yesterday, m...   \n",
       "...         ...    ...                                                ...   \n",
       "3261       none   1103  i just sewed through my finger and my brain is...   \n",
       "3262       none   2125  I hired a housekeeper...I didn’t want to but I...   \n",
       "3263  ambiguous   2184  Nothing pisses me off more than people with do...   \n",
       "3264       none   1524  The nice thing about being back in school is t...   \n",
       "3265       none   2969  The Fourth of July is my favorite holiday for ...   \n",
       "\n",
       "      label_2 category_2  \n",
       "0           0       easy  \n",
       "1           1  ambiguous  \n",
       "2           0       easy  \n",
       "3           1  ambiguous  \n",
       "4           0       none  \n",
       "...       ...        ...  \n",
       "3261        0  ambiguous  \n",
       "3262        0       easy  \n",
       "3263        0       easy  \n",
       "3264        0       easy  \n",
       "3265        0       none  \n",
       "\n",
       "[3266 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_mixup_dataset = prepare_dataset_random_mixup(df, info_data=info_df, include_none=True, use_label=False)\n",
    "get_count(random_mixup_dataset, 'label', 'category')\n",
    "random_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ecacf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  1.0\n",
      "same category:  0.535517452541335\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1043</td>\n",
       "      <td>The @theAJpub is open again @gpollakis 👀</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>1679</td>\n",
       "      <td>This second jab has made me feel off the box t...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>837</td>\n",
       "      <td>@majornelson Sounds like a strong contender fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>613</td>\n",
       "      <td>Matt Hancock is a top shagger</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3386</td>\n",
       "      <td>All I needed was some Lady Gaga and some Nicki...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2711</td>\n",
       "      <td>10 years today since I left school. I’m in sho...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1728</td>\n",
       "      <td>i just overheard this bunch of children behind...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2386</td>\n",
       "      <td>#LoveIsland been back for one night and I'm al...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1598</td>\n",
       "      <td>to be completely honest, my ego is MASSIVE and...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3443</td>\n",
       "      <td>Only want someone who loves dancing just as mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>814</td>\n",
       "      <td>tex-mex restaurants that stop serving breakfas...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>446</td>\n",
       "      <td>Love that someone broke into my car this morning</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>1827</td>\n",
       "      <td>Reminding myself that things take time &amp;amp; I...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>926</td>\n",
       "      <td>Nothing can ruin a summer like hayfever</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>2312</td>\n",
       "      <td>Anyone else remember Twix Tops and M&amp;amp;M's B...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2156</td>\n",
       "      <td>The end of of S3E2 money heist makes me v happy</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>106</td>\n",
       "      <td>@DonaldJTrumpJr If only he had a press Corp in...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>764</td>\n",
       "      <td>If anyone wants to know how my nights going I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>1436</td>\n",
       "      <td>Many people have said “I’ll always be here for...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>2537</td>\n",
       "      <td>Our mate @bikeitben1 has just broken the 48-ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     1043           The @theAJpub is open again @gpollakis 👀      0   \n",
       "1      837  @majornelson Sounds like a strong contender fo...      1   \n",
       "2     3386  All I needed was some Lady Gaga and some Nicki...      0   \n",
       "3     1728  i just overheard this bunch of children behind...      0   \n",
       "4     1598  to be completely honest, my ego is MASSIVE and...      0   \n",
       "...    ...                                                ...    ...   \n",
       "3261   814  tex-mex restaurants that stop serving breakfas...      1   \n",
       "3262  1827  Reminding myself that things take time &amp; I...      0   \n",
       "3263  2312  Anyone else remember Twix Tops and M&amp;M's B...      0   \n",
       "3264   106  @DonaldJTrumpJr If only he had a press Corp in...      1   \n",
       "3265  1436  Many people have said “I’ll always be here for...      0   \n",
       "\n",
       "       category  idx_2                                             text_2  \\\n",
       "0          none   1679  This second jab has made me feel off the box t...   \n",
       "1     ambiguous    613                      Matt Hancock is a top shagger   \n",
       "2          none   2711  10 years today since I left school. I’m in sho...   \n",
       "3          none   2386  #LoveIsland been back for one night and I'm al...   \n",
       "4          easy   3443  Only want someone who loves dancing just as mu...   \n",
       "...         ...    ...                                                ...   \n",
       "3261  ambiguous    446   Love that someone broke into my car this morning   \n",
       "3262       easy    926            Nothing can ruin a summer like hayfever   \n",
       "3263       none   2156    The end of of S3E2 money heist makes me v happy   \n",
       "3264  ambiguous    764  If anyone wants to know how my nights going I ...   \n",
       "3265       easy   2537  Our mate @bikeitben1 has just broken the 48-ho...   \n",
       "\n",
       "      label_2 category_2  \n",
       "0           0       easy  \n",
       "1           1  ambiguous  \n",
       "2           0       easy  \n",
       "3           0       none  \n",
       "4           0       easy  \n",
       "...       ...        ...  \n",
       "3261        1  ambiguous  \n",
       "3262        0       easy  \n",
       "3263        0       none  \n",
       "3264        1  ambiguous  \n",
       "3265        0       easy  \n",
       "\n",
       "[3266 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_mixup_dataset = prepare_dataset_random_mixup(df, info_data=info_df, include_none=True, use_label=True)\n",
    "get_count(random_mixup_dataset, 'label', 'category')\n",
    "random_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dca96ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.8560930802204532\n",
      "same category:  1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>mixup_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1223</td>\n",
       "      <td>HOW DO THEY GET THEIR HORSES TO JAPAN</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3109</td>\n",
       "      <td>Give the #FarmersProtests at least half the ti...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1311</td>\n",
       "      <td>Can anyone enlighten me as to what is near Imp...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>3318</td>\n",
       "      <td>every time someone asks me about getting a pie...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>same_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3326</td>\n",
       "      <td>Why am I sad for no reason ALL THE TIME</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>2555</td>\n",
       "      <td>IM GETTING MY NAILS DONE TODAY!!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2558</td>\n",
       "      <td>@RachaelSmarty Honestly cannot wait to give th...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>1416</td>\n",
       "      <td>@Naughty_Dog Remake them like Spyro and Crash ...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>same_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517</td>\n",
       "      <td>I love when I learn about stuff I've apparentl...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>617</td>\n",
       "      <td>Just watched a man smoke crack on the R train ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>670</td>\n",
       "      <td>@AmazonUK why does Alexa UK not get improvemen...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>350</td>\n",
       "      <td>Yay for being locked out of your own  house be...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>2393</td>\n",
       "      <td>I despise 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>1073</td>\n",
       "      <td>No but the worst type of boy is the one that w...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>same_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>2533</td>\n",
       "      <td>Resisting homemade cheesecake at work for 3 da...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>578</td>\n",
       "      <td>Can’t wait to see Ed sheeran at tramlines tomoz x</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>92</td>\n",
       "      <td>@rits_meg @freedomsenpai hot loli</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>220</td>\n",
       "      <td>day 5 of people being mad about ellen sitting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>622</td>\n",
       "      <td>If I were the Red Sox I would simply play the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>204</td>\n",
       "      <td>Brother's off on a night out and I've gone to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     1223              HOW DO THEY GET THEIR HORSES TO JAPAN      0   \n",
       "1     1311  Can anyone enlighten me as to what is near Imp...      0   \n",
       "2     3326            Why am I sad for no reason ALL THE TIME      0   \n",
       "3     2558  @RachaelSmarty Honestly cannot wait to give th...      0   \n",
       "4      517  I love when I learn about stuff I've apparentl...      1   \n",
       "...    ...                                                ...    ...   \n",
       "3261   670  @AmazonUK why does Alexa UK not get improvemen...      1   \n",
       "3262  2393                                     I despise 2021      0   \n",
       "3263  2533  Resisting homemade cheesecake at work for 3 da...      0   \n",
       "3264    92                  @rits_meg @freedomsenpai hot loli      1   \n",
       "3265   622  If I were the Red Sox I would simply play the ...      1   \n",
       "\n",
       "       category  idx_2                                             text_2  \\\n",
       "0          easy   3109  Give the #FarmersProtests at least half the ti...   \n",
       "1          none   3318  every time someone asks me about getting a pie...   \n",
       "2          easy   2555                IM GETTING MY NAILS DONE TODAY!!!!!   \n",
       "3          none   1416  @Naughty_Dog Remake them like Spyro and Crash ...   \n",
       "4     ambiguous    617  Just watched a man smoke crack on the R train ...   \n",
       "...         ...    ...                                                ...   \n",
       "3261  ambiguous    350  Yay for being locked out of your own  house be...   \n",
       "3262       none   1073  No but the worst type of boy is the one that w...   \n",
       "3263  ambiguous    578  Can’t wait to see Ed sheeran at tramlines tomoz x   \n",
       "3264  ambiguous    220  day 5 of people being mad about ellen sitting ...   \n",
       "3265  ambiguous    204  Brother's off on a night out and I've gone to ...   \n",
       "\n",
       "      label_2 category_2      mixup_type  \n",
       "0           0       easy       same_easy  \n",
       "1           0       none       same_none  \n",
       "2           0       easy       same_easy  \n",
       "3           0       none       same_none  \n",
       "4           1  ambiguous  same_ambiguous  \n",
       "...       ...        ...             ...  \n",
       "3261        1  ambiguous  same_ambiguous  \n",
       "3262        0       none       same_none  \n",
       "3263        1  ambiguous  same_ambiguous  \n",
       "3264        1  ambiguous  same_ambiguous  \n",
       "3265        1  ambiguous  same_ambiguous  \n",
       "\n",
       "[3266 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, include_none=True, use_label=False, use_entropy=False)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a0c04df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  1.0\n",
      "same category:  1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>mixup_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3459</td>\n",
       "      <td>sorry folks,,, but new t swift album bangs</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3147</td>\n",
       "      <td>i’d like to share that i am the happiest i’ve ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1020</td>\n",
       "      <td>In terms of usability is there a more frustrat...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>1035</td>\n",
       "      <td>missed this the first time round but a convers...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1957</td>\n",
       "      <td>My mother is going to Bonnaroo today I love her</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>2164</td>\n",
       "      <td>seonghwa is still one of the most gorgeous men...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>412</td>\n",
       "      <td>Anyone else hear some like thunder or something?</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>162</td>\n",
       "      <td>no more instagram. we must all return to scrap...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2231</td>\n",
       "      <td>Reina when we first put on her Halloween costu...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3154</td>\n",
       "      <td>Just a casual day at Oklahoma state university...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>1847</td>\n",
       "      <td>Finding out my internship keeps Coke products ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>2273</td>\n",
       "      <td>I've started an etsy shop selling handmade cla...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>3430</td>\n",
       "      <td>Chloe and Toby just minding their business 🤣</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>1777</td>\n",
       "      <td>days before rodeo is Travis Scott's best project</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>same_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>828</td>\n",
       "      <td>People who think cancel culture goes too far n...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>5</td>\n",
       "      <td>@jimrossignol I choose to interpret it as \"XD\"...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>5</td>\n",
       "      <td>@jimrossignol I choose to interpret it as \"XD\"...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>73</td>\n",
       "      <td>You know the wolves match is boring when you'r...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>1192</td>\n",
       "      <td>@OwenSmith_MP @BBCr4today It wasn't a million ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3328</td>\n",
       "      <td>I am so thankful for Christian</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     3459         sorry folks,,, but new t swift album bangs      0   \n",
       "1     1020  In terms of usability is there a more frustrat...      0   \n",
       "2     1957    My mother is going to Bonnaroo today I love her      0   \n",
       "3      412   Anyone else hear some like thunder or something?      1   \n",
       "4     2231  Reina when we first put on her Halloween costu...      0   \n",
       "...    ...                                                ...    ...   \n",
       "3261  1847  Finding out my internship keeps Coke products ...      0   \n",
       "3262  3430       Chloe and Toby just minding their business 🤣      0   \n",
       "3263   828  People who think cancel culture goes too far n...      1   \n",
       "3264     5  @jimrossignol I choose to interpret it as \"XD\"...      1   \n",
       "3265  1192  @OwenSmith_MP @BBCr4today It wasn't a million ...      0   \n",
       "\n",
       "       category  idx_2                                             text_2  \\\n",
       "0          easy   3147  i’d like to share that i am the happiest i’ve ...   \n",
       "1          easy   1035  missed this the first time round but a convers...   \n",
       "2          easy   2164  seonghwa is still one of the most gorgeous men...   \n",
       "3     ambiguous    162  no more instagram. we must all return to scrap...   \n",
       "4          easy   3154  Just a casual day at Oklahoma state university...   \n",
       "...         ...    ...                                                ...   \n",
       "3261       easy   2273  I've started an etsy shop selling handmade cla...   \n",
       "3262       none   1777   days before rodeo is Travis Scott's best project   \n",
       "3263  ambiguous      5  @jimrossignol I choose to interpret it as \"XD\"...   \n",
       "3264  ambiguous     73  You know the wolves match is boring when you'r...   \n",
       "3265       easy   3328                     I am so thankful for Christian   \n",
       "\n",
       "      label_2 category_2      mixup_type  \n",
       "0           0       easy       same_easy  \n",
       "1           0       easy       same_easy  \n",
       "2           0       easy       same_easy  \n",
       "3           1  ambiguous  same_ambiguous  \n",
       "4           0       easy       same_easy  \n",
       "...       ...        ...             ...  \n",
       "3261        0       easy       same_easy  \n",
       "3262        0       none       same_none  \n",
       "3263        1  ambiguous  same_ambiguous  \n",
       "3264        1  ambiguous  same_ambiguous  \n",
       "3265        0       easy       same_easy  \n",
       "\n",
       "[3266 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, include_none=True, use_label=True, use_entropy=False)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "715a2cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.8677281077770974\n",
      "same category:  1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>softmax</th>\n",
       "      <th>entropy</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>softmax_2</th>\n",
       "      <th>entropy_2</th>\n",
       "      <th>mixup_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2356</td>\n",
       "      <td>u know those days where you just need to stare...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99150319 0.00849681]</td>\n",
       "      <td>0.048974</td>\n",
       "      <td>3201</td>\n",
       "      <td>Manage your personal budget and work out how b...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99621554 0.00378446]</td>\n",
       "      <td>0.024883</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1556</td>\n",
       "      <td>@coffeedreamer1 @kingssnacks Omg me too I have...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>[0.98205954 0.01794046]</td>\n",
       "      <td>0.089912</td>\n",
       "      <td>1949</td>\n",
       "      <td>Teenage daughters and I learned how to play \"C...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>[0.99273302 0.00726698]</td>\n",
       "      <td>0.043026</td>\n",
       "      <td>same_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1261</td>\n",
       "      <td>bye Obama, luv u</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>[0.99536345 0.00463655]</td>\n",
       "      <td>0.029542</td>\n",
       "      <td>1904</td>\n",
       "      <td>I have more fun with my parents and boyfriend ...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>[0.9510251 0.0489749]</td>\n",
       "      <td>0.195486</td>\n",
       "      <td>same_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2154</td>\n",
       "      <td>Well that was humiliating for India. No clouds...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>[0.99223256 0.00776744]</td>\n",
       "      <td>0.045470</td>\n",
       "      <td>2957</td>\n",
       "      <td>me: *finishes book 100 of the year, gets excit...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>[0.98345804 0.01654196]</td>\n",
       "      <td>0.084257</td>\n",
       "      <td>same_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2259</td>\n",
       "      <td>how close i am to losing it. https://t.co/iQ5o...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99434516 0.00565484]</td>\n",
       "      <td>0.034904</td>\n",
       "      <td>2697</td>\n",
       "      <td>Dating apps are like trying to find a snack in...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99468073 0.00531927]</td>\n",
       "      <td>0.033159</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>2343</td>\n",
       "      <td>the media really likes the word 'allegedly', i...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>[0.96134311 0.03865689]</td>\n",
       "      <td>0.163652</td>\n",
       "      <td>2517</td>\n",
       "      <td>If the COVID variants make its way to the lett...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>[0.9950097 0.0049903]</td>\n",
       "      <td>0.031428</td>\n",
       "      <td>same_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>3317</td>\n",
       "      <td>craving hot sauce</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.98899906 0.01100094]</td>\n",
       "      <td>0.060552</td>\n",
       "      <td>1488</td>\n",
       "      <td>Excited to share the latest addition to my #et...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99682422 0.00317578]</td>\n",
       "      <td>0.021438</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>2056</td>\n",
       "      <td>I don't follow the Women's Super League becaus...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99318085 0.00681915]</td>\n",
       "      <td>0.040810</td>\n",
       "      <td>1287</td>\n",
       "      <td>A message to all Muslims and Refugees: I'm sor...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99565069 0.00434931]</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>2264</td>\n",
       "      <td>@humorandanimals There is nothing better on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99400428 0.00599572]</td>\n",
       "      <td>0.036656</td>\n",
       "      <td>2033</td>\n",
       "      <td>Chris Krebs: There is no foreign power that is...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99495881 0.00504119]</td>\n",
       "      <td>0.031697</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>2335</td>\n",
       "      <td>it’s official, I’m the luckiest girl 🥺 my boyf...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99447769 0.00552231]</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>1481</td>\n",
       "      <td>Fascinating article about the Vatican’s financ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99456572 0.00543428]</td>\n",
       "      <td>0.033759</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label category  \\\n",
       "0     2356  u know those days where you just need to stare...      0     easy   \n",
       "1     1556  @coffeedreamer1 @kingssnacks Omg me too I have...      0     none   \n",
       "2     1261                                   bye Obama, luv u      0     none   \n",
       "3     2154  Well that was humiliating for India. No clouds...      0     none   \n",
       "4     2259  how close i am to losing it. https://t.co/iQ5o...      0     easy   \n",
       "...    ...                                                ...    ...      ...   \n",
       "3261  2343  the media really likes the word 'allegedly', i...      0     none   \n",
       "3262  3317                                  craving hot sauce      0     easy   \n",
       "3263  2056  I don't follow the Women's Super League becaus...      0     easy   \n",
       "3264  2264  @humorandanimals There is nothing better on th...      0     easy   \n",
       "3265  2335  it’s official, I’m the luckiest girl 🥺 my boyf...      0     easy   \n",
       "\n",
       "                      softmax   entropy  idx_2  \\\n",
       "0     [0.99150319 0.00849681]  0.048974   3201   \n",
       "1     [0.98205954 0.01794046]  0.089912   1949   \n",
       "2     [0.99536345 0.00463655]  0.029542   1904   \n",
       "3     [0.99223256 0.00776744]  0.045470   2957   \n",
       "4     [0.99434516 0.00565484]  0.034904   2697   \n",
       "...                       ...       ...    ...   \n",
       "3261  [0.96134311 0.03865689]  0.163652   2517   \n",
       "3262  [0.98899906 0.01100094]  0.060552   1488   \n",
       "3263  [0.99318085 0.00681915]  0.040810   1287   \n",
       "3264  [0.99400428 0.00599572]  0.036656   2033   \n",
       "3265  [0.99447769 0.00552231]  0.034217   1481   \n",
       "\n",
       "                                                 text_2  label_2 category_2  \\\n",
       "0     Manage your personal budget and work out how b...        0       easy   \n",
       "1     Teenage daughters and I learned how to play \"C...        0       none   \n",
       "2     I have more fun with my parents and boyfriend ...        0       none   \n",
       "3     me: *finishes book 100 of the year, gets excit...        0       none   \n",
       "4     Dating apps are like trying to find a snack in...        0       easy   \n",
       "...                                                 ...      ...        ...   \n",
       "3261  If the COVID variants make its way to the lett...        0       none   \n",
       "3262  Excited to share the latest addition to my #et...        0       easy   \n",
       "3263  A message to all Muslims and Refugees: I'm sor...        0       easy   \n",
       "3264  Chris Krebs: There is no foreign power that is...        0       easy   \n",
       "3265  Fascinating article about the Vatican’s financ...        0       easy   \n",
       "\n",
       "                    softmax_2  entropy_2 mixup_type  \n",
       "0     [0.99621554 0.00378446]   0.024883  same_easy  \n",
       "1     [0.99273302 0.00726698]   0.043026  same_none  \n",
       "2       [0.9510251 0.0489749]   0.195486  same_none  \n",
       "3     [0.98345804 0.01654196]   0.084257  same_none  \n",
       "4     [0.99468073 0.00531927]   0.033159  same_easy  \n",
       "...                       ...        ...        ...  \n",
       "3261    [0.9950097 0.0049903]   0.031428  same_none  \n",
       "3262  [0.99682422 0.00317578]   0.021438  same_easy  \n",
       "3263  [0.99565069 0.00434931]   0.027990  same_easy  \n",
       "3264  [0.99495881 0.00504119]   0.031697  same_easy  \n",
       "3265  [0.99456572 0.00543428]   0.033759  same_easy  \n",
       "\n",
       "[3266 rows x 13 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, include_none=True, use_label=False, use_entropy=True)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b090065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  1.0\n",
      "same category:  0.8706240487062404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>softmax</th>\n",
       "      <th>entropy</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>softmax_2</th>\n",
       "      <th>entropy_2</th>\n",
       "      <th>mixup_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1114</td>\n",
       "      <td>i want thai food Right Now :(</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99444999 0.00555001]</td>\n",
       "      <td>0.034361</td>\n",
       "      <td>1372</td>\n",
       "      <td>“there is no greater joy than to have an endle...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99459495 0.00540505]</td>\n",
       "      <td>0.033607</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2725</td>\n",
       "      <td>my new glasses prescription is messing with my...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99613885 0.00386115]</td>\n",
       "      <td>0.025309</td>\n",
       "      <td>2501</td>\n",
       "      <td>My adventure this week was going to a park and...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99174245 0.00825755]</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1529</td>\n",
       "      <td>Has anyone written about the complexity of sic...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.98996828 0.01003172]</td>\n",
       "      <td>0.056147</td>\n",
       "      <td>2073</td>\n",
       "      <td>hey @phoebe_bridgers i’m free tomorrow btw. Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.98022248 0.01977752]</td>\n",
       "      <td>0.097172</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428</td>\n",
       "      <td>Just disposed of a dead opossum. Good morning!</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.00816608 0.99183392]</td>\n",
       "      <td>0.047393</td>\n",
       "      <td>797</td>\n",
       "      <td>Afghanistan just completely crumbling after sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.08395871 0.91604129]</td>\n",
       "      <td>0.288333</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2134</td>\n",
       "      <td>Entering my third year of consistent yoga prac...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1538</td>\n",
       "      <td>I feel like most teens/women are not told that...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>3317</td>\n",
       "      <td>craving hot sauce</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.98899906 0.01100094]</td>\n",
       "      <td>0.060552</td>\n",
       "      <td>1488</td>\n",
       "      <td>Excited to share the latest addition to my #et...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99682422 0.00317578]</td>\n",
       "      <td>0.021438</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>2872</td>\n",
       "      <td>This bunch of players is something special. Re...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2549</td>\n",
       "      <td>if Covid didn’t happen, I’d have free travel r...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>2287</td>\n",
       "      <td>80% of #LittleHouseonthePrairie episodes invol...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.98806428 0.01193572]</td>\n",
       "      <td>0.064718</td>\n",
       "      <td>3027</td>\n",
       "      <td>horseshoe crabs are so sweet and gentle i love...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99697631 0.00302369]</td>\n",
       "      <td>0.020560</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>502</td>\n",
       "      <td>Shaving in the shower without your glasses/con...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.017154 0.982846]</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>297</td>\n",
       "      <td>The fact I nearly froze on my way to work this...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.01566353 0.98433647]</td>\n",
       "      <td>0.080644</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>2778</td>\n",
       "      <td>Why is asking for a letter of recommendation s...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99117625 0.00882375]</td>\n",
       "      <td>0.050524</td>\n",
       "      <td>2744</td>\n",
       "      <td>Something about being up late and seeing  #Too...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.996324 0.003676]</td>\n",
       "      <td>0.024277</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2628 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     1114                      i want thai food Right Now :(      0   \n",
       "1     2725  my new glasses prescription is messing with my...      0   \n",
       "2     1529  Has anyone written about the complexity of sic...      0   \n",
       "3      428     Just disposed of a dead opossum. Good morning!      1   \n",
       "4     2134  Entering my third year of consistent yoga prac...      0   \n",
       "...    ...                                                ...    ...   \n",
       "2623  3317                                  craving hot sauce      0   \n",
       "2624  2872  This bunch of players is something special. Re...      0   \n",
       "2625  2287  80% of #LittleHouseonthePrairie episodes invol...      0   \n",
       "2626   502  Shaving in the shower without your glasses/con...      1   \n",
       "2627  2778  Why is asking for a letter of recommendation s...      0   \n",
       "\n",
       "       category                  softmax   entropy  idx_2  \\\n",
       "0          easy  [0.99444999 0.00555001]  0.034361   1372   \n",
       "1          easy  [0.99613885 0.00386115]  0.025309   2501   \n",
       "2     ambiguous  [0.98996828 0.01003172]  0.056147   2073   \n",
       "3     ambiguous  [0.00816608 0.99183392]  0.047393    797   \n",
       "4          easy                      NaN       NaN   1538   \n",
       "...         ...                      ...       ...    ...   \n",
       "2623       easy  [0.98899906 0.01100094]  0.060552   1488   \n",
       "2624       easy                      NaN       NaN   2549   \n",
       "2625       easy  [0.98806428 0.01193572]  0.064718   3027   \n",
       "2626  ambiguous      [0.017154 0.982846]  0.086746    297   \n",
       "2627       easy  [0.99117625 0.00882375]  0.050524   2744   \n",
       "\n",
       "                                                 text_2  label_2 category_2  \\\n",
       "0     “there is no greater joy than to have an endle...        0       easy   \n",
       "1     My adventure this week was going to a park and...        0       easy   \n",
       "2     hey @phoebe_bridgers i’m free tomorrow btw. Th...        0  ambiguous   \n",
       "3     Afghanistan just completely crumbling after sp...        1  ambiguous   \n",
       "4     I feel like most teens/women are not told that...        0  ambiguous   \n",
       "...                                                 ...      ...        ...   \n",
       "2623  Excited to share the latest addition to my #et...        0       easy   \n",
       "2624  if Covid didn’t happen, I’d have free travel r...        0  ambiguous   \n",
       "2625  horseshoe crabs are so sweet and gentle i love...        0       easy   \n",
       "2626  The fact I nearly froze on my way to work this...        1  ambiguous   \n",
       "2627  Something about being up late and seeing  #Too...        0       easy   \n",
       "\n",
       "                    softmax_2  entropy_2      mixup_type  \n",
       "0     [0.99459495 0.00540505]   0.033607       same_easy  \n",
       "1     [0.99174245 0.00825755]   0.047832       same_easy  \n",
       "2     [0.98022248 0.01977752]   0.097172  same_ambiguous  \n",
       "3     [0.08395871 0.91604129]   0.288333  same_ambiguous  \n",
       "4                         NaN        NaN  easy_ambiguous  \n",
       "...                       ...        ...             ...  \n",
       "2623  [0.99682422 0.00317578]   0.021438       same_easy  \n",
       "2624                      NaN        NaN  easy_ambiguous  \n",
       "2625  [0.99697631 0.00302369]   0.020560       same_easy  \n",
       "2626  [0.01566353 0.98433647]   0.080644  same_ambiguous  \n",
       "2627      [0.996324 0.003676]   0.024277       same_easy  \n",
       "\n",
       "[2628 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, use_label=True, use_entropy=True)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6fef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixupDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        data: pd.DataFrame,\n",
    "        sampling_type: str\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.sampling_type = sampling_type\n",
    "        \n",
    "        if self.sampling_type == 'sequential':\n",
    "            sorting_dict = {\n",
    "                'non_mixup_easy': 0,\n",
    "                'non_mixup_ambiguous': 1,\n",
    "                'same_easy': 2,\n",
    "                'different_easy': 3,\n",
    "                'same_ambiguous': 4,\n",
    "                'different_ambiguous': 5\n",
    "            }\n",
    "            self.data['data_type'] = self.data['mixup_type'] + '_' + self.data['category']\n",
    "            self.data = self.data.iloc[self.data.data_type.map(sorting_dict).argsort()].reset_index(drop=True)\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenizer.batch_encode_plus(\n",
    "            self.data[INPUT_COLUMN].tolist(),\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,            \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "\n",
    "                \n",
    "    def __len__(\n",
    "        self\n",
    "    ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        data = {\n",
    "            'input_ids_1': self.tokenized_data['input_ids'][index].flatten(),\n",
    "            'attention_mask_1': self.tokenized_data['attention_mask'][index].flatten(),\n",
    "            'labels_1': torch.tensor(self.data.iloc[index][OUTPUT_COLUMN], dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        idx2 = self.data.iloc[index]['idx_2']\n",
    "\n",
    "        index2 = self.data[self.data['idx'] == idx2].index[0]\n",
    "        data['input_ids_2'] = self.tokenized_data['input_ids'][index2].flatten()\n",
    "        data['attention_mask_2'] = self.tokenized_data['attention_mask'][index2].flatten()\n",
    "        data['labels_2'] = torch.tensor(self.data.iloc[index2][OUTPUT_COLUMN], dtype=torch.long)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f241da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MixupDataset(tokenizer=tokenizer, data=dataset, sampling_type='random')\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6631d2",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e8145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, optimizer, device, train_loader, num_epochs, output_dir):\n",
    "    losses = []\n",
    "    train_iterator = trange(int(num_epochs), desc='Epoch')\n",
    "    for _ in train_iterator:\n",
    "        tr_loss = 0\n",
    "        step = None\n",
    "        epoch_iterator = tqdm(train_loader, desc='Training')\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "\n",
    "            inputs = {}\n",
    "            for k, v in batch.items():\n",
    "                if isinstance(v, list):\n",
    "                    inputs[k] = None\n",
    "                else:\n",
    "                    inputs[k] = v.to(device)\n",
    "\n",
    "            labels = inputs['labels_1']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            out = outputs['logits'].double().to(device)\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            tr_loss += loss.item()\n",
    "        losses.append(tr_loss/(step+1))\n",
    "        print('train loss: {}'.format(tr_loss/(step+1)))\n",
    "\n",
    "    # save model and tokenizer\n",
    "    print('Saving model and tokenizer')\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69183448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, eval_loader, device, with_labels=True):\n",
    "    probs = None\n",
    "    gold_labels = None\n",
    "\n",
    "    eval_loss = 0\n",
    "    step = None\n",
    "    eval_iterator = tqdm(eval_loader, desc='Evaluating')\n",
    "    for step, batch in enumerate(eval_iterator):\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            inputs = {k:v.to(device) for k, v in batch.items()}\n",
    "            labels = inputs['labels_1']\n",
    "            del inputs['labels_1']\n",
    "            del inputs['labels_2']\n",
    " \n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            out = outputs['logits'].double().to(device)\n",
    "            out = F.softmax(out, dim=1)\n",
    "\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            if probs is None:\n",
    "                probs = out.detach().cpu().numpy()\n",
    "                if with_labels:\n",
    "                    gold_labels = labels.detach().cpu().numpy()\n",
    "            else:\n",
    "                probs = np.append(probs, out.detach().cpu().numpy(), axis=0)\n",
    "                if with_labels:\n",
    "                    gold_labels = np.append(gold_labels, labels.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            if with_labels:\n",
    "                eval_loss += loss.item()\n",
    "    \n",
    "    if with_labels:\n",
    "        eval_loss /= (step+1)\n",
    "        print('eval loss: {}'.format(eval_loss))\n",
    "\n",
    "        # compute accuracy\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        accuracy = np.sum(preds == gold_labels)/len(preds)\n",
    "        print('eval accuracy: {}'.format(accuracy))\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b1df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a6f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 8250, 8250, 8250, 0.33)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/imdb/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_imdb = json.load(f)\n",
    "f.close()\n",
    "print(data_imdb.keys())\n",
    "\n",
    "df_imdb = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/imdb/train.csv')\n",
    "len(df_imdb), len(data_imdb['easy']), len(data_imdb['ambiguous']), len(data_imdb['hard']), len(data_imdb['easy'])/len(df_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01fad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " set(),\n",
       " {0,\n",
       "  13,\n",
       "  14,\n",
       "  20,\n",
       "  23,\n",
       "  32,\n",
       "  39,\n",
       "  48,\n",
       "  50,\n",
       "  52,\n",
       "  53,\n",
       "  55,\n",
       "  61,\n",
       "  65,\n",
       "  69,\n",
       "  70,\n",
       "  74,\n",
       "  77,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  84,\n",
       "  87,\n",
       "  89,\n",
       "  90,\n",
       "  92,\n",
       "  97,\n",
       "  98,\n",
       "  105,\n",
       "  108,\n",
       "  109,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  130,\n",
       "  132,\n",
       "  133,\n",
       "  136,\n",
       "  138,\n",
       "  142,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  163,\n",
       "  167,\n",
       "  168,\n",
       "  171,\n",
       "  173,\n",
       "  179,\n",
       "  180,\n",
       "  188,\n",
       "  190,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  224,\n",
       "  228,\n",
       "  233,\n",
       "  236,\n",
       "  245,\n",
       "  246,\n",
       "  252,\n",
       "  255,\n",
       "  257,\n",
       "  260,\n",
       "  263,\n",
       "  264,\n",
       "  267,\n",
       "  271,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  285,\n",
       "  289,\n",
       "  292,\n",
       "  299,\n",
       "  301,\n",
       "  304,\n",
       "  306,\n",
       "  313,\n",
       "  318,\n",
       "  336,\n",
       "  339,\n",
       "  340,\n",
       "  342,\n",
       "  345,\n",
       "  346,\n",
       "  348,\n",
       "  354,\n",
       "  362,\n",
       "  366,\n",
       "  367,\n",
       "  371,\n",
       "  373,\n",
       "  375,\n",
       "  377,\n",
       "  378,\n",
       "  384,\n",
       "  387,\n",
       "  394,\n",
       "  397,\n",
       "  403,\n",
       "  405,\n",
       "  406,\n",
       "  408,\n",
       "  410,\n",
       "  413,\n",
       "  418,\n",
       "  419,\n",
       "  421,\n",
       "  427,\n",
       "  430,\n",
       "  440,\n",
       "  441,\n",
       "  446,\n",
       "  455,\n",
       "  457,\n",
       "  459,\n",
       "  461,\n",
       "  464,\n",
       "  469,\n",
       "  471,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  481,\n",
       "  483,\n",
       "  488,\n",
       "  492,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  501,\n",
       "  504,\n",
       "  509,\n",
       "  511,\n",
       "  515,\n",
       "  516,\n",
       "  518,\n",
       "  522,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  532,\n",
       "  540,\n",
       "  543,\n",
       "  544,\n",
       "  552,\n",
       "  553,\n",
       "  564,\n",
       "  567,\n",
       "  569,\n",
       "  587,\n",
       "  591,\n",
       "  593,\n",
       "  603,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  614,\n",
       "  615,\n",
       "  618,\n",
       "  623,\n",
       "  624,\n",
       "  628,\n",
       "  634,\n",
       "  642,\n",
       "  650,\n",
       "  653,\n",
       "  655,\n",
       "  671,\n",
       "  672,\n",
       "  685,\n",
       "  687,\n",
       "  688,\n",
       "  695,\n",
       "  699,\n",
       "  701,\n",
       "  704,\n",
       "  706,\n",
       "  707,\n",
       "  710,\n",
       "  712,\n",
       "  721,\n",
       "  724,\n",
       "  726,\n",
       "  727,\n",
       "  737,\n",
       "  739,\n",
       "  742,\n",
       "  744,\n",
       "  750,\n",
       "  751,\n",
       "  753,\n",
       "  758,\n",
       "  766,\n",
       "  768,\n",
       "  772,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  781,\n",
       "  783,\n",
       "  784,\n",
       "  788,\n",
       "  790,\n",
       "  791,\n",
       "  793,\n",
       "  794,\n",
       "  802,\n",
       "  815,\n",
       "  816,\n",
       "  825,\n",
       "  827,\n",
       "  828,\n",
       "  832,\n",
       "  835,\n",
       "  838,\n",
       "  845,\n",
       "  848,\n",
       "  853,\n",
       "  858,\n",
       "  860,\n",
       "  863,\n",
       "  865,\n",
       "  868,\n",
       "  871,\n",
       "  872,\n",
       "  876,\n",
       "  877,\n",
       "  879,\n",
       "  885,\n",
       "  886,\n",
       "  888,\n",
       "  891,\n",
       "  893,\n",
       "  895,\n",
       "  896,\n",
       "  899,\n",
       "  905,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  918,\n",
       "  922,\n",
       "  924,\n",
       "  926,\n",
       "  930,\n",
       "  932,\n",
       "  939,\n",
       "  954,\n",
       "  964,\n",
       "  965,\n",
       "  972,\n",
       "  974,\n",
       "  976,\n",
       "  990,\n",
       "  993,\n",
       "  994,\n",
       "  1001,\n",
       "  1009,\n",
       "  1015,\n",
       "  1022,\n",
       "  1026,\n",
       "  1029,\n",
       "  1031,\n",
       "  1034,\n",
       "  1035,\n",
       "  1038,\n",
       "  1040,\n",
       "  1050,\n",
       "  1073,\n",
       "  1075,\n",
       "  1079,\n",
       "  1081,\n",
       "  1082,\n",
       "  1092,\n",
       "  1097,\n",
       "  1106,\n",
       "  1107,\n",
       "  1112,\n",
       "  1114,\n",
       "  1117,\n",
       "  1119,\n",
       "  1121,\n",
       "  1123,\n",
       "  1124,\n",
       "  1125,\n",
       "  1127,\n",
       "  1131,\n",
       "  1142,\n",
       "  1145,\n",
       "  1146,\n",
       "  1152,\n",
       "  1154,\n",
       "  1162,\n",
       "  1166,\n",
       "  1168,\n",
       "  1170,\n",
       "  1175,\n",
       "  1177,\n",
       "  1181,\n",
       "  1187,\n",
       "  1197,\n",
       "  1200,\n",
       "  1204,\n",
       "  1210,\n",
       "  1212,\n",
       "  1214,\n",
       "  1221,\n",
       "  1224,\n",
       "  1225,\n",
       "  1227,\n",
       "  1233,\n",
       "  1242,\n",
       "  1244,\n",
       "  1246,\n",
       "  1251,\n",
       "  1256,\n",
       "  1261,\n",
       "  1263,\n",
       "  1267,\n",
       "  1270,\n",
       "  1275,\n",
       "  1277,\n",
       "  1279,\n",
       "  1281,\n",
       "  1282,\n",
       "  1283,\n",
       "  1285,\n",
       "  1289,\n",
       "  1293,\n",
       "  1296,\n",
       "  1297,\n",
       "  1306,\n",
       "  1308,\n",
       "  1310,\n",
       "  1318,\n",
       "  1321,\n",
       "  1331,\n",
       "  1336,\n",
       "  1340,\n",
       "  1341,\n",
       "  1345,\n",
       "  1347,\n",
       "  1348,\n",
       "  1349,\n",
       "  1351,\n",
       "  1353,\n",
       "  1362,\n",
       "  1367,\n",
       "  1370,\n",
       "  1371,\n",
       "  1372,\n",
       "  1375,\n",
       "  1376,\n",
       "  1377,\n",
       "  1381,\n",
       "  1386,\n",
       "  1390,\n",
       "  1391,\n",
       "  1400,\n",
       "  1401,\n",
       "  1419,\n",
       "  1421,\n",
       "  1422,\n",
       "  1434,\n",
       "  1457,\n",
       "  1466,\n",
       "  1467,\n",
       "  1469,\n",
       "  1477,\n",
       "  1478,\n",
       "  1485,\n",
       "  1493,\n",
       "  1496,\n",
       "  1498,\n",
       "  1499,\n",
       "  1508,\n",
       "  1510,\n",
       "  1511,\n",
       "  1515,\n",
       "  1518,\n",
       "  1523,\n",
       "  1525,\n",
       "  1526,\n",
       "  1530,\n",
       "  1533,\n",
       "  1534,\n",
       "  1539,\n",
       "  1550,\n",
       "  1565,\n",
       "  1568,\n",
       "  1570,\n",
       "  1578,\n",
       "  1582,\n",
       "  1583,\n",
       "  1584,\n",
       "  1585,\n",
       "  1586,\n",
       "  1590,\n",
       "  1594,\n",
       "  1599,\n",
       "  1603,\n",
       "  1606,\n",
       "  1612,\n",
       "  1613,\n",
       "  1614,\n",
       "  1616,\n",
       "  1622,\n",
       "  1643,\n",
       "  1646,\n",
       "  1648,\n",
       "  1651,\n",
       "  1654,\n",
       "  1655,\n",
       "  1661,\n",
       "  1664,\n",
       "  1666,\n",
       "  1668,\n",
       "  1673,\n",
       "  1674,\n",
       "  1675,\n",
       "  1680,\n",
       "  1681,\n",
       "  1686,\n",
       "  1691,\n",
       "  1701,\n",
       "  1705,\n",
       "  1711,\n",
       "  1716,\n",
       "  1722,\n",
       "  1723,\n",
       "  1724,\n",
       "  1729,\n",
       "  1737,\n",
       "  1740,\n",
       "  1755,\n",
       "  1757,\n",
       "  1760,\n",
       "  1773,\n",
       "  1776,\n",
       "  1779,\n",
       "  1782,\n",
       "  1783,\n",
       "  1784,\n",
       "  1788,\n",
       "  1790,\n",
       "  1791,\n",
       "  1792,\n",
       "  1793,\n",
       "  1796,\n",
       "  1797,\n",
       "  1799,\n",
       "  1810,\n",
       "  1811,\n",
       "  1822,\n",
       "  1823,\n",
       "  1831,\n",
       "  1833,\n",
       "  1835,\n",
       "  1841,\n",
       "  1842,\n",
       "  1850,\n",
       "  1851,\n",
       "  1852,\n",
       "  1853,\n",
       "  1856,\n",
       "  1861,\n",
       "  1866,\n",
       "  1867,\n",
       "  1870,\n",
       "  1879,\n",
       "  1891,\n",
       "  1894,\n",
       "  1895,\n",
       "  1897,\n",
       "  1899,\n",
       "  1903,\n",
       "  1905,\n",
       "  1907,\n",
       "  1908,\n",
       "  1910,\n",
       "  1917,\n",
       "  1931,\n",
       "  1937,\n",
       "  1940,\n",
       "  1947,\n",
       "  1949,\n",
       "  1955,\n",
       "  1959,\n",
       "  1961,\n",
       "  1963,\n",
       "  1964,\n",
       "  1968,\n",
       "  1969,\n",
       "  1974,\n",
       "  1984,\n",
       "  1986,\n",
       "  1993,\n",
       "  1994,\n",
       "  1997,\n",
       "  1999,\n",
       "  2000,\n",
       "  2004,\n",
       "  2008,\n",
       "  2013,\n",
       "  2015,\n",
       "  2016,\n",
       "  2021,\n",
       "  2025,\n",
       "  2028,\n",
       "  2029,\n",
       "  2033,\n",
       "  2034,\n",
       "  2039,\n",
       "  2040,\n",
       "  2042,\n",
       "  2043,\n",
       "  2044,\n",
       "  2046,\n",
       "  2052,\n",
       "  2053,\n",
       "  2056,\n",
       "  2057,\n",
       "  2066,\n",
       "  2073,\n",
       "  2079,\n",
       "  2080,\n",
       "  2081,\n",
       "  2083,\n",
       "  2085,\n",
       "  2090,\n",
       "  2096,\n",
       "  2097,\n",
       "  2101,\n",
       "  2103,\n",
       "  2104,\n",
       "  2106,\n",
       "  2110,\n",
       "  2111,\n",
       "  2112,\n",
       "  2113,\n",
       "  2114,\n",
       "  2115,\n",
       "  2116,\n",
       "  2122,\n",
       "  2123,\n",
       "  2126,\n",
       "  2128,\n",
       "  2129,\n",
       "  2132,\n",
       "  2144,\n",
       "  2154,\n",
       "  2168,\n",
       "  2169,\n",
       "  2175,\n",
       "  2177,\n",
       "  2178,\n",
       "  2181,\n",
       "  2182,\n",
       "  2183,\n",
       "  2187,\n",
       "  2188,\n",
       "  2190,\n",
       "  2192,\n",
       "  2195,\n",
       "  2196,\n",
       "  2198,\n",
       "  2199,\n",
       "  2201,\n",
       "  2202,\n",
       "  2203,\n",
       "  2222,\n",
       "  2223,\n",
       "  2225,\n",
       "  2226,\n",
       "  2229,\n",
       "  2233,\n",
       "  2235,\n",
       "  2239,\n",
       "  2240,\n",
       "  2252,\n",
       "  2255,\n",
       "  2256,\n",
       "  2257,\n",
       "  2269,\n",
       "  2270,\n",
       "  2271,\n",
       "  2272,\n",
       "  2279,\n",
       "  2283,\n",
       "  2294,\n",
       "  2300,\n",
       "  2301,\n",
       "  2305,\n",
       "  2312,\n",
       "  2315,\n",
       "  2316,\n",
       "  2321,\n",
       "  2324,\n",
       "  2338,\n",
       "  2344,\n",
       "  2346,\n",
       "  2359,\n",
       "  2362,\n",
       "  2365,\n",
       "  2371,\n",
       "  2379,\n",
       "  2380,\n",
       "  2387,\n",
       "  2392,\n",
       "  2394,\n",
       "  2395,\n",
       "  2397,\n",
       "  2405,\n",
       "  2406,\n",
       "  2407,\n",
       "  2414,\n",
       "  2415,\n",
       "  2418,\n",
       "  2419,\n",
       "  2422,\n",
       "  2429,\n",
       "  2439,\n",
       "  2444,\n",
       "  2447,\n",
       "  2449,\n",
       "  2451,\n",
       "  2455,\n",
       "  2456,\n",
       "  2458,\n",
       "  2459,\n",
       "  2460,\n",
       "  2463,\n",
       "  2464,\n",
       "  2467,\n",
       "  2473,\n",
       "  2476,\n",
       "  2477,\n",
       "  2478,\n",
       "  2479,\n",
       "  2481,\n",
       "  2482,\n",
       "  2485,\n",
       "  2496,\n",
       "  2502,\n",
       "  2504,\n",
       "  2511,\n",
       "  2512,\n",
       "  2524,\n",
       "  2525,\n",
       "  2528,\n",
       "  2529,\n",
       "  2530,\n",
       "  2532,\n",
       "  2534,\n",
       "  2535,\n",
       "  2536,\n",
       "  2537,\n",
       "  2539,\n",
       "  2541,\n",
       "  2544,\n",
       "  2545,\n",
       "  2547,\n",
       "  2553,\n",
       "  2559,\n",
       "  2562,\n",
       "  2565,\n",
       "  2568,\n",
       "  2570,\n",
       "  2585,\n",
       "  2596,\n",
       "  2601,\n",
       "  2618,\n",
       "  2619,\n",
       "  2621,\n",
       "  2625,\n",
       "  2627,\n",
       "  2628,\n",
       "  2632,\n",
       "  2635,\n",
       "  2645,\n",
       "  2650,\n",
       "  2659,\n",
       "  2674,\n",
       "  2675,\n",
       "  2679,\n",
       "  2686,\n",
       "  2689,\n",
       "  2692,\n",
       "  2694,\n",
       "  2699,\n",
       "  2708,\n",
       "  2713,\n",
       "  2715,\n",
       "  2716,\n",
       "  2717,\n",
       "  2718,\n",
       "  2719,\n",
       "  2723,\n",
       "  2733,\n",
       "  2740,\n",
       "  2742,\n",
       "  2743,\n",
       "  2747,\n",
       "  2757,\n",
       "  2758,\n",
       "  2761,\n",
       "  2768,\n",
       "  2769,\n",
       "  2771,\n",
       "  2774,\n",
       "  2775,\n",
       "  2776,\n",
       "  2777,\n",
       "  2778,\n",
       "  2779,\n",
       "  2780,\n",
       "  2783,\n",
       "  2785,\n",
       "  2788,\n",
       "  2789,\n",
       "  2802,\n",
       "  2803,\n",
       "  2804,\n",
       "  2806,\n",
       "  2810,\n",
       "  2818,\n",
       "  2819,\n",
       "  2825,\n",
       "  2828,\n",
       "  2835,\n",
       "  2836,\n",
       "  2837,\n",
       "  2838,\n",
       "  2841,\n",
       "  2845,\n",
       "  2846,\n",
       "  2849,\n",
       "  2854,\n",
       "  2865,\n",
       "  2882,\n",
       "  2885,\n",
       "  2890,\n",
       "  2892,\n",
       "  2902,\n",
       "  2904,\n",
       "  2911,\n",
       "  2916,\n",
       "  2917,\n",
       "  2920,\n",
       "  2924,\n",
       "  2927,\n",
       "  2928,\n",
       "  2931,\n",
       "  2932,\n",
       "  2941,\n",
       "  2943,\n",
       "  2950,\n",
       "  2955,\n",
       "  2964,\n",
       "  2968,\n",
       "  2969,\n",
       "  2970,\n",
       "  2971,\n",
       "  2974,\n",
       "  2993,\n",
       "  3001,\n",
       "  3002,\n",
       "  3003,\n",
       "  3006,\n",
       "  3007,\n",
       "  3009,\n",
       "  3012,\n",
       "  3015,\n",
       "  3021,\n",
       "  3022,\n",
       "  3023,\n",
       "  3024,\n",
       "  3025,\n",
       "  3026,\n",
       "  3030,\n",
       "  3031,\n",
       "  3032,\n",
       "  3034,\n",
       "  3035,\n",
       "  3036,\n",
       "  3037,\n",
       "  3038,\n",
       "  3040,\n",
       "  3042,\n",
       "  3043,\n",
       "  3045,\n",
       "  3046,\n",
       "  3050,\n",
       "  3072,\n",
       "  3073,\n",
       "  3074,\n",
       "  3075,\n",
       "  3076,\n",
       "  3085,\n",
       "  3087,\n",
       "  3089,\n",
       "  3090,\n",
       "  3093,\n",
       "  3094,\n",
       "  3095,\n",
       "  3096,\n",
       "  3100,\n",
       "  3101,\n",
       "  3104,\n",
       "  3112,\n",
       "  3117,\n",
       "  3123,\n",
       "  3124,\n",
       "  3126,\n",
       "  3128,\n",
       "  3130,\n",
       "  3131,\n",
       "  3139,\n",
       "  3140,\n",
       "  3144,\n",
       "  3146,\n",
       "  3147,\n",
       "  3149,\n",
       "  3151,\n",
       "  3152,\n",
       "  3155,\n",
       "  3161,\n",
       "  3162,\n",
       "  3168,\n",
       "  3177,\n",
       "  3183,\n",
       "  3192,\n",
       "  3199,\n",
       "  3201,\n",
       "  3203,\n",
       "  3206,\n",
       "  3209,\n",
       "  3215,\n",
       "  3221,\n",
       "  3223,\n",
       "  3227,\n",
       "  3238,\n",
       "  3247,\n",
       "  3249,\n",
       "  3251,\n",
       "  3252,\n",
       "  3258,\n",
       "  3259,\n",
       "  3261,\n",
       "  3263,\n",
       "  3264,\n",
       "  3265,\n",
       "  3266,\n",
       "  3272,\n",
       "  3273,\n",
       "  3278,\n",
       "  3289,\n",
       "  3296,\n",
       "  3300,\n",
       "  3303,\n",
       "  3306,\n",
       "  3308,\n",
       "  3310,\n",
       "  3311,\n",
       "  3313,\n",
       "  3321,\n",
       "  3322,\n",
       "  3323,\n",
       "  3329,\n",
       "  3332,\n",
       "  3333,\n",
       "  3338,\n",
       "  3344,\n",
       "  3345,\n",
       "  3351,\n",
       "  3354,\n",
       "  3355,\n",
       "  3357,\n",
       "  3360,\n",
       "  3362,\n",
       "  3363,\n",
       "  3368,\n",
       "  3370,\n",
       "  3375,\n",
       "  3382,\n",
       "  3385,\n",
       "  3391,\n",
       "  3394,\n",
       "  3396,\n",
       "  3400,\n",
       "  3403,\n",
       "  3405,\n",
       "  3407,\n",
       "  3408,\n",
       "  3416,\n",
       "  3419,\n",
       "  3422,\n",
       "  3423,\n",
       "  3426,\n",
       "  3427,\n",
       "  3434,\n",
       "  3436,\n",
       "  3437,\n",
       "  3438,\n",
       "  3439,\n",
       "  3440,\n",
       "  3441,\n",
       "  3445,\n",
       "  3449,\n",
       "  3453,\n",
       "  3457,\n",
       "  3459,\n",
       "  3460,\n",
       "  3466,\n",
       "  3472,\n",
       "  3473,\n",
       "  3477,\n",
       "  3480,\n",
       "  3484,\n",
       "  3486,\n",
       "  3489,\n",
       "  3491,\n",
       "  3493,\n",
       "  3496,\n",
       "  3497,\n",
       "  3498,\n",
       "  3499,\n",
       "  3506,\n",
       "  3510,\n",
       "  3523,\n",
       "  3524,\n",
       "  3525,\n",
       "  3531,\n",
       "  3537,\n",
       "  3551,\n",
       "  3555,\n",
       "  3558,\n",
       "  3561,\n",
       "  3563,\n",
       "  3570,\n",
       "  3572,\n",
       "  3574,\n",
       "  3582,\n",
       "  3583,\n",
       "  3587,\n",
       "  3596,\n",
       "  3597,\n",
       "  3599,\n",
       "  3603,\n",
       "  3606,\n",
       "  3607,\n",
       "  3609,\n",
       "  3610,\n",
       "  3611,\n",
       "  3615,\n",
       "  3616,\n",
       "  3617,\n",
       "  3619,\n",
       "  3625,\n",
       "  3631,\n",
       "  3632,\n",
       "  3633,\n",
       "  3635,\n",
       "  3639,\n",
       "  3640,\n",
       "  3641,\n",
       "  3643,\n",
       "  3644,\n",
       "  3646,\n",
       "  3648,\n",
       "  3654,\n",
       "  3657,\n",
       "  3658,\n",
       "  3660,\n",
       "  3666,\n",
       "  3670,\n",
       "  3671,\n",
       "  3672,\n",
       "  3674,\n",
       "  3682,\n",
       "  3684,\n",
       "  3694,\n",
       "  3700,\n",
       "  3704,\n",
       "  3708,\n",
       "  3709,\n",
       "  3711,\n",
       "  3714,\n",
       "  3722,\n",
       "  3726,\n",
       "  3727,\n",
       "  3728,\n",
       "  3729,\n",
       "  3732,\n",
       "  ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_imdb['easy']).intersection(set(data_imdb['ambiguous'])), set(data_imdb['easy']).intersection(set(data_imdb['hard'])), set(data_imdb['hard']).intersection(set(data_imdb['ambiguous']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d082d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67349, 22225, 22225, 22225, 0.3299974758348305)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/sst2/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_sst2 = json.load(f)\n",
    "f.close()\n",
    "print(data_sst2.keys())\n",
    "df_sst2 = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/sst2/train.csv')\n",
    "len(df_sst2), len(data_sst2['easy']), len(data_sst2['ambiguous']), len(data_sst2['hard']), len(data_sst2['easy'])/len(df_sst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23f6304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(560000, 184800, 184800, 184800, 0.33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/yelp_polarity/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_yelp_polarity = json.load(f)\n",
    "f.close()\n",
    "print(data_yelp_polarity.keys())\n",
    "df_yelp_polarity = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/yelp_polarity/train.csv')\n",
    "len(df_yelp_polarity), len(data_yelp_polarity['easy']), len(data_yelp_polarity['ambiguous']), len(data_yelp_polarity['hard']), len(data_yelp_polarity['easy'])/len(df_yelp_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acbb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
