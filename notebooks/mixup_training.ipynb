{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7016ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fire\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    set_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9defa",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f39739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_COLUMN:  text\n",
      "DATA_COLUMN:  category\n",
      "OUTPUT_COLUMN:  label\n",
      "NUM_EPOCHS:  2\n",
      "MAX_LEN:  256\n",
      "BATCH_SIZE:  32\n",
      "LAMBDA:  0.5\n",
      "FLAG:  False\n",
      "MIXUP_START:  10\n"
     ]
    }
   ],
   "source": [
    "FLAG = False\n",
    "MIXUP_START = 10\n",
    "LAMBDA = 0.5\n",
    "\n",
    "INPUT_COLUMN = 'text'\n",
    "DATA_COLUMN = 'category'\n",
    "OUTPUT_COLUMN = 'label'\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"INPUT_COLUMN: \", INPUT_COLUMN)\n",
    "print(\"DATA_COLUMN: \", DATA_COLUMN)\n",
    "print(\"OUTPUT_COLUMN: \", OUTPUT_COLUMN)\n",
    "print(\"NUM_EPOCHS: \", NUM_EPOCHS)\n",
    "print(\"MAX_LEN: \", MAX_LEN)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"LAMBDA: \", LAMBDA)\n",
    "print(\"FLAG: \", FLAG)\n",
    "print(\"MIXUP_START: \", MIXUP_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9b61a",
   "metadata": {},
   "source": [
    "# Mixup Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83ce9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaPreTrainedModel,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaPooler,\n",
    "    RobertaClassificationHead\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    SequenceClassifierOutput\n",
    ")\n",
    "\n",
    "# from config import *\n",
    "\n",
    "\n",
    "class RobertaMixerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class RobertaMixerModel(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n",
    "    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
    "    Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = RobertaMixerEncoder(config)\n",
    "\n",
    "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "\n",
    "            \n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class RobertaMixerForSequenceClassification(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaMixerModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        if FLAG:\n",
    "            self.mixup_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "            self.mixup_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "            self.mixup_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_1: Optional[torch.LongTensor] = None,\n",
    "        attention_mask_1: Optional[torch.FloatTensor] = None,\n",
    "        input_ids_2: Optional[torch.LongTensor] = None,\n",
    "        attention_mask_2: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels_1: Optional[torch.LongTensor] = None,\n",
    "        labels_2: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs_1 = self.roberta(\n",
    "            input_ids_1,\n",
    "            attention_mask=attention_mask_1,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output_1 = outputs_1[0]\n",
    "        \n",
    "        # Mixup train\n",
    "        if (input_ids_2 is not None) and (attention_mask_2 is not None) and (labels_2 is not None):\n",
    "            \n",
    "            outputs_2 = self.roberta(\n",
    "                input_ids_2,\n",
    "                attention_mask=attention_mask_2,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            sequence_output_2 = outputs_2[0]\n",
    "\n",
    "            sequence_output = (LAMBDA * sequence_output_1) + ((1 - LAMBDA) * sequence_output_2)\n",
    "\n",
    "            if FLAG:\n",
    "                sequence_output = self.mixup_dense(sequence_output)\n",
    "                sequence_output = self.mixup_layernorm(sequence_output)\n",
    "                sequence_output = self.mixup_dropout(sequence_output)\n",
    "\n",
    "            logits = self.classifier(sequence_output)\n",
    "\n",
    "            loss = None\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = (LAMBDA * loss_fct(logits.view(-1, self.num_labels), labels_1.view(-1))) + ((1 - LAMBDA) * loss_fct(logits.view(-1, self.num_labels), labels_2.view(-1)))\n",
    "\n",
    "        # Mixup eval\n",
    "        else:\n",
    "            logits = self.classifier(sequence_output_1)\n",
    "            loss = None\n",
    "            if labels_1 is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels_1.view(-1))\n",
    "         \n",
    "        # Return logits, loss, and hidden states\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs_1[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs_1.hidden_states,\n",
    "            attentions=outputs_1.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1db9e",
   "metadata": {},
   "source": [
    "## MixupDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875aa2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a829e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(data, info_data=None, mixup=False):\n",
    "    \n",
    "#     # Remove none and hard examples\n",
    "#     data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "\n",
    "#     # Add softmax and entropy info\n",
    "#     if (info_data is not None) and mixup:\n",
    "#         data = pd.merge(data, info_data, on='idx')[['idx', 'text', 'label', 'category', 'softmax', 'entropy']]\n",
    "#         mixup_size = len(info_data) - len(data)\n",
    "\n",
    "#         # --------------------------------------- Same class mixup ---------------------------------------  \n",
    "\n",
    "#         # Easy-Easy Mixup\n",
    "#         easy_data = data[data['category'] == 'easy']\n",
    "#         easy_low_ent_idx = easy_data.sort_values('entropy', ascending=True).head(mixup_size//3)['idx'].tolist()\n",
    "#         easy_high_ent_idx = easy_data.sort_values('entropy', ascending=False).head(mixup_size//3)['idx'].tolist()\n",
    "        \n",
    "#         easy_mixup_data = easy_data[easy_data['idx'].isin(easy_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(easy_high_ent_idx)\n",
    "#         easy_mixup_data['idx_2'] = easy_high_ent_idx\n",
    "#         easy_mixup_data['text_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['text'].values[0])\n",
    "#         easy_mixup_data['label_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['label'].values[0])\n",
    "#         easy_mixup_data['category_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['category'].values[0])\n",
    "#         easy_mixup_data['mixup_type'] = 'same_easy'\n",
    "        \n",
    "#         # Ambi-Ambi Mixup\n",
    "#         ambiguous_data = data[data['category'] == 'ambiguous']\n",
    "#         ambiguous_low_ent_idx = ambiguous_data.sort_values('entropy', ascending=True).head(mixup_size//3)['idx'].tolist()\n",
    "#         ambiguous_high_ent_idx = ambiguous_data.sort_values('entropy', ascending=False).head(mixup_size//3)['idx'].tolist()\n",
    "        \n",
    "#         ambiguous_mixup_data = ambiguous_data[ambiguous_data['idx'].isin(ambiguous_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(ambiguous_high_ent_idx)\n",
    "#         ambiguous_mixup_data['idx_2'] = ambiguous_high_ent_idx\n",
    "#         ambiguous_mixup_data['text_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['text'].values[0])\n",
    "#         ambiguous_mixup_data['label_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['label'].values[0])\n",
    "#         ambiguous_mixup_data['category_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['category'].values[0])\n",
    "#         ambiguous_mixup_data['mixup_type'] = 'same_ambiguous'\n",
    "        \n",
    "#         same_mixup_data = pd.concat([easy_mixup_data, ambiguous_mixup_data])\n",
    "        \n",
    "#         # --------------------------------------- Different class mixup ---------------------------------------  \n",
    "        \n",
    "#         # Random easy-ambi mixup\n",
    "#         different_samples = mixup_size - len(same_mixup_data)\n",
    "#         easy_tuple = list(zip(easy_data['idx'].tolist(), easy_data['text'].tolist(), easy_data['label'].tolist(), easy_data['category'].tolist()))\n",
    "#         ambiguous_tuple = list(zip(ambiguous_data['idx'].tolist(), ambiguous_data['text'].tolist(), ambiguous_data['label'].tolist(), ambiguous_data['category'].tolist()))\n",
    "        \n",
    "#         easy_data = easy_data.sample(n=different_samples//2).reset_index(drop=True)\n",
    "#         ambiguous4easy = random.choices(ambiguous_tuple, weights=np.ones(len(ambiguous_tuple)), k=different_samples//2)\n",
    "#         ambiguous4easy = pd.DataFrame(ambiguous4easy, columns=['idx_2', 'text_2', 'label_2', 'category_2'])\n",
    "#         ambiguous4easy = pd.concat([easy_data, ambiguous4easy], axis=1).reset_index(drop=True)\n",
    "#         ambiguous4easy['mixup_type'] = 'ambiguous_easy'\n",
    "        \n",
    "#         ambiguous_data = ambiguous_data.sample(n=different_samples//2).reset_index(drop=True)\n",
    "#         easy4ambiguous = random.choices(easy_tuple, weights=np.ones(len(easy_tuple)), k=different_samples//2)\n",
    "#         easy4ambiguous = pd.DataFrame(easy4ambiguous, columns=['idx_2', 'text_2', 'label_2', 'category_2'])\n",
    "#         easy4ambiguous = pd.concat([ambiguous_data, easy4ambiguous], axis=1).reset_index(drop=True)\n",
    "#         easy4ambiguous['mixup_type'] = 'easy_ambiguous'\n",
    "        \n",
    "#         return pd.concat([same_mixup_data, easy4ambiguous, ambiguous4easy]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892a511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_original(data, include_none=False):\n",
    "    if include_none:\n",
    "        return data[data['category'] != 'hard'].reset_index(drop=True)\n",
    "    else:\n",
    "        return data[(data['category'] != 'none') & (data['category'] != 'hard')].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafd0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_data(df, label=0, use_entropy=False):\n",
    "    df_label = df[df['label'] == label].reset_index(drop=True)\n",
    "    if use_entropy:\n",
    "        df_label = df_label.sort_values('entropy', ascending=True).reset_index(drop=True)\n",
    "        temp_label = df_label.sort_values('entropy', ascending=False).reset_index(drop=True)\n",
    "        temp_label = temp_label.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "        return pd.concat([df_label, temp_label], axis=1).reset_index(drop=True)\n",
    "    else:\n",
    "        temp_label = df_label.sample(frac=1).reset_index(drop=True)\n",
    "        temp_label = temp_label.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})   \n",
    "        return pd.concat([df_label, temp_label], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a458859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_random_mixup(data, info_data=None, include_none=False, use_label=False):\n",
    "    data_len = len(data)\n",
    "    \n",
    "    if include_none:\n",
    "        data = data[data['category'] != 'hard'].reset_index(drop=True)\n",
    "    else:\n",
    "        data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "    \n",
    "    if use_label:\n",
    "        data_0 = get_label_data(data, label=0)\n",
    "        data_1 = get_label_data(data, label=1)\n",
    "        final_data = pd.concat([data_0, data_1]).reset_index(drop=True)\n",
    "        \n",
    "    else:\n",
    "        temp = data.copy()\n",
    "        temp = temp.sample(frac=1).reset_index(drop=True)\n",
    "        temp = temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "        final_data = pd.concat([data, temp], axis=1)\n",
    "        \n",
    "    random_subset_1 = data.sample(n=data_len-len(final_data)).reset_index(drop=True)\n",
    "    random_subset_2 = data.sample(n=data_len-len(final_data)).reset_index(drop=True)\n",
    "    random_subset_2 = random_subset_2.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "    random_subset = pd.concat([random_subset_1, random_subset_2], axis=1).reset_index(drop=True)  \n",
    "\n",
    "    return pd.concat([final_data, random_subset]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886f894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_category_mixup(data, info_data=None, include_none=False, use_label=False, use_entropy=False):\n",
    "    data_len = len(data)\n",
    "    \n",
    "    if include_none:\n",
    "        data = data[data['category'] != 'hard'].reset_index(drop=True)\n",
    "    else:\n",
    "        data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "    \n",
    "    if use_entropy: \n",
    "        data = pd.merge(data, info_data, on='idx')[['idx', 'text', 'label', 'category', 'softmax', 'entropy']]\n",
    "\n",
    "        if use_label:\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_data_0 = get_label_data(easy_data, label=0, use_entropy=True)\n",
    "            easy_data_1 = get_label_data(easy_data, label=1, use_entropy=True)\n",
    "            easy_data  = pd.concat([easy_data_0, easy_data_1]).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "\n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_data_0 = get_label_data(ambiguous_data, label=0, use_entropy=True)\n",
    "            ambiguous_data_1 = get_label_data(ambiguous_data, label=1, use_entropy=True)\n",
    "            ambiguous_data  = pd.concat([ambiguous_data_0, ambiguous_data_1]).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "\n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            if include_none:\n",
    "                # none-none Mixup\n",
    "                none_data = data[data['category'] == 'none'].reset_index(drop=True)\n",
    "                none_data_0 = get_label_data(none_data, label=0, use_entropy=True)\n",
    "                none_data_1 = get_label_data(none_data, label=1, use_entropy=True)\n",
    "                none_data  = pd.concat([none_data_0, none_data_1]).reset_index(drop=True)\n",
    "                none_data['mixup_type'] = 'same_none'\n",
    "                \n",
    "                final_data = pd.concat([final_data, none_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            return final_data\n",
    "\n",
    "\n",
    "#             # Easy-Ambi Mixup\n",
    "#             easy_ambiguous_len = data_len - len(final_data)\n",
    "\n",
    "#             easy_0 = easy_data_0.head(min(easy_ambiguous_len//2, len(easy_data_0), len(ambiguous_data_0)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_0 = ambiguous_data_0.tail(min(easy_ambiguous_len//2, len(easy_data_0), len(ambiguous_data_0)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_0 = ambiguous_0.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "#             easy_ambiguous_0 = pd.concat([easy_0, ambiguous_0], axis=1).reset_index(drop=True)\n",
    "\n",
    "#             easy_1 = easy_data_1.head(min(easy_ambiguous_len//2, len(easy_data_1), len(ambiguous_data_1)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_1 = ambiguous_data_1.tail(min(easy_ambiguous_len//2, len(easy_data_1), len(ambiguous_data_1)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_1 = ambiguous_1.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "\n",
    "#             easy_ambiguous_1 = pd.concat([easy_1, ambiguous_1], axis=1).reset_index(drop=True)\n",
    "#             easy_ambiguous_data = pd.concat([easy_ambiguous_0, easy_ambiguous_1]).reset_index(drop=True)\n",
    "#             easy_ambiguous_data['mixup_type'] = 'easy_ambiguous'\n",
    "\n",
    "#             return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "        else:\n",
    "            # Easy-Easy Mixup\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_data = easy_data.sort_values('entropy', ascending=True).reset_index(drop=True)\n",
    "            easy_temp = easy_data.sort_values('entropy', ascending=False).reset_index(drop=True)\n",
    "            easy_temp = easy_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "            easy_data = pd.concat([easy_data, easy_temp], axis=1).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "\n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_data = ambiguous_data.sort_values('entropy', ascending=True).reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_data.sort_values('entropy', ascending=False).reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "            ambiguous_data = pd.concat([ambiguous_data, ambiguous_temp], axis=1).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "\n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "            if include_none:\n",
    "                # none-none Mixup\n",
    "                none_data = data[data['category'] == 'none'].reset_index(drop=True)\n",
    "                none_data = none_data.sort_values('entropy', ascending=True).reset_index(drop=True)\n",
    "                none_temp = none_data.sort_values('entropy', ascending=False).reset_index(drop=True)\n",
    "                none_temp = none_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "                none_data = pd.concat([none_data, none_temp], axis=1).reset_index(drop=True)\n",
    "                none_data['mixup_type'] = 'same_none'\n",
    "                \n",
    "                final_data = pd.concat([final_data, none_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            return final_data\n",
    "\n",
    "#             # Easy-Ambi Mixup\n",
    "#             easy_ambiguous_len = data_len - len(final_data)\n",
    "#             easy_ambiguous_data = easy_data.head(min(easy_ambiguous_len, len(easy_data)))[['idx', 'text', 'label', 'category', 'softmax', 'entropy']].reset_index(drop=True)\n",
    "\n",
    "#             easy_ambiguous_temp = ambiguous_data.tail(min(easy_ambiguous_len, len(ambiguous_data)))[['idx', 'text', 'label', 'category', 'softmax', 'entropy']].reset_index(drop=True)\n",
    "#             easy_ambiguous_temp = easy_ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2', 'softmax': 'softmax_2', 'entropy': 'entropy_2'})\n",
    "\n",
    "#             easy_ambiguous_data = pd.concat([easy_ambiguous_data, easy_ambiguous_temp], axis=1)\n",
    "#             easy_ambiguous_data['mixup_type'] = 'easy_ambiguous'\n",
    "\n",
    "#             return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        if use_label:\n",
    "            # Easy-Easy mixup\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_data_0 = get_label_data(easy_data, label=0)\n",
    "            easy_data_1 = get_label_data(easy_data, label=1)\n",
    "            easy_data  = pd.concat([easy_data_0, easy_data_1]).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "            \n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_data_0 = get_label_data(ambiguous_data, label=0)\n",
    "            ambiguous_data_1 = get_label_data(ambiguous_data, label=1)\n",
    "            ambiguous_data  = pd.concat([ambiguous_data_0, ambiguous_data_1]).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "            \n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            if include_none:\n",
    "                # none-none mixup\n",
    "                none_data = data[data['category'] == 'none'].reset_index(drop=True)\n",
    "                none_data_0 = get_label_data(none_data, label=0)\n",
    "                none_data_1 = get_label_data(none_data, label=1)\n",
    "                none_data  = pd.concat([none_data_0, none_data_1]).reset_index(drop=True)\n",
    "                none_data['mixup_type'] = 'same_none'\n",
    "                \n",
    "                final_data = pd.concat([final_data, none_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            return final_data\n",
    "            \n",
    "#             # Easy-Ambi Mixup\n",
    "#             easy_ambiguous_len = data_len - len(final_data)\n",
    "            \n",
    "#             easy_0 = easy_data_0.sample(n=min(easy_ambiguous_len//2, len(easy_data_0), len(ambiguous_data_0)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_0 = ambiguous_data_0.sample(n=min(easy_ambiguous_len//2, len(easy_data_0), len(ambiguous_data_0)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_0 = ambiguous_0.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "#             easy_ambiguous_0 = pd.concat([easy_0, ambiguous_0], axis=1).reset_index(drop=True)\n",
    "\n",
    "#             easy_1 = easy_data_1.sample(n=min(easy_ambiguous_len//2, len(easy_data_1), len(ambiguous_data_1)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_1 = ambiguous_data_1.sample(n=min(easy_ambiguous_len//2, len(easy_data_1), len(ambiguous_data_1)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             ambiguous_1 = ambiguous_1.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "#             easy_ambiguous_1 = pd.concat([easy_1, ambiguous_1], axis=1).reset_index(drop=True)\n",
    "            \n",
    "#             easy_ambiguous_data = pd.concat([easy_ambiguous_0, easy_ambiguous_1]).reset_index(drop=True)\n",
    "#             easy_ambiguous_data['mixup_type'] = 'easy_ambiguous'\n",
    "#             return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        else:\n",
    "            # Easy-Easy mixup\n",
    "            easy_data = data[data['category'] == 'easy'].reset_index(drop=True)\n",
    "            easy_temp = easy_data.copy()\n",
    "            easy_temp = easy_temp.sample(frac=1).reset_index(drop=True)\n",
    "            easy_temp = easy_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "            easy_data = pd.concat([easy_data, easy_temp], axis=1).reset_index(drop=True)\n",
    "            easy_data['mixup_type'] = 'same_easy'\n",
    "\n",
    "            # Ambi-Ambi mixup\n",
    "            ambiguous_data = data[data['category'] == 'ambiguous'].reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_data.copy()\n",
    "            ambiguous_temp = ambiguous_temp.sample(frac=1).reset_index(drop=True)\n",
    "            ambiguous_temp = ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "            ambiguous_data = pd.concat([ambiguous_data, ambiguous_temp], axis=1).reset_index(drop=True)\n",
    "            ambiguous_data['mixup_type'] = 'same_ambiguous'\n",
    "            \n",
    "            final_data = pd.concat([easy_data, ambiguous_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            if include_none:\n",
    "                # None-None mixup\n",
    "                none_data = data[data['category'] == 'none'].reset_index(drop=True)\n",
    "                none_temp = none_data.copy()\n",
    "                none_temp = none_temp.sample(frac=1).reset_index(drop=True)\n",
    "                none_temp = none_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "                none_data = pd.concat([none_data, none_temp], axis=1).reset_index(drop=True)\n",
    "                none_data['mixup_type'] = 'same_none'\n",
    "\n",
    "                final_data = pd.concat([final_data, none_data]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            return final_data\n",
    "        \n",
    "#             # Easy-Ambi Mixup\n",
    "#             easy_ambiguous_len = data_len - len(final_data)\n",
    "#             easy_ambiguous_data = easy_data.sample(n=min(easy_ambiguous_len, len(easy_data)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             easy_ambiguous_temp = ambiguous_data.sample(n=min(easy_ambiguous_len, len(ambiguous_data)))[['idx', 'text', 'label', 'category']].reset_index(drop=True)\n",
    "#             easy_ambiguous_temp = easy_ambiguous_temp.rename(columns={'idx': 'idx_2', 'text': 'text_2', 'label': 'label_2', 'category': 'category_2'})\n",
    "#             easy_ambiguous_data = pd.concat([easy_ambiguous_data, easy_ambiguous_temp], axis=1)\n",
    "#             easy_ambiguous_data['mixup_type'] = 'easy_ambiguous'\n",
    "            \n",
    "#             return pd.concat([final_data, easy_ambiguous_data]).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897f36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(data, info_data=None, mixup=False):\n",
    "    \n",
    "#     # Remove none and hard examples\n",
    "#     data = data[(data['category'] != 'none') & (data['category'] != 'hard')].reset_index(drop=True)\n",
    "\n",
    "#     # Add softmax and entropy info\n",
    "#     if (info_data is not None) and mixup:\n",
    "#         data = pd.merge(data, info_data, on='idx')[['idx', 'text', 'label', 'category', 'softmax', 'entropy']]\n",
    "#         mixup_size = len(info_data) - len(data)\n",
    "\n",
    "#         # --------------------------------------- Same class mixup ---------------------------------------  \n",
    "\n",
    "#         # Easy-Easy Mixup\n",
    "#         easy_data = data[data['category'] == 'easy']\n",
    "#         easy_low_ent_idx = easy_data.sort_values('entropy', ascending=True).head(mixup_size//2)['idx'].tolist()\n",
    "#         easy_high_ent_idx = easy_data.sort_values('entropy', ascending=False).head(mixup_size//2)['idx'].tolist()\n",
    "        \n",
    "#         easy_mixup_data = easy_data[easy_data['idx'].isin(easy_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(easy_high_ent_idx)\n",
    "#         easy_mixup_data['idx_2'] = easy_high_ent_idx\n",
    "#         easy_mixup_data['text_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['text'].values[0])\n",
    "#         easy_mixup_data['label_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['label'].values[0])\n",
    "#         easy_mixup_data['category_2'] = easy_mixup_data['idx_2'].apply(lambda x: easy_data[easy_data['idx'] == x]['category'].values[0])\n",
    "#         easy_mixup_data['mixup_type'] = 'same_easy'\n",
    "        \n",
    "#         # Ambi-Ambi Mixup\n",
    "#         ambiguous_data = data[data['category'] == 'ambiguous']\n",
    "#         ambiguous_low_ent_idx = ambiguous_data.sort_values('entropy', ascending=True).head(mixup_size//2)['idx'].tolist()\n",
    "#         ambiguous_high_ent_idx = ambiguous_data.sort_values('entropy', ascending=False).head(mixup_size//2)['idx'].tolist()\n",
    "        \n",
    "#         ambiguous_mixup_data = ambiguous_data[ambiguous_data['idx'].isin(ambiguous_low_ent_idx)].reset_index(drop=True)\n",
    "#         random.shuffle(ambiguous_high_ent_idx)\n",
    "#         ambiguous_mixup_data['idx_2'] = ambiguous_high_ent_idx\n",
    "#         ambiguous_mixup_data['text_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['text'].values[0])\n",
    "#         ambiguous_mixup_data['label_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['label'].values[0])\n",
    "#         ambiguous_mixup_data['category_2'] = ambiguous_mixup_data['idx_2'].apply(lambda x: ambiguous_data[ambiguous_data['idx'] == x]['category'].values[0])\n",
    "#         ambiguous_mixup_data['mixup_type'] = 'same_ambiguous'\n",
    "        \n",
    "#         return pd.concat([easy_mixup_data, ambiguous_mixup_data]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3a9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(df, label1, label2):\n",
    "    cnt_label = 0\n",
    "    cnt_category = 0\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, label1] == df.at[i, f\"{label1}_2\"]:\n",
    "            cnt_label += 1\n",
    "        if df.at[i, label2] == df.at[i, f\"{label2}_2\"]:\n",
    "            cnt_category += 1\n",
    "\n",
    "    print(\"same label: \", cnt_label/len(df))\n",
    "    print(\"same category: \", cnt_category/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f91e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'sarcasm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcdde589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not forced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>3462</td>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>3463</td>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>3464</td>\n",
       "      <td>Im finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>3465</td>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466</td>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0        0  The only thing I got from college is a caffein...      1   \n",
       "1        1  I love it when professors draw a big question ...      1   \n",
       "2        2  Remember the hundred emails from companies whe...      1   \n",
       "3        3  Today my pop-pop told me I was not forced to...      1   \n",
       "4        4  @VolphanCarol @littlewhitty @mysticalmanatee I...      1   \n",
       "...    ...                                                ...    ...   \n",
       "3462  3462  The population spike in Chicago in 9 months is...      0   \n",
       "3463  3463  You'd think in the second to last English clas...      0   \n",
       "3464  3464  Im finally surfacing after a holiday to Scotl...      0   \n",
       "3465  3465  Couldn't be prouder today. Well done to every ...      0   \n",
       "3466  3466  Overheard as my 13 year old games with a frien...      0   \n",
       "\n",
       "       category  \n",
       "0          hard  \n",
       "1     ambiguous  \n",
       "2     ambiguous  \n",
       "3     ambiguous  \n",
       "4     ambiguous  \n",
       "...         ...  \n",
       "3462       none  \n",
       "3463       none  \n",
       "3464       easy  \n",
       "3465       easy  \n",
       "3466       none  \n",
       "\n",
       "[3467 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'/projects/metis2/atharvak/Data_Cartography/datasets/{data_name}/{data_name}_categorized.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0a6b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ambiguous    1144\n",
       "easy         1144\n",
       "none          978\n",
       "hard          201\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a6ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>gold</th>\n",
       "      <th>softmax</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3061</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.96356308 0.03643692]</td>\n",
       "      <td>0.156450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.22491207 0.77508793]</td>\n",
       "      <td>0.533055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99220995 0.00779005]</td>\n",
       "      <td>0.045580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.00963458 0.99036542]</td>\n",
       "      <td>0.054316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99644967 0.00355033]</td>\n",
       "      <td>0.023570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>2640</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.96515393 0.03484607]</td>\n",
       "      <td>0.151204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>1204</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99699631 0.00300369]</td>\n",
       "      <td>0.020444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99511012 0.00488988]</td>\n",
       "      <td>0.030895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>2704</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99134994 0.00865006]</td>\n",
       "      <td>0.049702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>1133</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.97961178 0.02038822]</td>\n",
       "      <td>0.099546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  gold                  softmax   entropy\n",
       "0     3061     0  [0.96356308 0.03643692]  0.156450\n",
       "1      326     1  [0.22491207 0.77508793]  0.533055\n",
       "2     2802     0  [0.99220995 0.00779005]  0.045580\n",
       "3      365     1  [0.00963458 0.99036542]  0.054316\n",
       "4     2770     0  [0.99644967 0.00355033]  0.023570\n",
       "...    ...   ...                      ...       ...\n",
       "3462  2640     0  [0.96515393 0.03484607]  0.151204\n",
       "3463  1204     0  [0.99699631 0.00300369]  0.020444\n",
       "3464  1370     0  [0.99511012 0.00488988]  0.030895\n",
       "3465  2704     0  [0.99134994 0.00865006]  0.049702\n",
       "3466  1133     0  [0.97961178 0.02038822]  0.099546\n",
       "\n",
       "[3467 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df = pd.read_csv(f'/projects/metis2/atharvak/Data_Cartography/dy_log/{data_name}/roberta-base/training_dynamics/final_4.csv')\n",
    "info_df = info_df.rename(columns={'guid': 'idx', 'sm': 'softmax', 'en': 'entropy'})\n",
    "info_df = info_df[['idx', 'gold', 'softmax', 'entropy']]\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4212da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not forced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@jimrossignol I choose to interpret it as \"XD\"...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>3462</td>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>3463</td>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>3464</td>\n",
       "      <td>Im finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>3465</td>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>3466</td>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0        1  I love it when professors draw a big question ...      1   \n",
       "1        2  Remember the hundred emails from companies whe...      1   \n",
       "2        3  Today my pop-pop told me I was not forced to...      1   \n",
       "3        4  @VolphanCarol @littlewhitty @mysticalmanatee I...      1   \n",
       "4        5  @jimrossignol I choose to interpret it as \"XD\"...      1   \n",
       "...    ...                                                ...    ...   \n",
       "3261  3462  The population spike in Chicago in 9 months is...      0   \n",
       "3262  3463  You'd think in the second to last English clas...      0   \n",
       "3263  3464  Im finally surfacing after a holiday to Scotl...      0   \n",
       "3264  3465  Couldn't be prouder today. Well done to every ...      0   \n",
       "3265  3466  Overheard as my 13 year old games with a frien...      0   \n",
       "\n",
       "       category  \n",
       "0     ambiguous  \n",
       "1     ambiguous  \n",
       "2     ambiguous  \n",
       "3     ambiguous  \n",
       "4     ambiguous  \n",
       "...         ...  \n",
       "3261       none  \n",
       "3262       none  \n",
       "3263       easy  \n",
       "3264       easy  \n",
       "3265       none  \n",
       "\n",
       "[3266 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_dataset = prepare_dataset_original(df, include_none=True)\n",
    "normal_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2708d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.6319584655321604\n",
      "same category:  0.3360253821747909\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1273</td>\n",
       "      <td>i almost forgot to tweet about jason bateman t...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>1877</td>\n",
       "      <td>@gough_hayden This art is giving me Mario Stri...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2630</td>\n",
       "      <td>After being mom shamed because my two and a ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2907</td>\n",
       "      <td>Somehow have to try and get a water sample fro...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1557</td>\n",
       "      <td>@MartinSLewis @GMB We share a birthday Martin!...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>1727</td>\n",
       "      <td>everyone gets all hyped about ancient debris, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>theres no better way to wake up than having o...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>998</td>\n",
       "      <td>Grealish is just good. You hate to admit it, b...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>Kind of rude of my stomach to still make whale...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>579</td>\n",
       "      <td>@kirkkorner \"But it's a deterrent\" </td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>2698</td>\n",
       "      <td>I had a dream last night that a talking snake ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>158</td>\n",
       "      <td>\"are you experiencing suicidal or homicidal id...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>1770</td>\n",
       "      <td>does anyone else buy pound cake in bulk from c...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>389</td>\n",
       "      <td>Are GoFundMe accounts a form of socialism?  I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>2475</td>\n",
       "      <td>bigger thighs means u have more room for a cat...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>246</td>\n",
       "      <td>wow this growth spurt is really taking longer ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>2339</td>\n",
       "      <td>For the first time today I've been dancing aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>682</td>\n",
       "      <td>i love 6 hour panic attacks</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>619</td>\n",
       "      <td>we love when the power goes out at 6:30 in the...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>2706</td>\n",
       "      <td>Only 10 minutes into #AEWAllOut and I'm alread...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     1273  i almost forgot to tweet about jason bateman t...      0   \n",
       "1     2630  After being mom shamed because my two and a ha...      0   \n",
       "2     1557  @MartinSLewis @GMB We share a birthday Martin!...      0   \n",
       "3      100  theres no better way to wake up than having o...      1   \n",
       "4      139  Kind of rude of my stomach to still make whale...      1   \n",
       "...    ...                                                ...    ...   \n",
       "3462  2698  I had a dream last night that a talking snake ...      0   \n",
       "3463  1770  does anyone else buy pound cake in bulk from c...      0   \n",
       "3464  2475  bigger thighs means u have more room for a cat...      0   \n",
       "3465  2339  For the first time today I've been dancing aro...      0   \n",
       "3466   619  we love when the power goes out at 6:30 in the...      1   \n",
       "\n",
       "       category  idx_2                                             text_2  \\\n",
       "0          easy   1877  @gough_hayden This art is giving me Mario Stri...   \n",
       "1          none   2907  Somehow have to try and get a water sample fro...   \n",
       "2          easy   1727  everyone gets all hyped about ancient debris, ...   \n",
       "3     ambiguous    998  Grealish is just good. You hate to admit it, b...   \n",
       "4     ambiguous    579               @kirkkorner \"But it's a deterrent\"    \n",
       "...         ...    ...                                                ...   \n",
       "3462       easy    158  \"are you experiencing suicidal or homicidal id...   \n",
       "3463       none    389  Are GoFundMe accounts a form of socialism?  I ...   \n",
       "3464       none    246  wow this growth spurt is really taking longer ...   \n",
       "3465  ambiguous    682                        i love 6 hour panic attacks   \n",
       "3466  ambiguous   2706  Only 10 minutes into #AEWAllOut and I'm alread...   \n",
       "\n",
       "      label_2 category_2  \n",
       "0           0       none  \n",
       "1           0       easy  \n",
       "2           0       none  \n",
       "3           0       easy  \n",
       "4           1  ambiguous  \n",
       "...       ...        ...  \n",
       "3462        1  ambiguous  \n",
       "3463        1  ambiguous  \n",
       "3464        1  ambiguous  \n",
       "3465        1  ambiguous  \n",
       "3466        0       none  \n",
       "\n",
       "[3467 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_mixup_dataset = prepare_dataset_random_mixup(df, info_data=info_df, include_none=True, use_label=False)\n",
    "get_count(random_mixup_dataset, 'label', 'category')\n",
    "random_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ecacf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.8430920103836169\n",
      "same category:  0.6844534179405827\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2390</td>\n",
       "      <td>been in college for 3 weeks and learned that I...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>2160</td>\n",
       "      <td>Just got off the phone with the human traffick...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>love getting assignments at 6:25pm on a Friday!!</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>108</td>\n",
       "      <td>\"He's not welcome in a tolerant place like Lon...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Not telling anyone how I voted in case it does...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>786</td>\n",
       "      <td>Incredible scenes in Nottingham, as it begins ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>Brother's off on a night out and I've gone to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>328</td>\n",
       "      <td>every date I go on I get a little scared will ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1788</td>\n",
       "      <td>Sunset over the Appalachian mountains. https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3056</td>\n",
       "      <td>summer just fully went to me: speaking of sing...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>1906</td>\n",
       "      <td>Why was everything in interior design so brown...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3225</td>\n",
       "      <td>This tennis is hilariously fantastic to watch....</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>2576</td>\n",
       "      <td>I'm going to miss #DesperateHousewives sooo mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>2873</td>\n",
       "      <td>BBC News - Andrea Levy: Small Island and Long ...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>3085</td>\n",
       "      <td>Wow we arent the favourites this time? https:...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>2761</td>\n",
       "      <td>being sad                     cutting my\\n-&amp;gt...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>65</td>\n",
       "      <td>my neighbors taunting me outside my office win...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>477</td>\n",
       "      <td>dont make me rewatch panic </td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>595</td>\n",
       "      <td>2 minutes into the book and I already hate the...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>547</td>\n",
       "      <td>@trpalomo Those boots must be tasty for how mu...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     2390  been in college for 3 weeks and learned that I...      0   \n",
       "1       90   love getting assignments at 6:25pm on a Friday!!      1   \n",
       "2       28  Not telling anyone how I voted in case it does...      1   \n",
       "3      204  Brother's off on a night out and I've gone to ...      1   \n",
       "4     1788  Sunset over the Appalachian mountains. https:/...      0   \n",
       "...    ...                                                ...    ...   \n",
       "3462  1906  Why was everything in interior design so brown...      0   \n",
       "3463  2576  I'm going to miss #DesperateHousewives sooo mu...      0   \n",
       "3464  3085  Wow we arent the favourites this time? https:...      0   \n",
       "3465    65  my neighbors taunting me outside my office win...      1   \n",
       "3466   595  2 minutes into the book and I already hate the...      1   \n",
       "\n",
       "       category  idx_2                                             text_2  \\\n",
       "0     ambiguous   2160  Just got off the phone with the human traffick...   \n",
       "1     ambiguous    108  \"He's not welcome in a tolerant place like Lon...   \n",
       "2     ambiguous    786  Incredible scenes in Nottingham, as it begins ...   \n",
       "3     ambiguous    328  every date I go on I get a little scared will ...   \n",
       "4          easy   3056  summer just fully went to me: speaking of sing...   \n",
       "...         ...    ...                                                ...   \n",
       "3462       easy   3225  This tennis is hilariously fantastic to watch....   \n",
       "3463  ambiguous   2873  BBC News - Andrea Levy: Small Island and Long ...   \n",
       "3464       easy   2761  being sad                     cutting my\\n-&gt...   \n",
       "3465  ambiguous    477                      dont make me rewatch panic    \n",
       "3466  ambiguous    547  @trpalomo Those boots must be tasty for how mu...   \n",
       "\n",
       "      label_2 category_2  \n",
       "0           0  ambiguous  \n",
       "1           1  ambiguous  \n",
       "2           1  ambiguous  \n",
       "3           1  ambiguous  \n",
       "4           0  ambiguous  \n",
       "...       ...        ...  \n",
       "3462        0       easy  \n",
       "3463        0       easy  \n",
       "3464        0       easy  \n",
       "3465        1  ambiguous  \n",
       "3466        1  ambiguous  \n",
       "\n",
       "[3467 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_mixup_dataset = prepare_dataset_random_mixup(df, info_data=info_df, use_label=True)\n",
    "get_count(random_mixup_dataset, 'label', 'category')\n",
    "random_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dca96ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.62995337995338\n",
      "same category:  0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>mixup_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2593</td>\n",
       "      <td>This Country is absolute crackers i love it so...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>586</td>\n",
       "      <td>Just won 3 cents on HQ words...early retiremen...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320</td>\n",
       "      <td>Having a bad week but what else is new!! At le...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>122</td>\n",
       "      <td>So in other news I just held the door open for...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1173</td>\n",
       "      <td>Liberty just went 95 yards on us with ease. Da...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>2559</td>\n",
       "      <td>I got an hours detention cause somebody set m...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>981</td>\n",
       "      <td>Been playing Ghost of Tsushima and I love it!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>1518</td>\n",
       "      <td>being a head nodder in zoom university is a curse</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3366</td>\n",
       "      <td>#europeansuperleague boycott it! Cancel any su...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>473</td>\n",
       "      <td>I spend the day sitting or napping, I ration m...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>1686</td>\n",
       "      <td>@vincent72309193 @Brechannell @misseverywhereg...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>945</td>\n",
       "      <td>@MI37867601 @Christo66412045 @FIimdog @UpTheVi...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>2712</td>\n",
       "      <td>Why am I obsessed with Frozen 2 </td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3092</td>\n",
       "      <td>I have chers dark lady stuck in my head but i...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>1815</td>\n",
       "      <td>I watched the first two episodes of American H...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>2153</td>\n",
       "      <td>Pitch looks flat as a pancake so India might a...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>2222</td>\n",
       "      <td>its been 3 years, i know i will sob the secon...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>935</td>\n",
       "      <td>#DominicCummings Sack him. Or better yet, char...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>262</td>\n",
       "      <td>I don't think Republicans have any self awaren...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>490</td>\n",
       "      <td>what a great day to be stuck inside </td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3432 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     2593  This Country is absolute crackers i love it so...      0   \n",
       "1      320  Having a bad week but what else is new!! At le...      1   \n",
       "2     1173  Liberty just went 95 yards on us with ease. Da...      0   \n",
       "3      981    Been playing Ghost of Tsushima and I love it!!!      0   \n",
       "4     3366  #europeansuperleague boycott it! Cancel any su...      0   \n",
       "...    ...                                                ...    ...   \n",
       "3427  1686  @vincent72309193 @Brechannell @misseverywhereg...      0   \n",
       "3428  2712                  Why am I obsessed with Frozen 2       0   \n",
       "3429  1815  I watched the first two episodes of American H...      0   \n",
       "3430  2222  its been 3 years, i know i will sob the secon...      0   \n",
       "3431   262  I don't think Republicans have any self awaren...      1   \n",
       "\n",
       "       category  idx_2                                             text_2  \\\n",
       "0          easy    586  Just won 3 cents on HQ words...early retiremen...   \n",
       "1     ambiguous    122  So in other news I just held the door open for...   \n",
       "2     ambiguous   2559  I got an hours detention cause somebody set m...   \n",
       "3          easy   1518  being a head nodder in zoom university is a curse   \n",
       "4          easy    473  I spend the day sitting or napping, I ration m...   \n",
       "...         ...    ...                                                ...   \n",
       "3427       easy    945  @MI37867601 @Christo66412045 @FIimdog @UpTheVi...   \n",
       "3428       easy   3092  I have chers dark lady stuck in my head but i...   \n",
       "3429       easy   2153  Pitch looks flat as a pancake so India might a...   \n",
       "3430       easy    935  #DominicCummings Sack him. Or better yet, char...   \n",
       "3431  ambiguous    490              what a great day to be stuck inside    \n",
       "\n",
       "      label_2 category_2      mixup_type  \n",
       "0           1  ambiguous  easy_ambiguous  \n",
       "1           1  ambiguous  same_ambiguous  \n",
       "2           0  ambiguous  same_ambiguous  \n",
       "3           0  ambiguous  easy_ambiguous  \n",
       "4           1  ambiguous  easy_ambiguous  \n",
       "...       ...        ...             ...  \n",
       "3427        0       easy       same_easy  \n",
       "3428        0       easy       same_easy  \n",
       "3429        0  ambiguous  easy_ambiguous  \n",
       "3430        0  ambiguous  easy_ambiguous  \n",
       "3431        1  ambiguous  same_ambiguous  \n",
       "\n",
       "[3432 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, use_label=False, use_entropy=False)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0c04df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  1.0\n",
      "same category:  0.8706240487062404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>mixup_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2713</td>\n",
       "      <td>Im back at my beloved Mecca Bingo and Im ver...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>3206</td>\n",
       "      <td>Is there anything better and more comforting t...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>\"He's not welcome in a tolerant place like Lon...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>366</td>\n",
       "      <td>If your website still has a google plus share ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3094</td>\n",
       "      <td>Im a HORRIBLE PERSON but one of my VERY FAVOR...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>2711</td>\n",
       "      <td>10 years today since I left school. Im in sho...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1114</td>\n",
       "      <td>i want thai food Right Now :(</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>1652</td>\n",
       "      <td>OH MY GOD I AM IN LOVE WITH MY FIANC. https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1022</td>\n",
       "      <td>Broadband back, starting to feel a bit more no...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>1229</td>\n",
       "      <td>Its frustrating that the moment in my scienti...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>459</td>\n",
       "      <td>i stg i probably talk to my tv more than i do ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>47</td>\n",
       "      <td>@AlexShawESPN @robwishart @GNev2 Does he order...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>765</td>\n",
       "      <td>its amazing how many people get mad at you fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>86</td>\n",
       "      <td>fully vaxxed feeling very girlboss</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>1396</td>\n",
       "      <td>Im defending the land I have left. Click http...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>2830</td>\n",
       "      <td>Just back from seeing Bridesmaids. Surprisingl...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>2454</td>\n",
       "      <td>my body is really out here sabotaging me bc i ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>1275</td>\n",
       "      <td>Okay, well nice of 2020 to bless us with the g...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>721</td>\n",
       "      <td>it's all about women in stem struggles. what a...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>273</td>\n",
       "      <td>@benshapiro its true, every time my little co...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2628 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     2713  Im back at my beloved Mecca Bingo and Im ver...      0   \n",
       "1      108  \"He's not welcome in a tolerant place like Lon...      1   \n",
       "2     3094  Im a HORRIBLE PERSON but one of my VERY FAVOR...      0   \n",
       "3     1114                      i want thai food Right Now :(      0   \n",
       "4     1022  Broadband back, starting to feel a bit more no...      0   \n",
       "...    ...                                                ...    ...   \n",
       "2623   459  i stg i probably talk to my tv more than i do ...      1   \n",
       "2624   765  its amazing how many people get mad at you fo...      1   \n",
       "2625  1396  Im defending the land I have left. Click http...      0   \n",
       "2626  2454  my body is really out here sabotaging me bc i ...      0   \n",
       "2627   721  it's all about women in stem struggles. what a...      1   \n",
       "\n",
       "       category  idx_2                                             text_2  \\\n",
       "0          easy   3206  Is there anything better and more comforting t...   \n",
       "1     ambiguous    366  If your website still has a google plus share ...   \n",
       "2          easy   2711  10 years today since I left school. Im in sho...   \n",
       "3          easy   1652  OH MY GOD I AM IN LOVE WITH MY FIANC. https:/...   \n",
       "4          easy   1229  Its frustrating that the moment in my scienti...   \n",
       "...         ...    ...                                                ...   \n",
       "2623  ambiguous     47  @AlexShawESPN @robwishart @GNev2 Does he order...   \n",
       "2624  ambiguous     86                 fully vaxxed feeling very girlboss   \n",
       "2625       easy   2830  Just back from seeing Bridesmaids. Surprisingl...   \n",
       "2626  ambiguous   1275  Okay, well nice of 2020 to bless us with the g...   \n",
       "2627  ambiguous    273  @benshapiro its true, every time my little co...   \n",
       "\n",
       "      label_2 category_2      mixup_type  \n",
       "0           0       easy       same_easy  \n",
       "1           1  ambiguous  same_ambiguous  \n",
       "2           0       easy       same_easy  \n",
       "3           0       easy       same_easy  \n",
       "4           0  ambiguous  easy_ambiguous  \n",
       "...       ...        ...             ...  \n",
       "2623        1  ambiguous  same_ambiguous  \n",
       "2624        1  ambiguous  same_ambiguous  \n",
       "2625        0       easy       same_easy  \n",
       "2626        0  ambiguous  same_ambiguous  \n",
       "2627        1  ambiguous  same_ambiguous  \n",
       "\n",
       "[2628 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, use_label=True, use_entropy=False)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "715a2cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  0.6398601398601399\n",
      "same category:  0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>softmax</th>\n",
       "      <th>entropy</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>softmax_2</th>\n",
       "      <th>entropy_2</th>\n",
       "      <th>mixup_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1765</td>\n",
       "      <td>Networking, networking and networking - big me...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99703513 0.00296487]</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>557</td>\n",
       "      <td>I'm so glad the clocks just went back, I have ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.0063167 0.9936833]</td>\n",
       "      <td>0.038288</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1742</td>\n",
       "      <td>milk and cookies are an undefeated delicacy</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.98719449 0.01280551]</td>\n",
       "      <td>0.068528</td>\n",
       "      <td>667</td>\n",
       "      <td>So staying up late was worth it..... </td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.82875006 0.17124994]</td>\n",
       "      <td>0.457863</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2879</td>\n",
       "      <td>An inspiring afternoon spent hearing the impac...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99739485 0.00260515]</td>\n",
       "      <td>0.018103</td>\n",
       "      <td>1323</td>\n",
       "      <td>the time is currently 00:24 and the single thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.98472159 0.01527841]</td>\n",
       "      <td>0.079045</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2687</td>\n",
       "      <td>I misspoke - knockdown instead of lockdown - f...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99353579 0.00646421]</td>\n",
       "      <td>0.039032</td>\n",
       "      <td>2450</td>\n",
       "      <td>I dont trust anyone who enjoys hunting</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99537097 0.00462903]</td>\n",
       "      <td>0.029501</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1387</td>\n",
       "      <td>Mormons are a deeply traumatized people. All o...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99708102 0.00291898]</td>\n",
       "      <td>0.019951</td>\n",
       "      <td>2963</td>\n",
       "      <td>why is kareem hunt allowed to be in the league?</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.99385096 0.00614904]</td>\n",
       "      <td>0.037438</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>530</td>\n",
       "      <td>Youre never going to have a white boyfriend ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.02343594 0.97656406]</td>\n",
       "      <td>0.111126</td>\n",
       "      <td>441</td>\n",
       "      <td>For a good laugh, follow me over on @vsheltons...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.01229523 0.98770477]</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>3407</td>\n",
       "      <td>Cast list posted for the fall drama club play!...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99386525 0.00613475]</td>\n",
       "      <td>0.037365</td>\n",
       "      <td>1888</td>\n",
       "      <td>@alligatoraliens tbh I think the only time I h...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.97969494 0.02030506]</td>\n",
       "      <td>0.099224</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>1932</td>\n",
       "      <td>@netflix the good shows that were cancelled fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.98773799 0.01226201]</td>\n",
       "      <td>0.066155</td>\n",
       "      <td>364</td>\n",
       "      <td>Love a good cry </td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.02351381 0.97648619]</td>\n",
       "      <td>0.111416</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>3239</td>\n",
       "      <td>GET YOUR VACCINES WHEN YOU CAN PEOPLE https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.9967436 0.0032564]</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>156</td>\n",
       "      <td>Wow it's almost as if it's their literal job t...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.00736013 0.99263987]</td>\n",
       "      <td>0.043484</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>3198</td>\n",
       "      <td>Discover more of Maranda Ridgway's greeting ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99732854 0.00267146]</td>\n",
       "      <td>0.018497</td>\n",
       "      <td>2868</td>\n",
       "      <td>Here for the #gbbo and #masterchef tweets only</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.98554744 0.01445256]</td>\n",
       "      <td>0.075581</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3432 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     1765  Networking, networking and networking - big me...      0   \n",
       "1     1742        milk and cookies are an undefeated delicacy      0   \n",
       "2     2879  An inspiring afternoon spent hearing the impac...      0   \n",
       "3     2687  I misspoke - knockdown instead of lockdown - f...      0   \n",
       "4     1387  Mormons are a deeply traumatized people. All o...      0   \n",
       "...    ...                                                ...    ...   \n",
       "3427   530  Youre never going to have a white boyfriend ...      1   \n",
       "3428  3407  Cast list posted for the fall drama club play!...      0   \n",
       "3429  1932  @netflix the good shows that were cancelled fo...      0   \n",
       "3430  3239  GET YOUR VACCINES WHEN YOU CAN PEOPLE https://...      0   \n",
       "3431  3198  Discover more of Maranda Ridgway's greeting ca...      0   \n",
       "\n",
       "       category                  softmax   entropy  idx_2  \\\n",
       "0          easy  [0.99703513 0.00296487]  0.020219    557   \n",
       "1          easy  [0.98719449 0.01280551]  0.068528    667   \n",
       "2          easy  [0.99739485 0.00260515]  0.018103   1323   \n",
       "3          easy  [0.99353579 0.00646421]  0.039032   2450   \n",
       "4          easy  [0.99708102 0.00291898]  0.019951   2963   \n",
       "...         ...                      ...       ...    ...   \n",
       "3427  ambiguous  [0.02343594 0.97656406]  0.111126    441   \n",
       "3428       easy  [0.99386525 0.00613475]  0.037365   1888   \n",
       "3429  ambiguous  [0.98773799 0.01226201]  0.066155    364   \n",
       "3430       easy    [0.9967436 0.0032564]  0.021901    156   \n",
       "3431       easy  [0.99732854 0.00267146]  0.018497   2868   \n",
       "\n",
       "                                                 text_2  label_2 category_2  \\\n",
       "0     I'm so glad the clocks just went back, I have ...        1  ambiguous   \n",
       "1               So staying up late was worth it.....         1  ambiguous   \n",
       "2     the time is currently 00:24 and the single thi...        0       easy   \n",
       "3               I dont trust anyone who enjoys hunting        0       easy   \n",
       "4       why is kareem hunt allowed to be in the league?        0  ambiguous   \n",
       "...                                                 ...      ...        ...   \n",
       "3427  For a good laugh, follow me over on @vsheltons...        1  ambiguous   \n",
       "3428  @alligatoraliens tbh I think the only time I h...        0  ambiguous   \n",
       "3429                                 Love a good cry         1  ambiguous   \n",
       "3430  Wow it's almost as if it's their literal job t...        1  ambiguous   \n",
       "3431     Here for the #gbbo and #masterchef tweets only        0       easy   \n",
       "\n",
       "                    softmax_2  entropy_2      mixup_type  \n",
       "0       [0.0063167 0.9936833]   0.038288  easy_ambiguous  \n",
       "1     [0.82875006 0.17124994]   0.457863  easy_ambiguous  \n",
       "2     [0.98472159 0.01527841]   0.079045       same_easy  \n",
       "3     [0.99537097 0.00462903]   0.029501       same_easy  \n",
       "4     [0.99385096 0.00614904]   0.037438  easy_ambiguous  \n",
       "...                       ...        ...             ...  \n",
       "3427  [0.01229523 0.98770477]   0.066300  same_ambiguous  \n",
       "3428  [0.97969494 0.02030506]   0.099224  easy_ambiguous  \n",
       "3429  [0.02351381 0.97648619]   0.111416  same_ambiguous  \n",
       "3430  [0.00736013 0.99263987]   0.043484  easy_ambiguous  \n",
       "3431  [0.98554744 0.01445256]   0.075581       same_easy  \n",
       "\n",
       "[3432 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, use_label=False, use_entropy=True)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07b5cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same label:  1.0\n",
      "same category:  0.8706240487062404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>softmax</th>\n",
       "      <th>entropy</th>\n",
       "      <th>idx_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>category_2</th>\n",
       "      <th>softmax_2</th>\n",
       "      <th>entropy_2</th>\n",
       "      <th>mixup_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1114</td>\n",
       "      <td>i want thai food Right Now :(</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99444999 0.00555001]</td>\n",
       "      <td>0.034361</td>\n",
       "      <td>1372</td>\n",
       "      <td>there is no greater joy than to have an endle...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99459495 0.00540505]</td>\n",
       "      <td>0.033607</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2725</td>\n",
       "      <td>my new glasses prescription is messing with my...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99613885 0.00386115]</td>\n",
       "      <td>0.025309</td>\n",
       "      <td>2501</td>\n",
       "      <td>My adventure this week was going to a park and...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99174245 0.00825755]</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1529</td>\n",
       "      <td>Has anyone written about the complexity of sic...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.98996828 0.01003172]</td>\n",
       "      <td>0.056147</td>\n",
       "      <td>2073</td>\n",
       "      <td>hey @phoebe_bridgers im free tomorrow btw. Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.98022248 0.01977752]</td>\n",
       "      <td>0.097172</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428</td>\n",
       "      <td>Just disposed of a dead opossum. Good morning!</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.00816608 0.99183392]</td>\n",
       "      <td>0.047393</td>\n",
       "      <td>797</td>\n",
       "      <td>Afghanistan just completely crumbling after sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.08395871 0.91604129]</td>\n",
       "      <td>0.288333</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2134</td>\n",
       "      <td>Entering my third year of consistent yoga prac...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1538</td>\n",
       "      <td>I feel like most teens/women are not told that...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>3317</td>\n",
       "      <td>craving hot sauce</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.98899906 0.01100094]</td>\n",
       "      <td>0.060552</td>\n",
       "      <td>1488</td>\n",
       "      <td>Excited to share the latest addition to my #et...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99682422 0.00317578]</td>\n",
       "      <td>0.021438</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>2872</td>\n",
       "      <td>This bunch of players is something special. Re...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2549</td>\n",
       "      <td>if Covid didnt happen, Id have free travel r...</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>easy_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>2287</td>\n",
       "      <td>80% of #LittleHouseonthePrairie episodes invol...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.98806428 0.01193572]</td>\n",
       "      <td>0.064718</td>\n",
       "      <td>3027</td>\n",
       "      <td>horseshoe crabs are so sweet and gentle i love...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99697631 0.00302369]</td>\n",
       "      <td>0.020560</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>502</td>\n",
       "      <td>Shaving in the shower without your glasses/con...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.017154 0.982846]</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>297</td>\n",
       "      <td>The fact I nearly froze on my way to work this...</td>\n",
       "      <td>1</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[0.01566353 0.98433647]</td>\n",
       "      <td>0.080644</td>\n",
       "      <td>same_ambiguous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>2778</td>\n",
       "      <td>Why is asking for a letter of recommendation s...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.99117625 0.00882375]</td>\n",
       "      <td>0.050524</td>\n",
       "      <td>2744</td>\n",
       "      <td>Something about being up late and seeing  #Too...</td>\n",
       "      <td>0</td>\n",
       "      <td>easy</td>\n",
       "      <td>[0.996324 0.003676]</td>\n",
       "      <td>0.024277</td>\n",
       "      <td>same_easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2628 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                               text  label  \\\n",
       "0     1114                      i want thai food Right Now :(      0   \n",
       "1     2725  my new glasses prescription is messing with my...      0   \n",
       "2     1529  Has anyone written about the complexity of sic...      0   \n",
       "3      428     Just disposed of a dead opossum. Good morning!      1   \n",
       "4     2134  Entering my third year of consistent yoga prac...      0   \n",
       "...    ...                                                ...    ...   \n",
       "2623  3317                                  craving hot sauce      0   \n",
       "2624  2872  This bunch of players is something special. Re...      0   \n",
       "2625  2287  80% of #LittleHouseonthePrairie episodes invol...      0   \n",
       "2626   502  Shaving in the shower without your glasses/con...      1   \n",
       "2627  2778  Why is asking for a letter of recommendation s...      0   \n",
       "\n",
       "       category                  softmax   entropy  idx_2  \\\n",
       "0          easy  [0.99444999 0.00555001]  0.034361   1372   \n",
       "1          easy  [0.99613885 0.00386115]  0.025309   2501   \n",
       "2     ambiguous  [0.98996828 0.01003172]  0.056147   2073   \n",
       "3     ambiguous  [0.00816608 0.99183392]  0.047393    797   \n",
       "4          easy                      NaN       NaN   1538   \n",
       "...         ...                      ...       ...    ...   \n",
       "2623       easy  [0.98899906 0.01100094]  0.060552   1488   \n",
       "2624       easy                      NaN       NaN   2549   \n",
       "2625       easy  [0.98806428 0.01193572]  0.064718   3027   \n",
       "2626  ambiguous      [0.017154 0.982846]  0.086746    297   \n",
       "2627       easy  [0.99117625 0.00882375]  0.050524   2744   \n",
       "\n",
       "                                                 text_2  label_2 category_2  \\\n",
       "0     there is no greater joy than to have an endle...        0       easy   \n",
       "1     My adventure this week was going to a park and...        0       easy   \n",
       "2     hey @phoebe_bridgers im free tomorrow btw. Th...        0  ambiguous   \n",
       "3     Afghanistan just completely crumbling after sp...        1  ambiguous   \n",
       "4     I feel like most teens/women are not told that...        0  ambiguous   \n",
       "...                                                 ...      ...        ...   \n",
       "2623  Excited to share the latest addition to my #et...        0       easy   \n",
       "2624  if Covid didnt happen, Id have free travel r...        0  ambiguous   \n",
       "2625  horseshoe crabs are so sweet and gentle i love...        0       easy   \n",
       "2626  The fact I nearly froze on my way to work this...        1  ambiguous   \n",
       "2627  Something about being up late and seeing  #Too...        0       easy   \n",
       "\n",
       "                    softmax_2  entropy_2      mixup_type  \n",
       "0     [0.99459495 0.00540505]   0.033607       same_easy  \n",
       "1     [0.99174245 0.00825755]   0.047832       same_easy  \n",
       "2     [0.98022248 0.01977752]   0.097172  same_ambiguous  \n",
       "3     [0.08395871 0.91604129]   0.288333  same_ambiguous  \n",
       "4                         NaN        NaN  easy_ambiguous  \n",
       "...                       ...        ...             ...  \n",
       "2623  [0.99682422 0.00317578]   0.021438       same_easy  \n",
       "2624                      NaN        NaN  easy_ambiguous  \n",
       "2625  [0.99697631 0.00302369]   0.020560       same_easy  \n",
       "2626  [0.01566353 0.98433647]   0.080644  same_ambiguous  \n",
       "2627      [0.996324 0.003676]   0.024277       same_easy  \n",
       "\n",
       "[2628 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mixup_dataset = prepare_dataset_category_mixup(df, info_data=info_df, use_label=True, use_entropy=True)\n",
    "get_count(category_mixup_dataset, 'label', 'category')\n",
    "category_mixup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6fef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixupDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        data: pd.DataFrame,\n",
    "        sampling_type: str\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.sampling_type = sampling_type\n",
    "        \n",
    "        if self.sampling_type == 'sequential':\n",
    "            sorting_dict = {\n",
    "                'non_mixup_easy': 0,\n",
    "                'non_mixup_ambiguous': 1,\n",
    "                'same_easy': 2,\n",
    "                'different_easy': 3,\n",
    "                'same_ambiguous': 4,\n",
    "                'different_ambiguous': 5\n",
    "            }\n",
    "            self.data['data_type'] = self.data['mixup_type'] + '_' + self.data['category']\n",
    "            self.data = self.data.iloc[self.data.data_type.map(sorting_dict).argsort()].reset_index(drop=True)\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenizer.batch_encode_plus(\n",
    "            self.data[INPUT_COLUMN].tolist(),\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,            \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "\n",
    "                \n",
    "    def __len__(\n",
    "        self\n",
    "    ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        data = {\n",
    "            'input_ids_1': self.tokenized_data['input_ids'][index].flatten(),\n",
    "            'attention_mask_1': self.tokenized_data['attention_mask'][index].flatten(),\n",
    "            'labels_1': torch.tensor(self.data.iloc[index][OUTPUT_COLUMN], dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        idx2 = self.data.iloc[index]['idx_2']\n",
    "\n",
    "        index2 = self.data[self.data['idx'] == idx2].index[0]\n",
    "        data['input_ids_2'] = self.tokenized_data['input_ids'][index2].flatten()\n",
    "        data['attention_mask_2'] = self.tokenized_data['attention_mask'][index2].flatten()\n",
    "        data['labels_2'] = torch.tensor(self.data.iloc[index2][OUTPUT_COLUMN], dtype=torch.long)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f241da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MixupDataset(tokenizer=tokenizer, data=dataset, sampling_type='random')\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6631d2",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e8145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, optimizer, device, train_loader, num_epochs, output_dir):\n",
    "    losses = []\n",
    "    train_iterator = trange(int(num_epochs), desc='Epoch')\n",
    "    for _ in train_iterator:\n",
    "        tr_loss = 0\n",
    "        step = None\n",
    "        epoch_iterator = tqdm(train_loader, desc='Training')\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "\n",
    "            inputs = {}\n",
    "            for k, v in batch.items():\n",
    "                if isinstance(v, list):\n",
    "                    inputs[k] = None\n",
    "                else:\n",
    "                    inputs[k] = v.to(device)\n",
    "\n",
    "            labels = inputs['labels_1']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            out = outputs['logits'].double().to(device)\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            tr_loss += loss.item()\n",
    "        losses.append(tr_loss/(step+1))\n",
    "        print('train loss: {}'.format(tr_loss/(step+1)))\n",
    "\n",
    "    # save model and tokenizer\n",
    "    print('Saving model and tokenizer')\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69183448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, eval_loader, device, with_labels=True):\n",
    "    probs = None\n",
    "    gold_labels = None\n",
    "\n",
    "    eval_loss = 0\n",
    "    step = None\n",
    "    eval_iterator = tqdm(eval_loader, desc='Evaluating')\n",
    "    for step, batch in enumerate(eval_iterator):\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            inputs = {k:v.to(device) for k, v in batch.items()}\n",
    "            labels = inputs['labels_1']\n",
    "            del inputs['labels_1']\n",
    "            del inputs['labels_2']\n",
    " \n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            out = outputs['logits'].double().to(device)\n",
    "            out = F.softmax(out, dim=1)\n",
    "\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            if probs is None:\n",
    "                probs = out.detach().cpu().numpy()\n",
    "                if with_labels:\n",
    "                    gold_labels = labels.detach().cpu().numpy()\n",
    "            else:\n",
    "                probs = np.append(probs, out.detach().cpu().numpy(), axis=0)\n",
    "                if with_labels:\n",
    "                    gold_labels = np.append(gold_labels, labels.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            if with_labels:\n",
    "                eval_loss += loss.item()\n",
    "    \n",
    "    if with_labels:\n",
    "        eval_loss /= (step+1)\n",
    "        print('eval loss: {}'.format(eval_loss))\n",
    "\n",
    "        # compute accuracy\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        accuracy = np.sum(preds == gold_labels)/len(preds)\n",
    "        print('eval accuracy: {}'.format(accuracy))\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b1df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a6f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 8250, 8250, 8250, 0.33)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/imdb/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_imdb = json.load(f)\n",
    "f.close()\n",
    "print(data_imdb.keys())\n",
    "\n",
    "df_imdb = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/imdb/train.csv')\n",
    "len(df_imdb), len(data_imdb['easy']), len(data_imdb['ambiguous']), len(data_imdb['hard']), len(data_imdb['easy'])/len(df_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01fad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " set(),\n",
       " {0,\n",
       "  13,\n",
       "  14,\n",
       "  20,\n",
       "  23,\n",
       "  32,\n",
       "  39,\n",
       "  48,\n",
       "  50,\n",
       "  52,\n",
       "  53,\n",
       "  55,\n",
       "  61,\n",
       "  65,\n",
       "  69,\n",
       "  70,\n",
       "  74,\n",
       "  77,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  84,\n",
       "  87,\n",
       "  89,\n",
       "  90,\n",
       "  92,\n",
       "  97,\n",
       "  98,\n",
       "  105,\n",
       "  108,\n",
       "  109,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  130,\n",
       "  132,\n",
       "  133,\n",
       "  136,\n",
       "  138,\n",
       "  142,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  163,\n",
       "  167,\n",
       "  168,\n",
       "  171,\n",
       "  173,\n",
       "  179,\n",
       "  180,\n",
       "  188,\n",
       "  190,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  224,\n",
       "  228,\n",
       "  233,\n",
       "  236,\n",
       "  245,\n",
       "  246,\n",
       "  252,\n",
       "  255,\n",
       "  257,\n",
       "  260,\n",
       "  263,\n",
       "  264,\n",
       "  267,\n",
       "  271,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  285,\n",
       "  289,\n",
       "  292,\n",
       "  299,\n",
       "  301,\n",
       "  304,\n",
       "  306,\n",
       "  313,\n",
       "  318,\n",
       "  336,\n",
       "  339,\n",
       "  340,\n",
       "  342,\n",
       "  345,\n",
       "  346,\n",
       "  348,\n",
       "  354,\n",
       "  362,\n",
       "  366,\n",
       "  367,\n",
       "  371,\n",
       "  373,\n",
       "  375,\n",
       "  377,\n",
       "  378,\n",
       "  384,\n",
       "  387,\n",
       "  394,\n",
       "  397,\n",
       "  403,\n",
       "  405,\n",
       "  406,\n",
       "  408,\n",
       "  410,\n",
       "  413,\n",
       "  418,\n",
       "  419,\n",
       "  421,\n",
       "  427,\n",
       "  430,\n",
       "  440,\n",
       "  441,\n",
       "  446,\n",
       "  455,\n",
       "  457,\n",
       "  459,\n",
       "  461,\n",
       "  464,\n",
       "  469,\n",
       "  471,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  481,\n",
       "  483,\n",
       "  488,\n",
       "  492,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  501,\n",
       "  504,\n",
       "  509,\n",
       "  511,\n",
       "  515,\n",
       "  516,\n",
       "  518,\n",
       "  522,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  532,\n",
       "  540,\n",
       "  543,\n",
       "  544,\n",
       "  552,\n",
       "  553,\n",
       "  564,\n",
       "  567,\n",
       "  569,\n",
       "  587,\n",
       "  591,\n",
       "  593,\n",
       "  603,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  614,\n",
       "  615,\n",
       "  618,\n",
       "  623,\n",
       "  624,\n",
       "  628,\n",
       "  634,\n",
       "  642,\n",
       "  650,\n",
       "  653,\n",
       "  655,\n",
       "  671,\n",
       "  672,\n",
       "  685,\n",
       "  687,\n",
       "  688,\n",
       "  695,\n",
       "  699,\n",
       "  701,\n",
       "  704,\n",
       "  706,\n",
       "  707,\n",
       "  710,\n",
       "  712,\n",
       "  721,\n",
       "  724,\n",
       "  726,\n",
       "  727,\n",
       "  737,\n",
       "  739,\n",
       "  742,\n",
       "  744,\n",
       "  750,\n",
       "  751,\n",
       "  753,\n",
       "  758,\n",
       "  766,\n",
       "  768,\n",
       "  772,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  781,\n",
       "  783,\n",
       "  784,\n",
       "  788,\n",
       "  790,\n",
       "  791,\n",
       "  793,\n",
       "  794,\n",
       "  802,\n",
       "  815,\n",
       "  816,\n",
       "  825,\n",
       "  827,\n",
       "  828,\n",
       "  832,\n",
       "  835,\n",
       "  838,\n",
       "  845,\n",
       "  848,\n",
       "  853,\n",
       "  858,\n",
       "  860,\n",
       "  863,\n",
       "  865,\n",
       "  868,\n",
       "  871,\n",
       "  872,\n",
       "  876,\n",
       "  877,\n",
       "  879,\n",
       "  885,\n",
       "  886,\n",
       "  888,\n",
       "  891,\n",
       "  893,\n",
       "  895,\n",
       "  896,\n",
       "  899,\n",
       "  905,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  918,\n",
       "  922,\n",
       "  924,\n",
       "  926,\n",
       "  930,\n",
       "  932,\n",
       "  939,\n",
       "  954,\n",
       "  964,\n",
       "  965,\n",
       "  972,\n",
       "  974,\n",
       "  976,\n",
       "  990,\n",
       "  993,\n",
       "  994,\n",
       "  1001,\n",
       "  1009,\n",
       "  1015,\n",
       "  1022,\n",
       "  1026,\n",
       "  1029,\n",
       "  1031,\n",
       "  1034,\n",
       "  1035,\n",
       "  1038,\n",
       "  1040,\n",
       "  1050,\n",
       "  1073,\n",
       "  1075,\n",
       "  1079,\n",
       "  1081,\n",
       "  1082,\n",
       "  1092,\n",
       "  1097,\n",
       "  1106,\n",
       "  1107,\n",
       "  1112,\n",
       "  1114,\n",
       "  1117,\n",
       "  1119,\n",
       "  1121,\n",
       "  1123,\n",
       "  1124,\n",
       "  1125,\n",
       "  1127,\n",
       "  1131,\n",
       "  1142,\n",
       "  1145,\n",
       "  1146,\n",
       "  1152,\n",
       "  1154,\n",
       "  1162,\n",
       "  1166,\n",
       "  1168,\n",
       "  1170,\n",
       "  1175,\n",
       "  1177,\n",
       "  1181,\n",
       "  1187,\n",
       "  1197,\n",
       "  1200,\n",
       "  1204,\n",
       "  1210,\n",
       "  1212,\n",
       "  1214,\n",
       "  1221,\n",
       "  1224,\n",
       "  1225,\n",
       "  1227,\n",
       "  1233,\n",
       "  1242,\n",
       "  1244,\n",
       "  1246,\n",
       "  1251,\n",
       "  1256,\n",
       "  1261,\n",
       "  1263,\n",
       "  1267,\n",
       "  1270,\n",
       "  1275,\n",
       "  1277,\n",
       "  1279,\n",
       "  1281,\n",
       "  1282,\n",
       "  1283,\n",
       "  1285,\n",
       "  1289,\n",
       "  1293,\n",
       "  1296,\n",
       "  1297,\n",
       "  1306,\n",
       "  1308,\n",
       "  1310,\n",
       "  1318,\n",
       "  1321,\n",
       "  1331,\n",
       "  1336,\n",
       "  1340,\n",
       "  1341,\n",
       "  1345,\n",
       "  1347,\n",
       "  1348,\n",
       "  1349,\n",
       "  1351,\n",
       "  1353,\n",
       "  1362,\n",
       "  1367,\n",
       "  1370,\n",
       "  1371,\n",
       "  1372,\n",
       "  1375,\n",
       "  1376,\n",
       "  1377,\n",
       "  1381,\n",
       "  1386,\n",
       "  1390,\n",
       "  1391,\n",
       "  1400,\n",
       "  1401,\n",
       "  1419,\n",
       "  1421,\n",
       "  1422,\n",
       "  1434,\n",
       "  1457,\n",
       "  1466,\n",
       "  1467,\n",
       "  1469,\n",
       "  1477,\n",
       "  1478,\n",
       "  1485,\n",
       "  1493,\n",
       "  1496,\n",
       "  1498,\n",
       "  1499,\n",
       "  1508,\n",
       "  1510,\n",
       "  1511,\n",
       "  1515,\n",
       "  1518,\n",
       "  1523,\n",
       "  1525,\n",
       "  1526,\n",
       "  1530,\n",
       "  1533,\n",
       "  1534,\n",
       "  1539,\n",
       "  1550,\n",
       "  1565,\n",
       "  1568,\n",
       "  1570,\n",
       "  1578,\n",
       "  1582,\n",
       "  1583,\n",
       "  1584,\n",
       "  1585,\n",
       "  1586,\n",
       "  1590,\n",
       "  1594,\n",
       "  1599,\n",
       "  1603,\n",
       "  1606,\n",
       "  1612,\n",
       "  1613,\n",
       "  1614,\n",
       "  1616,\n",
       "  1622,\n",
       "  1643,\n",
       "  1646,\n",
       "  1648,\n",
       "  1651,\n",
       "  1654,\n",
       "  1655,\n",
       "  1661,\n",
       "  1664,\n",
       "  1666,\n",
       "  1668,\n",
       "  1673,\n",
       "  1674,\n",
       "  1675,\n",
       "  1680,\n",
       "  1681,\n",
       "  1686,\n",
       "  1691,\n",
       "  1701,\n",
       "  1705,\n",
       "  1711,\n",
       "  1716,\n",
       "  1722,\n",
       "  1723,\n",
       "  1724,\n",
       "  1729,\n",
       "  1737,\n",
       "  1740,\n",
       "  1755,\n",
       "  1757,\n",
       "  1760,\n",
       "  1773,\n",
       "  1776,\n",
       "  1779,\n",
       "  1782,\n",
       "  1783,\n",
       "  1784,\n",
       "  1788,\n",
       "  1790,\n",
       "  1791,\n",
       "  1792,\n",
       "  1793,\n",
       "  1796,\n",
       "  1797,\n",
       "  1799,\n",
       "  1810,\n",
       "  1811,\n",
       "  1822,\n",
       "  1823,\n",
       "  1831,\n",
       "  1833,\n",
       "  1835,\n",
       "  1841,\n",
       "  1842,\n",
       "  1850,\n",
       "  1851,\n",
       "  1852,\n",
       "  1853,\n",
       "  1856,\n",
       "  1861,\n",
       "  1866,\n",
       "  1867,\n",
       "  1870,\n",
       "  1879,\n",
       "  1891,\n",
       "  1894,\n",
       "  1895,\n",
       "  1897,\n",
       "  1899,\n",
       "  1903,\n",
       "  1905,\n",
       "  1907,\n",
       "  1908,\n",
       "  1910,\n",
       "  1917,\n",
       "  1931,\n",
       "  1937,\n",
       "  1940,\n",
       "  1947,\n",
       "  1949,\n",
       "  1955,\n",
       "  1959,\n",
       "  1961,\n",
       "  1963,\n",
       "  1964,\n",
       "  1968,\n",
       "  1969,\n",
       "  1974,\n",
       "  1984,\n",
       "  1986,\n",
       "  1993,\n",
       "  1994,\n",
       "  1997,\n",
       "  1999,\n",
       "  2000,\n",
       "  2004,\n",
       "  2008,\n",
       "  2013,\n",
       "  2015,\n",
       "  2016,\n",
       "  2021,\n",
       "  2025,\n",
       "  2028,\n",
       "  2029,\n",
       "  2033,\n",
       "  2034,\n",
       "  2039,\n",
       "  2040,\n",
       "  2042,\n",
       "  2043,\n",
       "  2044,\n",
       "  2046,\n",
       "  2052,\n",
       "  2053,\n",
       "  2056,\n",
       "  2057,\n",
       "  2066,\n",
       "  2073,\n",
       "  2079,\n",
       "  2080,\n",
       "  2081,\n",
       "  2083,\n",
       "  2085,\n",
       "  2090,\n",
       "  2096,\n",
       "  2097,\n",
       "  2101,\n",
       "  2103,\n",
       "  2104,\n",
       "  2106,\n",
       "  2110,\n",
       "  2111,\n",
       "  2112,\n",
       "  2113,\n",
       "  2114,\n",
       "  2115,\n",
       "  2116,\n",
       "  2122,\n",
       "  2123,\n",
       "  2126,\n",
       "  2128,\n",
       "  2129,\n",
       "  2132,\n",
       "  2144,\n",
       "  2154,\n",
       "  2168,\n",
       "  2169,\n",
       "  2175,\n",
       "  2177,\n",
       "  2178,\n",
       "  2181,\n",
       "  2182,\n",
       "  2183,\n",
       "  2187,\n",
       "  2188,\n",
       "  2190,\n",
       "  2192,\n",
       "  2195,\n",
       "  2196,\n",
       "  2198,\n",
       "  2199,\n",
       "  2201,\n",
       "  2202,\n",
       "  2203,\n",
       "  2222,\n",
       "  2223,\n",
       "  2225,\n",
       "  2226,\n",
       "  2229,\n",
       "  2233,\n",
       "  2235,\n",
       "  2239,\n",
       "  2240,\n",
       "  2252,\n",
       "  2255,\n",
       "  2256,\n",
       "  2257,\n",
       "  2269,\n",
       "  2270,\n",
       "  2271,\n",
       "  2272,\n",
       "  2279,\n",
       "  2283,\n",
       "  2294,\n",
       "  2300,\n",
       "  2301,\n",
       "  2305,\n",
       "  2312,\n",
       "  2315,\n",
       "  2316,\n",
       "  2321,\n",
       "  2324,\n",
       "  2338,\n",
       "  2344,\n",
       "  2346,\n",
       "  2359,\n",
       "  2362,\n",
       "  2365,\n",
       "  2371,\n",
       "  2379,\n",
       "  2380,\n",
       "  2387,\n",
       "  2392,\n",
       "  2394,\n",
       "  2395,\n",
       "  2397,\n",
       "  2405,\n",
       "  2406,\n",
       "  2407,\n",
       "  2414,\n",
       "  2415,\n",
       "  2418,\n",
       "  2419,\n",
       "  2422,\n",
       "  2429,\n",
       "  2439,\n",
       "  2444,\n",
       "  2447,\n",
       "  2449,\n",
       "  2451,\n",
       "  2455,\n",
       "  2456,\n",
       "  2458,\n",
       "  2459,\n",
       "  2460,\n",
       "  2463,\n",
       "  2464,\n",
       "  2467,\n",
       "  2473,\n",
       "  2476,\n",
       "  2477,\n",
       "  2478,\n",
       "  2479,\n",
       "  2481,\n",
       "  2482,\n",
       "  2485,\n",
       "  2496,\n",
       "  2502,\n",
       "  2504,\n",
       "  2511,\n",
       "  2512,\n",
       "  2524,\n",
       "  2525,\n",
       "  2528,\n",
       "  2529,\n",
       "  2530,\n",
       "  2532,\n",
       "  2534,\n",
       "  2535,\n",
       "  2536,\n",
       "  2537,\n",
       "  2539,\n",
       "  2541,\n",
       "  2544,\n",
       "  2545,\n",
       "  2547,\n",
       "  2553,\n",
       "  2559,\n",
       "  2562,\n",
       "  2565,\n",
       "  2568,\n",
       "  2570,\n",
       "  2585,\n",
       "  2596,\n",
       "  2601,\n",
       "  2618,\n",
       "  2619,\n",
       "  2621,\n",
       "  2625,\n",
       "  2627,\n",
       "  2628,\n",
       "  2632,\n",
       "  2635,\n",
       "  2645,\n",
       "  2650,\n",
       "  2659,\n",
       "  2674,\n",
       "  2675,\n",
       "  2679,\n",
       "  2686,\n",
       "  2689,\n",
       "  2692,\n",
       "  2694,\n",
       "  2699,\n",
       "  2708,\n",
       "  2713,\n",
       "  2715,\n",
       "  2716,\n",
       "  2717,\n",
       "  2718,\n",
       "  2719,\n",
       "  2723,\n",
       "  2733,\n",
       "  2740,\n",
       "  2742,\n",
       "  2743,\n",
       "  2747,\n",
       "  2757,\n",
       "  2758,\n",
       "  2761,\n",
       "  2768,\n",
       "  2769,\n",
       "  2771,\n",
       "  2774,\n",
       "  2775,\n",
       "  2776,\n",
       "  2777,\n",
       "  2778,\n",
       "  2779,\n",
       "  2780,\n",
       "  2783,\n",
       "  2785,\n",
       "  2788,\n",
       "  2789,\n",
       "  2802,\n",
       "  2803,\n",
       "  2804,\n",
       "  2806,\n",
       "  2810,\n",
       "  2818,\n",
       "  2819,\n",
       "  2825,\n",
       "  2828,\n",
       "  2835,\n",
       "  2836,\n",
       "  2837,\n",
       "  2838,\n",
       "  2841,\n",
       "  2845,\n",
       "  2846,\n",
       "  2849,\n",
       "  2854,\n",
       "  2865,\n",
       "  2882,\n",
       "  2885,\n",
       "  2890,\n",
       "  2892,\n",
       "  2902,\n",
       "  2904,\n",
       "  2911,\n",
       "  2916,\n",
       "  2917,\n",
       "  2920,\n",
       "  2924,\n",
       "  2927,\n",
       "  2928,\n",
       "  2931,\n",
       "  2932,\n",
       "  2941,\n",
       "  2943,\n",
       "  2950,\n",
       "  2955,\n",
       "  2964,\n",
       "  2968,\n",
       "  2969,\n",
       "  2970,\n",
       "  2971,\n",
       "  2974,\n",
       "  2993,\n",
       "  3001,\n",
       "  3002,\n",
       "  3003,\n",
       "  3006,\n",
       "  3007,\n",
       "  3009,\n",
       "  3012,\n",
       "  3015,\n",
       "  3021,\n",
       "  3022,\n",
       "  3023,\n",
       "  3024,\n",
       "  3025,\n",
       "  3026,\n",
       "  3030,\n",
       "  3031,\n",
       "  3032,\n",
       "  3034,\n",
       "  3035,\n",
       "  3036,\n",
       "  3037,\n",
       "  3038,\n",
       "  3040,\n",
       "  3042,\n",
       "  3043,\n",
       "  3045,\n",
       "  3046,\n",
       "  3050,\n",
       "  3072,\n",
       "  3073,\n",
       "  3074,\n",
       "  3075,\n",
       "  3076,\n",
       "  3085,\n",
       "  3087,\n",
       "  3089,\n",
       "  3090,\n",
       "  3093,\n",
       "  3094,\n",
       "  3095,\n",
       "  3096,\n",
       "  3100,\n",
       "  3101,\n",
       "  3104,\n",
       "  3112,\n",
       "  3117,\n",
       "  3123,\n",
       "  3124,\n",
       "  3126,\n",
       "  3128,\n",
       "  3130,\n",
       "  3131,\n",
       "  3139,\n",
       "  3140,\n",
       "  3144,\n",
       "  3146,\n",
       "  3147,\n",
       "  3149,\n",
       "  3151,\n",
       "  3152,\n",
       "  3155,\n",
       "  3161,\n",
       "  3162,\n",
       "  3168,\n",
       "  3177,\n",
       "  3183,\n",
       "  3192,\n",
       "  3199,\n",
       "  3201,\n",
       "  3203,\n",
       "  3206,\n",
       "  3209,\n",
       "  3215,\n",
       "  3221,\n",
       "  3223,\n",
       "  3227,\n",
       "  3238,\n",
       "  3247,\n",
       "  3249,\n",
       "  3251,\n",
       "  3252,\n",
       "  3258,\n",
       "  3259,\n",
       "  3261,\n",
       "  3263,\n",
       "  3264,\n",
       "  3265,\n",
       "  3266,\n",
       "  3272,\n",
       "  3273,\n",
       "  3278,\n",
       "  3289,\n",
       "  3296,\n",
       "  3300,\n",
       "  3303,\n",
       "  3306,\n",
       "  3308,\n",
       "  3310,\n",
       "  3311,\n",
       "  3313,\n",
       "  3321,\n",
       "  3322,\n",
       "  3323,\n",
       "  3329,\n",
       "  3332,\n",
       "  3333,\n",
       "  3338,\n",
       "  3344,\n",
       "  3345,\n",
       "  3351,\n",
       "  3354,\n",
       "  3355,\n",
       "  3357,\n",
       "  3360,\n",
       "  3362,\n",
       "  3363,\n",
       "  3368,\n",
       "  3370,\n",
       "  3375,\n",
       "  3382,\n",
       "  3385,\n",
       "  3391,\n",
       "  3394,\n",
       "  3396,\n",
       "  3400,\n",
       "  3403,\n",
       "  3405,\n",
       "  3407,\n",
       "  3408,\n",
       "  3416,\n",
       "  3419,\n",
       "  3422,\n",
       "  3423,\n",
       "  3426,\n",
       "  3427,\n",
       "  3434,\n",
       "  3436,\n",
       "  3437,\n",
       "  3438,\n",
       "  3439,\n",
       "  3440,\n",
       "  3441,\n",
       "  3445,\n",
       "  3449,\n",
       "  3453,\n",
       "  3457,\n",
       "  3459,\n",
       "  3460,\n",
       "  3466,\n",
       "  3472,\n",
       "  3473,\n",
       "  3477,\n",
       "  3480,\n",
       "  3484,\n",
       "  3486,\n",
       "  3489,\n",
       "  3491,\n",
       "  3493,\n",
       "  3496,\n",
       "  3497,\n",
       "  3498,\n",
       "  3499,\n",
       "  3506,\n",
       "  3510,\n",
       "  3523,\n",
       "  3524,\n",
       "  3525,\n",
       "  3531,\n",
       "  3537,\n",
       "  3551,\n",
       "  3555,\n",
       "  3558,\n",
       "  3561,\n",
       "  3563,\n",
       "  3570,\n",
       "  3572,\n",
       "  3574,\n",
       "  3582,\n",
       "  3583,\n",
       "  3587,\n",
       "  3596,\n",
       "  3597,\n",
       "  3599,\n",
       "  3603,\n",
       "  3606,\n",
       "  3607,\n",
       "  3609,\n",
       "  3610,\n",
       "  3611,\n",
       "  3615,\n",
       "  3616,\n",
       "  3617,\n",
       "  3619,\n",
       "  3625,\n",
       "  3631,\n",
       "  3632,\n",
       "  3633,\n",
       "  3635,\n",
       "  3639,\n",
       "  3640,\n",
       "  3641,\n",
       "  3643,\n",
       "  3644,\n",
       "  3646,\n",
       "  3648,\n",
       "  3654,\n",
       "  3657,\n",
       "  3658,\n",
       "  3660,\n",
       "  3666,\n",
       "  3670,\n",
       "  3671,\n",
       "  3672,\n",
       "  3674,\n",
       "  3682,\n",
       "  3684,\n",
       "  3694,\n",
       "  3700,\n",
       "  3704,\n",
       "  3708,\n",
       "  3709,\n",
       "  3711,\n",
       "  3714,\n",
       "  3722,\n",
       "  3726,\n",
       "  3727,\n",
       "  3728,\n",
       "  3729,\n",
       "  3732,\n",
       "  ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_imdb['easy']).intersection(set(data_imdb['ambiguous'])), set(data_imdb['easy']).intersection(set(data_imdb['hard'])), set(data_imdb['hard']).intersection(set(data_imdb['ambiguous']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d082d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67349, 22225, 22225, 22225, 0.3299974758348305)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/sst2/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_sst2 = json.load(f)\n",
    "f.close()\n",
    "print(data_sst2.keys())\n",
    "df_sst2 = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/sst2/train.csv')\n",
    "len(df_sst2), len(data_sst2['easy']), len(data_sst2['ambiguous']), len(data_sst2['hard']), len(data_sst2['easy'])/len(df_sst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23f6304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hard', 'easy', 'ambiguous'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(560000, 184800, 184800, 184800, 0.33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dy_log/yelp_polarity/roberta-base/three_regions_data_indices.json', 'r') as f:\n",
    "    data_yelp_polarity = json.load(f)\n",
    "f.close()\n",
    "print(data_yelp_polarity.keys())\n",
    "df_yelp_polarity = pd.read_csv('/projects/ogma3/atharvak/Data_Cartography/datasets/yelp_polarity/train.csv')\n",
    "len(df_yelp_polarity), len(data_yelp_polarity['easy']), len(data_yelp_polarity['ambiguous']), len(data_yelp_polarity['hard']), len(data_yelp_polarity['easy'])/len(df_yelp_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acbb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
